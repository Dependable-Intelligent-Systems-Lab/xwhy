{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkuVVSU4wIDV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8da4deaa-7635-4bcf-d7c0-f994edca83da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m939.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for gdown (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install -q \\\n",
        "    img2img-turbo~=0.0.1 \\\n",
        "    transformers~=4.57.2 \\\n",
        "    torch~=2.9.0 \\\n",
        "    scipy~=1.16.3 \\\n",
        "    pillow~=11.3.0 \\\n",
        "    requests~=2.32.4 \\\n",
        "    numpy~=2.0.2 \\\n",
        "    pandas~=2.2.2 \\\n",
        "    sentence-transformers~=5.1.2 \\\n",
        "    opencv-python~=4.12.0.88 \\\n",
        "    scikit-learn~=1.6.1 \\\n",
        "    matplotlib~=3.10.0 \\\n",
        "    gensim~=4.4.0 pot~=0.9.6 \\\n",
        "    python-dotenv~=1.2.1 \\\n",
        "    google-genai \\\n",
        "    xgboost~=3.1.2 \\\n",
        "    openai~=2.9.0 \\\n",
        "    diffusers~=0.35.2 \\\n",
        "    accelerate~=1.12.0 \\\n",
        "    safetensors~=0.7.0 \\\n",
        "    torchvision~=0.24.0 \\\n",
        "    plotly~=5.24.1 \\\n",
        "    kaleido~=0.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Os9yHw_F-fk9",
        "outputId": "ea310e34-018d-4a3d-cceb-cd2bfb079fcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU compatibility patch applied - img2img-turbo now works on CPU!\n",
            "All CPU compatibility patches applied successfully!\n",
            "CLIP patch applied: Using open_clip-torch as backend for 'clip' imports.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import base64\n",
        "import logging\n",
        "import gdown\n",
        "import zipfile\n",
        "import pickle\n",
        "from enum import Enum\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Tuple, Optional, Union, Literal\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import requests\n",
        "import tiktoken\n",
        "\n",
        "import torch\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "from diffusers import (\n",
        "    StableDiffusionInpaintPipeline,\n",
        "    StableDiffusionInstructPix2PixPipeline,\n",
        "    EulerAncestralDiscreteScheduler\n",
        ")\n",
        "from torchvision.models.segmentation import (\n",
        "    deeplabv3_resnet101,\n",
        "    DeepLabV3_ResNet101_Weights,\n",
        ")\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont, ImageOps\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.io as pio\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import FancyBboxPatch\n",
        "from matplotlib import transforms\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.linear_model import LinearRegression, BayesianRidge, Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,\n",
        "    mean_absolute_error,\n",
        "    r2_score,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    auc,\n",
        ")\n",
        "from scipy.stats import wasserstein_distance\n",
        "from gensim.models import KeyedVectors\n",
        "import gensim.downloader as api\n",
        "from img2img_turbo import run_inference_paired\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH3gdtDWYb2b"
      },
      "source": [
        "# Conect to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXgamu0EYWk8",
        "outputId": "af3e52e3-fad3-4cd6-a71f-0a0c28787d57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1K-m9_MkLTW"
      },
      "source": [
        "# Load envs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yysCfiUPkMpB",
        "outputId": "d18cc331-06d1-48aa-f9ea-52fe4b896d53"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86VzTakHA0GE"
      },
      "source": [
        "# Configure logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4mbL63SAu9D"
      },
      "outputs": [],
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HisQuQYwZm_l"
      },
      "source": [
        "# Surrogate model types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dognnXa7Zk4e"
      },
      "outputs": [],
      "source": [
        "class SurrogateMethod(Enum):\n",
        "    \"\"\"Enumeration for supported surrogate model types.\"\"\"\n",
        "    # Linear Models\n",
        "    GLM_OLS = \"glm_ols\"        # Global/Unweighted Ordinary Least Squares (OLS)\n",
        "    GLM_RIDGE = \"glm_ridge\"    # Global/Unweighted Ridge Regression (L2)\n",
        "    LIME = \"lime_ols\"          # Local Weighted OLS\n",
        "    LIME_RIDGE = \"lime_ridge\"  # Local Weighted Ridge Regression (L2)\n",
        "    BAYLIME = \"baylime\"        # Bayesian Weighted Ridge (L2)\n",
        "    # Non-Linear Tree-Based Models\n",
        "    RANDOMFOREST = \"randomforest\"\n",
        "    GRADIENT_BOOSTING = \"gradientboosting\"\n",
        "    XGBOOST = \"xgboost\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXZpPgY8MGOy"
      },
      "source": [
        "# Download I2EBench [Dataset](https://github.com/cocoshe/I2EBench)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfc4xi7qMG_n"
      },
      "outputs": [],
      "source": [
        "def download_i2ebench_dataset(\n",
        "    url: str = \"https://drive.google.com/uc?id=10X2C6INLqhY_hbgnOcUNvBD03P-cpX78\",\n",
        "    output_filename: str = \"i2ebench.zip\",\n",
        "    extract_dir: str = \"i2ebench\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Downloads the I2EBench dataset from Google Drive and extracts it.\n",
        "\n",
        "    Args:\n",
        "        url (str): Google Drive URL of the dataset zip file.\n",
        "        output_filename (str): Local filename to save the downloaded zip file.\n",
        "        extract_dir (str): Directory where the zip file contents will be extracted.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the extracted directory.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Downloading dataset from: {url} to {output_filename}\")\n",
        "    gdown.download(url, output_filename, quiet=False)\n",
        "\n",
        "    print(f\"Creating extraction directory: {extract_dir}\")\n",
        "    os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Extracting {output_filename} to: {extract_dir}\")\n",
        "    with zipfile.ZipFile(output_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "\n",
        "    print(f\"Dataset extracted to: {extract_dir}\")\n",
        "    return extract_dir\n",
        "\n",
        "\n",
        "def load_i2ebench_data(\n",
        "    root_dir: str = \"i2ebench\",\n",
        "    categories_dir: List[str] | None = None,\n",
        "    limits_per_category: Union[List[int], int] = 1\n",
        ") -> Dict[str, List[Tuple[str, str]]]:\n",
        "    \"\"\"Parses the I2EBench dataset with limits and file validation.\n",
        "\n",
        "    Args:\n",
        "        root_dir: The root directory of the dataset. Defaults to \"i2ebench\".\n",
        "        categories_dir: A list of category directory names to process.\n",
        "            Defaults to the standard 8 categories if None.\n",
        "        limits_per_category: Defines how many items to load for each category.\n",
        "            - If a List[int]: Must match the length of categories_dir.\n",
        "            - If an int (N): It sets the limit to N for ALL categories, resulting\n",
        "              in a list like [N, N, N, ...] with length equal to categories_dir.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are category names and values are lists of\n",
        "        tuples. Each tuple contains (full_image_path, prompt).\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If main directories or JSON files are missing.\n",
        "        ValueError: If the lengths of limits and categories do not match.\n",
        "    \"\"\"\n",
        "    # Default mutable argument handling\n",
        "    if categories_dir is None:\n",
        "        categories_dir = [\n",
        "            'Deblurring', 'HazeRemoval', 'Lowlight', 'NoiseRemoval',\n",
        "            'RainRemoval', 'ShadowRemoval', 'SnowRemoval', 'WatermarkRemoval'\n",
        "        ]\n",
        "\n",
        "    # 1. Validate root directory structure\n",
        "    edit_data_path = os.path.join(root_dir, \"EditBench\", \"EditData\")\n",
        "    if not os.path.exists(edit_data_path):\n",
        "        raise FileNotFoundError(f\"The directory '{edit_data_path}' does not exist.\")\n",
        "\n",
        "    # 2. Handle limit generation logic\n",
        "    if isinstance(limits_per_category, int):\n",
        "        # Repeat the single integer value for all categories.\n",
        "        limit_value = limits_per_category\n",
        "        limits_per_category = [limit_value] * len(categories_dir)\n",
        "\n",
        "    # 3. Check length alignment\n",
        "    if len(limits_per_category) != len(categories_dir):\n",
        "        raise ValueError(\n",
        "            f\"Length mismatch: 'limits_per_category' has {len(limits_per_category)} elements, \"\n",
        "            f\"but 'categories_dir' has {len(categories_dir)} elements.\"\n",
        "        )\n",
        "\n",
        "    # 4. Check existence of all category directories\n",
        "    for category in categories_dir:\n",
        "        category_path = os.path.join(edit_data_path, category)\n",
        "        if not os.path.exists(category_path):\n",
        "            raise FileNotFoundError(f\"Category directory not found: {category_path}\")\n",
        "\n",
        "    # 5. Process data\n",
        "    dataset_dict = {}\n",
        "\n",
        "    for index, category in enumerate(categories_dir):\n",
        "        limit = limits_per_category[index]\n",
        "\n",
        "        category_path = os.path.join(edit_data_path, category)\n",
        "        json_file_path = os.path.join(category_path, f\"{category}.json\")\n",
        "        image_input_dir = os.path.join(category_path, \"input\")\n",
        "\n",
        "        try:\n",
        "            with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"JSON file missing: {json_file_path}\")\n",
        "\n",
        "        items_list = []\n",
        "\n",
        "        for key, info in data.items():\n",
        "            # STOP condition: Check if we successfully collected enough items\n",
        "            if len(items_list) >= limit:\n",
        "                break\n",
        "\n",
        "            image_filename = info.get(\"image\")\n",
        "            prompt = info.get(\"ori_exp\")\n",
        "\n",
        "            if image_filename and prompt:\n",
        "                full_img_path = os.path.join(image_input_dir, image_filename)\n",
        "\n",
        "                # Validation: Check if the image file actually exists\n",
        "                if not os.path.exists(full_img_path):\n",
        "                    # Skip this item if the file is missing\n",
        "                    continue\n",
        "\n",
        "                items_list.append((full_img_path, prompt))\n",
        "\n",
        "        dataset_dict[category] = items_list\n",
        "\n",
        "    return dataset_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yziSl3J7ddP-"
      },
      "source": [
        "# Save and load data to/from pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgW2xmMmdeGm"
      },
      "outputs": [],
      "source": [
        "def save_to_pickle(output_path: str = \"pickle_data.pkl\", **data: Any) -> None:\n",
        "    \"\"\"\n",
        "    Saves keyword arguments directly to a pickle file.\n",
        "\n",
        "    Args:\n",
        "        output_path (str): Full path (including filename) where the\n",
        "            output pickle should be saved.\n",
        "        **data: Arbitrary named data to persist.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(output_path, \"wb\") as f:\n",
        "            pickle.dump(data, f)\n",
        "        print(f\"Data successfully saved to: {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving data to pickle file: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def load_from_pickle(file_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Loads and returns the object stored in a pickle file.\n",
        "\n",
        "    This function returns exactly the same dictionary that was written\n",
        "    by `save_to_pickle`, without modifying its structure or contents.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the pickle file.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: The unpickled dictionary containing the data\n",
        "        that was originally saved.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the specified file does not exist.\n",
        "        Exception: If an error occurs during unpickling.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            loaded_data = pickle.load(f)\n",
        "        print(f\"Data successfully loaded from: {file_path}\")\n",
        "        return loaded_data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data from pickle file: {e}\")\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bENyR0aaA1V0"
      },
      "source": [
        "# Word2Vec model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8moXIvNAyIq"
      },
      "outputs": [],
      "source": [
        "def _download_file_fast(url: str, output_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Download a file quickly, using requests.\n",
        "    Includes a basic check for file size to detect failed downloads.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Attempting direct download from {url} to {output_path} using requests...\")\n",
        "    try:\n",
        "        response = requests.get(url, stream=True, timeout=300)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Minimum expected size for GoogleNews-vectors-negative300.bin.gz is ~1.5GB\n",
        "        min_expected_compressed_size_bytes = 100 * 1024 * 1024 # 100 MB as a sanity check\n",
        "\n",
        "        downloaded_size = 0\n",
        "        with open(output_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "                    downloaded_size += len(chunk)\n",
        "\n",
        "        if downloaded_size < min_expected_compressed_size_bytes:\n",
        "            raise IOError(\n",
        "                f\"Downloaded file is too small ({downloaded_size / (1024*1024):.2f} MB). \"\n",
        "                f\"Expected compressed file size > {min_expected_compressed_size_bytes / (1024*1024):.2f} MB.\"\n",
        "            )\n",
        "        logger.info(f\"Successfully downloaded {downloaded_size / (1024*1024):.2f} MB to {output_path}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error during direct download from {url}: {e}\")\n",
        "        if os.path.exists(output_path):\n",
        "            os.remove(output_path)\n",
        "        raise\n",
        "    except IOError as e:\n",
        "        logger.error(f\"File system or size check error: {e}\")\n",
        "        if os.path.exists(output_path):\n",
        "            os.remove(output_path)\n",
        "        raise\n",
        "\n",
        "\n",
        "def _extract_gzip(gz_path: str, output_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Extract a .gz file safely.\n",
        "    \"\"\"\n",
        "    import gzip\n",
        "    import shutil\n",
        "\n",
        "    logger.info(\"Extracting gzip archive...\")\n",
        "    with gzip.open(gz_path, \"rb\") as f_in, open(output_path, \"wb\") as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "\n",
        "# TODO: Improving and using better Word2Vec Models and also giving user an option\n",
        "# to have preference on this\n",
        "\n",
        "def load_google_news_vectors(\n",
        "    cache_dir: str = \"~/.cache/google_news_vectors\",\n",
        "    force_download: bool = False,\n",
        ") -> KeyedVectors:\n",
        "    \"\"\"\n",
        "    Load Google News Word2Vec vectors with local caching, prioritizing gensim.downloader.\n",
        "\n",
        "    Args:\n",
        "        cache_dir (str): Directory where the final .bin model will be stored/loaded from.\n",
        "        force_download (bool): If True, bypass existing local cache and force re-download/re-load from gensim.\n",
        "                               If gensim.downloader also bypasses its cache, it will re-download.\n",
        "\n",
        "    Returns:\n",
        "        KeyedVectors: Loaded Google News Word2Vec model.\n",
        "    \"\"\"\n",
        "    cache_dir = os.path.expanduser(cache_dir)\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    final_bin_path = os.path.join(cache_dir, \"GoogleNews-vectors-negative300.bin\")\n",
        "\n",
        "    # 1. Try to load from the user-specified cache_dir first (unless force_download)\n",
        "    if os.path.exists(final_bin_path) and not force_download:\n",
        "        logger.info(f\"Loading Google News vectors from specified local cache: {final_bin_path}...\")\n",
        "        return KeyedVectors.load_word2vec_format(final_bin_path, binary=True)\n",
        "\n",
        "    # 2. If not in cache_dir or force_download, use gensim.downloader\n",
        "    logger.info(\"Attempting to load 'word2vec-google-news-300' via gensim.downloader...\")\n",
        "    try:\n",
        "        # gensim.downloader manages its own caching (usually in ~/.gensim/data)\n",
        "        model = api.load(\"word2vec-google-news-300\")\n",
        "        logger.info(\"Successfully loaded model via gensim.downloader.\")\n",
        "\n",
        "        # Save the model to the user's specified cache_dir for consistent storage\n",
        "        logger.info(f\"Saving gensim-loaded model to specified cache: {final_bin_path}...\")\n",
        "        model.save_word2vec_format(final_bin_path, binary=True)\n",
        "        return model\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Gensim.downloader failed to load 'word2vec-google-news-300': {e}. Falling back to direct HTTP download.\")\n",
        "\n",
        "        # 3. Fallback to direct HTTP download if gensim.downloader fails\n",
        "        # This URL is prone to changing/breaking. It's a last resort.\n",
        "        manual_gz_path = final_bin_path + \".gz\"\n",
        "        manual_download_url = (\n",
        "            \"https://public.ukp.informatik.tu-darmstadt.de/reimers/wordembeddings/GoogleNews-vectors-negative300.bin.gz\"\n",
        "        )\n",
        "        try:\n",
        "            _download_file_fast(manual_download_url, manual_gz_path)\n",
        "            _extract_gzip(manual_gz_path, final_bin_path)\n",
        "            logger.info(\"Successfully loaded Google News vectors via direct HTTP download.\")\n",
        "            return KeyedVectors.load_word2vec_format(final_bin_path, binary=True)\n",
        "        except Exception as manual_e:\n",
        "            logger.error(f\"Direct HTTP download and extraction failed: {manual_e}\")\n",
        "            raise RuntimeError(\"Failed to load Google News Word2Vec model from all sources.\") from manual_e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD4KKdr6A52K"
      },
      "source": [
        "# Perturbations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fdc-ZoHpA7V2"
      },
      "outputs": [],
      "source": [
        "def apply_binary_mask(\n",
        "    text_list: list[str],\n",
        "    mask: tuple[int, ...],\n",
        "    rng: np.random.Generator\n",
        ") -> list[str]:\n",
        "    \"\"\"Apply a binary mask to a list of words and ensure minimum word count.\n",
        "\n",
        "    The mask selects words where a `1` indicates inclusion. If the mask\n",
        "    includes fewer than two valid words, additional random words from the\n",
        "    original text are added to preserve semantic stability.\n",
        "\n",
        "    Args:\n",
        "        text_list (list[str]): Tokenized original text.\n",
        "        mask (tuple[int, ...]): Binary mask indicating selected words.\n",
        "        rng (np.random.Generator): Random generator for selecting fallback\n",
        "            words.\n",
        "\n",
        "    Returns:\n",
        "        list[str]: The perturbed list of words.\n",
        "    \"\"\"\n",
        "    selected = [\n",
        "        w for w, flag in zip(text_list, mask)\n",
        "        if flag == 1 and w.strip() != \"\"\n",
        "    ]\n",
        "\n",
        "    if len(selected) < 2:\n",
        "        needed = 2 - len(selected)\n",
        "        candidates = [w for w in text_list if w.strip() != \"\"]\n",
        "        extra = rng.choice(candidates, needed, replace=False)\n",
        "        result = list(set(selected + list(extra)))\n",
        "    else:\n",
        "        result = list(set(selected))\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def generate_perturbations(\n",
        "    text: str,\n",
        "    num_perturb: int = 64,\n",
        "    seed: int = 1024\n",
        ") -> tuple[list[str], list[tuple]]:\n",
        "    \"\"\"Generate unique binary perturbations and perturbed text versions.\n",
        "\n",
        "    Args:\n",
        "        text (str): Original input text.\n",
        "        num_perturb (int): Number of perturbations to generate.\n",
        "        seed (int): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        tuple[list[str], list[tuple]]:\n",
        "            - List of perturbed texts (Responses)\n",
        "            - List of binary perturbation vectors (Perturbations)\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    text_list = words.copy()\n",
        "    num_words = len(words)\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    responses = []\n",
        "    perturbations = []\n",
        "    unique_perturbations = set()\n",
        "\n",
        "    attempts = 0\n",
        "    max_attempts = num_perturb * 10\n",
        "\n",
        "    # Loop for unique perturbations only\n",
        "    while len(unique_perturbations) < num_perturb and attempts < max_attempts:\n",
        "        p = tuple(rng.binomial(1, 0.5, size=num_words))\n",
        "\n",
        "        if p not in unique_perturbations and sum(p) > 0:\n",
        "            unique_perturbations.add(p)\n",
        "            perturbed_txt = apply_binary_mask(text_list, p, rng)\n",
        "            corpus = \" \".join(perturbed_txt)\n",
        "\n",
        "            responses.append(corpus)\n",
        "            perturbations.append(p)\n",
        "\n",
        "            logger.info(\n",
        "                \"Perturbation: %s, Perturbed Text: %s\", p, corpus\n",
        "            )\n",
        "\n",
        "        attempts += 1\n",
        "\n",
        "    # If more perturbations are needed, allow repeats by sampling\n",
        "    # from unique_perturbations\n",
        "    while len(responses) < num_perturb:\n",
        "        p = rng.choice(list(unique_perturbations))\n",
        "        perturbed_txt = apply_binary_mask(text_list, p, rng)\n",
        "        corpus = \" \".join(perturbed_txt)\n",
        "\n",
        "        responses.append(corpus)\n",
        "        perturbations.append(p)\n",
        "\n",
        "        logger.info(\n",
        "            \"Perturbation (reused): %s, Perturbed Text: %s\", p, corpus\n",
        "        )\n",
        "\n",
        "    return responses, perturbations\n",
        "\n",
        "\n",
        "def inject_text_at_position(\n",
        "    original_text: str,\n",
        "    position: Literal[\"start\", \"middle\", \"end\"],\n",
        "    default_index: int = 4,\n",
        "    inject_text: Optional[str] = None\n",
        ") -> str:\n",
        "    \"\"\"Injects text into a string at a specific position based on a selection.\n",
        "\n",
        "    The function uses a provided 'inject_text' string if available; otherwise,\n",
        "    it selects a word from a default list using 'default_index'.\n",
        "\n",
        "    Args:\n",
        "        original_text (str): The base prompt or text.\n",
        "        position (Literal[\"start\", \"middle\", \"end\"]): The placement target.\n",
        "        default_index (int): Index to select from ['could', 'would', 'should',\n",
        "            '###', 'please']. Defaults to 4 (\"please\").\n",
        "        inject_text (Optional[str]): A custom string to inject. If provided,\n",
        "            it overrides the default_index selection.\n",
        "\n",
        "    Returns:\n",
        "        str: The new text with the injected word/phrase.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the position is invalid or default_index is out of range.\n",
        "    \"\"\"\n",
        "    default_options = ['could', 'would', 'should', '###', 'please']\n",
        "\n",
        "    # Selection logic\n",
        "    if inject_text is not None:\n",
        "        word_to_add = inject_text\n",
        "    else:\n",
        "        if 0 <= default_index < len(default_options):\n",
        "            word_to_add = default_options[default_index]\n",
        "        else:\n",
        "            raise ValueError(f\"default_index must be between 0 and {len(default_options)-1}\")\n",
        "\n",
        "    words = original_text.split()\n",
        "\n",
        "    if position == \"start\":\n",
        "        words.insert(0, word_to_add)\n",
        "    elif position == \"middle\":\n",
        "        # Calculates middle based on word count\n",
        "        mid = len(words) // 2\n",
        "        words.insert(mid, word_to_add)\n",
        "    elif position == \"end\":\n",
        "        words.append(word_to_add)\n",
        "    else:\n",
        "        raise ValueError(\"Position must be 'start', 'middle', or 'end'.\")\n",
        "\n",
        "    return \" \".join(words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvBwvyw4A-D1"
      },
      "source": [
        "# Generate image and embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZR-OPkxA_4S"
      },
      "outputs": [],
      "source": [
        "def _create_placeholder_image(\n",
        "    prompt: str,\n",
        "    output_dir: str,\n",
        "    filename_prefix: str = \"gemini_generated\",\n",
        "    save: bool = True\n",
        ") -> Union[str, Image.Image]:\n",
        "    \"\"\"Creates a black placeholder image with error text using Pillow.\n",
        "\n",
        "    Helper function to maintain consistency with the original logic.\n",
        "    \"\"\"\n",
        "    img_size = (600, 600)\n",
        "    black_image = Image.new('RGB', img_size, color='black')\n",
        "    draw = ImageDraw.Draw(black_image)\n",
        "\n",
        "    text_to_draw = f\"image wasn't generated because of\\n\\\"{prompt}\\\"\\ndoesn't mean\"\n",
        "\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arialbd.ttf\", 40)  # Adjusted size for 600x600\n",
        "    except IOError:\n",
        "        font = ImageFont.load_default()\n",
        "\n",
        "    # Calculate text position to center it\n",
        "    bbox = draw.textbbox((0, 0), text_to_draw, font=font)\n",
        "    text_width = bbox[2] - bbox[0]\n",
        "    text_height = bbox[3] - bbox[1]\n",
        "    x = (img_size[0] - text_width) / 2\n",
        "    y = (img_size[1] - text_height) / 2\n",
        "\n",
        "    draw.text((x, y), text_to_draw, fill=\"white\", font=font, align=\"center\")\n",
        "\n",
        "    if save:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        timestamp = int(time.time() * 1000)\n",
        "        filename = f\"{filename_prefix}_{timestamp}.png\"\n",
        "        gen_path = os.path.join(output_dir, filename)\n",
        "\n",
        "        black_image.save(gen_path)\n",
        "        return gen_path\n",
        "    else:\n",
        "        return black_image\n",
        "\n",
        "\n",
        "def extract_image_embedding(\n",
        "    image_path: str,\n",
        "    processor: Any,\n",
        "    model: Any\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Extract an image embedding using a DINOv2 model.\n",
        "\n",
        "    Args:\n",
        "        image (str): Input image.\n",
        "        processor (Any): DINOv2 image processor.\n",
        "        model (Any): DINOv2 model.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Extracted embedding vector.\n",
        "    \"\"\"\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "    inputs['pixel_values'] = inputs['pixel_values'].to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    emb = outputs.last_hidden_state.mean(dim=1)\n",
        "    emb = emb.squeeze().cpu().numpy()\n",
        "    return emb\n",
        "\n",
        "\n",
        "def generate_image_gemini(\n",
        "    input_image_path: str,\n",
        "    prompt: Union[str, List[str]],\n",
        "    output_dir: str,\n",
        "    api_key: str,\n",
        "    model_name: str = \"gemini-2.5-flash-image\",\n",
        "    temperature: float = 1.0,\n",
        "    top_p: float = 0.95,\n",
        "    top_k: int = 40,\n",
        "    max_output_tokens: int = 8192,\n",
        "    stream: bool = True,\n",
        "    seed: Optional[int] = None,\n",
        ") -> Union[Tuple[bool, str, float], List[Tuple[bool, str, float]]]:\n",
        "    \"\"\"Generate an edited image using the Gemini API.\n",
        "\n",
        "    Supports single (standard mode) or batch mode (list of prompts).\n",
        "\n",
        "    Args:\n",
        "        input_image_path (str): Path to the original image.\n",
        "        prompt (str | list[str]): Text instruction(s) for image editing.\n",
        "        output_dir (str): Directory to store generated images.\n",
        "        api_key (str): Gemini API key.\n",
        "        model_name (str): Name of the Gemini model.\n",
        "        temperature (float): Sampling temperature.\n",
        "        top_p (float): Nucleus sampling probability.\n",
        "        top_k (int): Top-k sampling parameter.\n",
        "        max_output_tokens (int): Maximum output tokens.\n",
        "        stream (bool): Whether to use streaming generation.\n",
        "        seed (int | None): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        tuple[bool, str, float] | list[tuple[bool, str, float]]:\n",
        "            - bool (gen_img_flag): True if image was generated by AI, False if placeholder was used.\n",
        "            - str: Path to the generated image.\n",
        "            - float: The estimated cost of generation.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the input image is not found.\n",
        "        TypeError: If prompt is not str or list[str].\n",
        "    \"\"\"\n",
        "    # TODO: modify this function to accept only prompt not list of prompt\n",
        "    def generate_single(single_prompt: str, image_data: bytes, mime_type: str, client: genai.Client) -> Tuple[str, bool, float]:\n",
        "        # Define pricing for Gemini models\n",
        "        input_cost_2_5_flash = 0.0005\n",
        "        output_cost_2_5_flash = 0.002\n",
        "        input_cost_3_pro = 0.001\n",
        "        output_cost_3_pro = 0.004\n",
        "\n",
        "        text_part = types.Part.from_text(text=single_prompt)\n",
        "        image_part = types.Part.from_bytes(data=image_data, mime_type=mime_type)\n",
        "        contents = [types.Content(role=\"user\", parts=[image_part, text_part])]\n",
        "\n",
        "        generate_content_config = types.GenerateContentConfig(\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            max_output_tokens=max_output_tokens,\n",
        "            response_modalities=[\"image\", \"text\"],\n",
        "            response_mime_type=\"text/plain\",\n",
        "            seed=seed\n",
        "        )\n",
        "\n",
        "        generated_img = None\n",
        "        gen_img_flag = True\n",
        "        cost = 0.0  # Initialize cost\n",
        "\n",
        "        # Calculate input_cost based on model_name\n",
        "        if model_name == \"gemini-2.5-flash-image\":\n",
        "            input_cost = input_cost_2_5_flash\n",
        "        elif model_name == \"gemini-3-pro-image-preview\":\n",
        "            input_cost = input_cost_3_pro\n",
        "        else:\n",
        "            input_cost = 0.0\n",
        "\n",
        "        try:\n",
        "            if stream:\n",
        "                response_iter = client.models.generate_content_stream(\n",
        "                    model=model_name, contents=contents, config=generate_content_config\n",
        "                )\n",
        "                for chunk in response_iter:\n",
        "                    for part in chunk.parts:\n",
        "                        if part.inline_data is not None:\n",
        "                            generated_img = Image.open(BytesIO(part.inline_data.data))\n",
        "                            mime_type = part.inline_data.mime_type\n",
        "                            # Calculate output_cost and update total cost\n",
        "                            if model_name == \"gemini-2.5-flash-image\":\n",
        "                                output_cost = output_cost_2_5_flash\n",
        "                            elif model_name == \"gemini-3-pro-image-preview\":\n",
        "                                output_cost = output_cost_3_pro\n",
        "                            else:\n",
        "                                output_cost = 0.0\n",
        "                            cost = input_cost + output_cost\n",
        "                            break  # Stop after first successful response\n",
        "            else:\n",
        "                response = client.models.generate_content(\n",
        "                    model=model_name, contents=contents, config=generate_content_config\n",
        "                )\n",
        "                for part in response.parts:\n",
        "                    if part.inline_data is not None:\n",
        "                        generated_img = Image.open(BytesIO(part.inline_data.data))\n",
        "                        mime_type = part.inline_data.mime_type\n",
        "                        # Calculate output_cost and update total cost\n",
        "                        if model_name == \"gemini-2.5-flash-image\":\n",
        "                            output_cost = output_cost_2_5_flash\n",
        "                        elif model_name == \"gemini-3-pro-image-preview\":\n",
        "                            output_cost = output_cost_3_pro\n",
        "                        else:\n",
        "                            output_cost = 0.0\n",
        "                        cost = input_cost + output_cost\n",
        "                        break\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"Error during API call: {e}\")\n",
        "\n",
        "        if generated_img is None:\n",
        "            gen_img_flag = False\n",
        "            logger.info(f\"Failed to generate image for prompt: '{single_prompt}'. Creating placeholder.\")\n",
        "\n",
        "            generated_img = _create_placeholder_image(\n",
        "                prompt=single_prompt,\n",
        "                output_dir=output_dir,\n",
        "                save=False\n",
        "            )\n",
        "            cost = 0.0  # No cost for placeholder\n",
        "            mime_type = \"image/png\"  # Force PNG for the placeholder\n",
        "\n",
        "        # Determine file extension based on MIME type\n",
        "        if mime_type == \"image/jpeg\":\n",
        "            ext = \".jpg\"\n",
        "        elif mime_type == \"image/png\":\n",
        "            ext = \".png\"\n",
        "        else:\n",
        "            ext = \".png\"\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        timestamp = int(time.time() * 1000)\n",
        "        filename = f\"gemini_generated_{timestamp}{ext}\"\n",
        "        gen_path = os.path.join(output_dir, filename)\n",
        "\n",
        "        generated_img.save(gen_path)\n",
        "\n",
        "        print(f\"------------------- \\\"{gen_path}\\\" generated! (Success: {gen_img_flag}) -------------------\")\n",
        "        return gen_img_flag, gen_path, cost\n",
        "\n",
        "    if not os.path.exists(input_image_path):\n",
        "        raise FileNotFoundError(f\"Input image not found: {input_image_path}\")\n",
        "\n",
        "    with open(input_image_path, \"rb\") as image_file:\n",
        "        image_data = image_file.read()\n",
        "\n",
        "    # Determine initial file extension based on path (for the request)\n",
        "    if input_image_path.lower().endswith((\".jpg\", \".jpeg\")):\n",
        "        mime_type = \"image/jpeg\"\n",
        "    elif input_image_path.lower().endswith(\".png\"):\n",
        "        mime_type = \"image/png\"\n",
        "    else:\n",
        "        mime_type = \"image/jpeg\"\n",
        "\n",
        "    with genai.Client(api_key=api_key) as client:\n",
        "        if isinstance(prompt, str):\n",
        "            return generate_single(prompt, image_data, mime_type, client)\n",
        "        elif isinstance(prompt, list):\n",
        "            return [generate_single(p, image_data, mime_type, client) for p in prompt]\n",
        "        else:\n",
        "            raise TypeError(\"Prompt must be str or list[str] for batch mode.\")\n",
        "\n",
        "\n",
        "def generate_image_openai(\n",
        "    input_image_path: str,\n",
        "    prompt: Union[str, List[str]],\n",
        "    output_dir: str,\n",
        "    api_key: str,\n",
        "    model_name: str = \"gpt-image-1\",\n",
        "    size: str = \"1024x1024\",\n",
        "    quality: str = \"auto\",\n",
        "    n: int = 1,\n",
        ") -> Union[Tuple[bool, str, float], List[Tuple[bool, str, float]]]:\n",
        "    \"\"\"\n",
        "    Edit an image using the OpenAI Image Editing API (image + prompt).\n",
        "\n",
        "    This behaves like Gemini image editing: the input image is modified\n",
        "    according to the provided text instruction.\n",
        "\n",
        "    Args:\n",
        "        input_image_path (str): Path to the original image.\n",
        "        prompt (str | list[str]): Text instruction(s) for image editing.\n",
        "        output_dir (str): Directory to store generated images.\n",
        "        api_key (str): OpenAI API key.\n",
        "        model_name (str): Image model (default: gpt-image-1).\n",
        "        size (str): Output image size (e.g., \"1024x1024\").\n",
        "        quality (str): Image quality (\"low\", \"medium\", \"high\", and \"auto\").\n",
        "        n (int): Number of images to generate (OpenAI usually supports 1).\n",
        "\n",
        "    Returns:\n",
        "        tuple[bool, str, float] | list[tuple[bool, str, float]]:\n",
        "            - bool: True if generated by AI, False if placeholder was used\n",
        "            - str: Path to the generated image\n",
        "            - float: The estimated cost of generation.\n",
        "    \"\"\"\n",
        "\n",
        "    if not api_key:\n",
        "        raise ValueError(\"OpenAI API key is required.\")\n",
        "\n",
        "    if not os.path.exists(input_image_path):\n",
        "        raise FileNotFoundError(f\"Input image not found: {input_image_path}\")\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    client = OpenAI(api_key=api_key)\n",
        "\n",
        "    def generate_single(single_prompt: str) -> Tuple[bool, str, float]:\n",
        "        gen_img_flag = True\n",
        "        gen_path = \"\"\n",
        "        cost = 0.0  # Initialize cost\n",
        "\n",
        "        # Define pricing for OpenAI models (per 1M tokens)\n",
        "        INPUT_COST_PER_TOKEN_GPT_IMAGE_1 = 10.00 / 1_000_000\n",
        "        OUTPUT_COST_PER_TOKEN_GPT_IMAGE_1 = 40.00 / 1_000_000\n",
        "        INPUT_COST_PER_TOKEN_GPT_IMAGE_1_MINI = 2.50 / 1_000_000\n",
        "        OUTPUT_COST_PER_TOKEN_GPT_IMAGE_1_MINI = 8.00 / 1_000_000\n",
        "\n",
        "        # Output image token consumption based on quality and size\n",
        "        output_token_consumption_map = {\n",
        "            (\"1024x1024\", \"low\"): 272,\n",
        "            (\"1024x1024\", \"medium\"): 1056,\n",
        "            (\"1024x1024\", \"high\"): 4160,\n",
        "            (\"1024x1536\", \"low\"): 408,\n",
        "            (\"1024x1536\", \"medium\"): 1584,\n",
        "            (\"1024x1536\", \"high\"): 6240,\n",
        "            (\"1536x1024\", \"low\"): 400,\n",
        "            (\"1536x1024\", \"medium\"): 1568,\n",
        "            (\"1536x1024\", \"high\"): 6208,\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Calculate input prompt tokens using tiktoken\n",
        "            encoding = tiktoken.encoding_for_model(\"gpt-4\") # Using gpt-4 tokenizer as a general tokenizer\n",
        "            input_tokens = len(encoding.encode(single_prompt))\n",
        "\n",
        "            # Determine input and output token costs per token based on model_name\n",
        "            if \"gpt-image-1\" in model_name.lower():\n",
        "                input_cost_per_token = INPUT_COST_PER_TOKEN_GPT_IMAGE_1\n",
        "                output_cost_per_token = OUTPUT_COST_PER_TOKEN_GPT_IMAGE_1\n",
        "            elif \"gpt-image-1-mini\" in model_name.lower():\n",
        "                input_cost_per_token = INPUT_COST_PER_TOKEN_GPT_IMAGE_1_MINI\n",
        "                output_cost_per_token = OUTPUT_COST_PER_TOKEN_GPT_IMAGE_1_MINI\n",
        "            else:\n",
        "                input_cost_per_token = 0.0\n",
        "                output_cost_per_token = 0.0\n",
        "                logger.warning(f\"Unknown OpenAI model '{model_name}'. Cost will be 0.\")\n",
        "\n",
        "            # Calculate input prompt cost\n",
        "            cost += input_tokens * input_cost_per_token\n",
        "\n",
        "            with open(input_image_path, \"rb\") as img_file:\n",
        "                response = client.images.edit(\n",
        "                    model=model_name,\n",
        "                    image=img_file,\n",
        "                    prompt=single_prompt,\n",
        "                    size=size,\n",
        "                    quality=quality,\n",
        "                    n=n,\n",
        "                )\n",
        "\n",
        "            if response.data and response.data[0].b64_json:\n",
        "                image_bytes = BytesIO(\n",
        "                    base64.b64decode(response.data[0].b64_json)\n",
        "                )\n",
        "                image_pil = Image.open(image_bytes)\n",
        "\n",
        "                timestamp = int(time.time() * 1000)\n",
        "                filename = f\"openai_edited_{timestamp}.png\"\n",
        "                gen_path = os.path.join(output_dir, filename)\n",
        "                image_pil.save(gen_path)\n",
        "\n",
        "                # Calculate output image token cost based on quality and size\n",
        "                # Default to medium quality token cost if 'auto' or not found\n",
        "                output_tokens_per_image = output_token_consumption_map.get(\n",
        "                    (size, quality.lower()), output_token_consumption_map.get((\"1024x1024\", \"medium\"))\n",
        "                )\n",
        "                if output_tokens_per_image is None:\n",
        "                    logger.warning(f\"Could not determine output token count for size '{size}' and quality '{quality}'. Using default medium (1024x1024) tokens.\")\n",
        "                    output_tokens_per_image = output_token_consumption_map[('1024x1024', 'medium')]\n",
        "\n",
        "                cost += output_tokens_per_image * output_cost_per_token\n",
        "\n",
        "                logger.info(\n",
        "                    f\"------------------- \\\"{gen_path}\\\" generated! \"\n",
        "                    f\"(Success: {gen_img_flag}) -------------------\"\n",
        "                )\n",
        "            else:\n",
        "                raise RuntimeError(\"No image data returned from OpenAI.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            gen_img_flag = False\n",
        "            cost = 0.0  # No cost for failed generation or placeholder\n",
        "            logger.exception(\n",
        "                f\"Error editing image with OpenAI for prompt '{single_prompt}': {e}\"\n",
        "            )\n",
        "            gen_path = _create_placeholder_image(\n",
        "                prompt=single_prompt,\n",
        "                output_dir=output_dir,\n",
        "                filename_prefix=\"openai_generated\"\n",
        "            )\n",
        "\n",
        "        return gen_img_flag, gen_path, cost\n",
        "\n",
        "    if isinstance(prompt, str):\n",
        "        return generate_single(prompt)\n",
        "    elif isinstance(prompt, list):\n",
        "        return [generate_single(p) for p in prompt]\n",
        "    else:\n",
        "        raise TypeError(\"Prompt must be str or list[str].\")\n",
        "\n",
        "\n",
        "def generate_image_seedream(\n",
        "    input_image_path: str,\n",
        "    prompt: Union[str, List[str]],\n",
        "    output_dir: str,\n",
        "    api_key: str, # This should be ARK_API_KEY from the example\n",
        "    model_name: str = \"seedream-4-5-251128\", # Default model from user's example\n",
        "    base_url: str = \"https://ark.ap-southeast.bytepluses.com/api/v3\", # Default base_url from user's example\n",
        "    size: str = \"2K\", # Default size from user's example\n",
        "    watermark: bool = False, # Default watermark from user's example\n",
        ") -> Union[Tuple[bool, str, float], List[Tuple[bool, str, float]]]:\n",
        "    \"\"\"\n",
        "    Generate an image using the BytePlus SeeDream API via the OpenAI client,\n",
        "    following the image-to-image pattern provided in the example.\n",
        "\n",
        "    Args:\n",
        "        input_image_path (str): Path to the original image.\n",
        "        prompt (str | list[str]): Text instruction(s) for image generation/editing.\n",
        "        output_dir (str): Directory to store generated images.\n",
        "        api_key (str): BytePlus ARK API key.\n",
        "        model_name (str): SeeDream model name (e.g., \\\"seedream-4-5-251128\\\").\n",
        "        base_url (str): Base URL for the BytePlus ARK API.\n",
        "        size (str): Output image size (e.g., \\\"2K\\\").\n",
        "        watermark (bool): Whether to add a watermark (False by default).\n",
        "\n",
        "    Returns:\n",
        "        tuple[bool, str, float] | list[tuple[bool, str, float]]:\n",
        "            - bool: True if generated by AI, False if placeholder was used\n",
        "            - str: Path to the generated image\n",
        "            - float: The estimated cost of generation.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If BytePlus ARK API key is missing.\n",
        "        FileNotFoundError: If the input image is not found.\n",
        "        TypeError: If prompt is not str or list[str].\n",
        "    \"\"\"\n",
        "\n",
        "    if not api_key:\n",
        "        raise ValueError(\"BytePlus ARK API key is required.\")\n",
        "\n",
        "    if not os.path.exists(input_image_path):\n",
        "        raise FileNotFoundError(f\"Input image not found: {input_image_path}\")\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Initialize OpenAI client with BytePlus ARK endpoint and API key\n",
        "    client = OpenAI(base_url=base_url, api_key=api_key)\n",
        "\n",
        "    # Read input image and convert to base64 data URI\n",
        "    with open(input_image_path, \"rb\") as image_file:\n",
        "        encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "        mime_type = \"image/jpeg\"\n",
        "        if input_image_path.lower().endswith(\".png\"):\n",
        "            mime_type = \"image/png\"\n",
        "        input_image_data_uri = f\"data:{mime_type};base64,{encoded_string}\"\n",
        "\n",
        "    def _generate_single_see_dream(single_prompt: str) -> Tuple[bool, str, float]:\n",
        "        gen_img_flag = True\n",
        "        gen_path = \"\"\n",
        "        cost = 0.0  # Initialize cost\n",
        "\n",
        "        # Define pricing for SeeDream models per image\n",
        "        price_seedream_4_5 = 0.04\n",
        "        price_seedream_4_0 = 0.03\n",
        "\n",
        "        try:\n",
        "            images_response = client.images.generate(\n",
        "                model=model_name,\n",
        "                prompt=single_prompt,\n",
        "                size=size,\n",
        "                response_format=\"url\", # The example shows 'url' as response_format\n",
        "                extra_body={\n",
        "                    \"image\": input_image_data_uri, # Input image as base64 data URI\n",
        "                    \"watermark\": watermark,\n",
        "                }\n",
        "            )\n",
        "\n",
        "            if images_response.data and images_response.data[0].url:\n",
        "                image_url = images_response.data[0].url\n",
        "                logger.info(f\"BytePlus SeeDream generated image URL: {image_url}\")\n",
        "\n",
        "                # Download the image from the URL\n",
        "                response = requests.get(image_url)\n",
        "                response.raise_for_status() # Raise an exception for HTTP errors (4xx or 5xx)\n",
        "\n",
        "                image_bytes = BytesIO(response.content)\n",
        "                image_pil = Image.open(image_bytes)\n",
        "\n",
        "                timestamp = int(time.time() * 1000)\n",
        "                filename = f\"seedream_generated_{timestamp}.png\"\n",
        "                gen_path = os.path.join(output_dir, filename)\n",
        "                image_pil.save(gen_path)\n",
        "\n",
        "                # Assign cost based on model name for successful generation\n",
        "                if model_name == \"seedream-4-5-251128\":\n",
        "                    cost = price_seedream_4_5\n",
        "                elif model_name == \"seedream-4-0-250828\":\n",
        "                    cost = price_seedream_4_0\n",
        "                else:\n",
        "                    logger.warning(f\"Unknown SeeDream model '{model_name}'. Cost will be 0.\")\n",
        "\n",
        "                logger.info(\n",
        "                    f\"------------------- \\\"{gen_path}\\\" generated by SeeDream! \"\n",
        "                    f\"(Success: {gen_img_flag}) -------------------\"\n",
        "                )\n",
        "            else:\n",
        "                raise RuntimeError(\"No image URL returned from BytePlus SeeDream API response.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            gen_img_flag = False\n",
        "            cost = 0.0  # No cost for failed generation or placeholder\n",
        "            logger.exception(\n",
        "                f\"Error generating image with BytePlus SeeDream for prompt '{single_prompt}': {e}\"\n",
        "            )\n",
        "            # Use the placeholder image function if generation fails\n",
        "            gen_path = _create_placeholder_image(\n",
        "                prompt=single_prompt,\n",
        "                output_dir=output_dir,\n",
        "                filename_prefix=\"seedream_generated\"\n",
        "            )\n",
        "\n",
        "        return gen_img_flag, gen_path, cost\n",
        "\n",
        "    if isinstance(prompt, str):\n",
        "        return _generate_single_see_dream(prompt)\n",
        "    elif isinstance(prompt, list):\n",
        "        return [_generate_single_see_dream(p) for p in prompt]\n",
        "    else:\n",
        "        raise TypeError(\"Prompt must be str or list[str].\")\n",
        "\n",
        "\n",
        "def _get_deeplab_mask(\n",
        "    image: Image.Image,\n",
        "    model: Any,\n",
        "    weights: Any\n",
        ") -> Image.Image:\n",
        "    \"\"\"\n",
        "    Helper function to generate a binary mask using a pre-loaded DeepLabV3 model.\n",
        "    Masks the foreground (objects) as White (255) and background as Black (0).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Move model to device if not already there\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        # Check if model is already on the correct device to avoid overhead\n",
        "        if next(model.parameters()).device.type != device:\n",
        "            model.to(device)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        # Preprocess using the provided weights transforms\n",
        "        preprocess = weights.transforms()\n",
        "        input_tensor = preprocess(image).unsqueeze(0)\n",
        "\n",
        "        input_tensor = input_tensor.to(device)\n",
        "\n",
        "        # Inference\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor)['out'][0]\n",
        "\n",
        "        # Output is (21, H, W). 0 is background.\n",
        "        output_predictions = output.argmax(0)\n",
        "\n",
        "        # Create mask: 1 where class != 0 (background), else 0\n",
        "        mask_arr = (output_predictions != 0).byte().cpu().numpy() * 255\n",
        "\n",
        "        # Convert to PIL\n",
        "        mask_image = Image.fromarray(mask_arr).convert(\"L\")\n",
        "\n",
        "        # Resize mask back to original image size if transforms changed it\n",
        "        if mask_image.size != image.size:\n",
        "            mask_image = mask_image.resize(image.size, resample=Image.NEAREST)\n",
        "\n",
        "        return mask_image\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to generate mask with DeepLabV3: {e}\")\n",
        "        # Return a blank white mask (edit everything) as fallback\n",
        "        return Image.new(\"L\", image.size, 255)\n",
        "\n",
        "\n",
        "def generate_image_huggingface(\n",
        "    input_image_path: str,\n",
        "    prompt: str,\n",
        "    pipe: Any,\n",
        "    output_dir: str = \"outputs\",\n",
        "    mask_model: Optional[Any] = None,\n",
        "    mask_model_weights: Optional[Any] = None\n",
        ") -> Tuple[bool, str, float]:\n",
        "    \"\"\"\n",
        "    Generates an edited image using a HuggingFace Diffusers pipeline.\n",
        "\n",
        "    Args:\n",
        "        input_image_path (str): Path to the original input image.\n",
        "        prompt (str): Text instruction describing how the input image should be modified.\n",
        "        pipe (Any): Pre-initialized HuggingFace Diffusers pipeline object (e.g., StableDiffusionInstructPix2PixPipeline).\n",
        "        output_dir (str): Directory where the generated image will be saved.\n",
        "        mask_model (Optional[Any]): An optional model that takes a PIL Image and returns a PIL Image mask.\n",
        "        mask_model_weights (Optional[Any]): Weights/Transforms associated with the mask_model.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[bool, str, float]: A tuple containing:\n",
        "            - gen_img_flag (bool): True if the image was successfully generated, False otherwise.\n",
        "            - gen_path (str): Path to the generated image or placeholder image.\n",
        "            - cost (float): The estimated cost of generation (0.0 for local models).\n",
        "    \"\"\"\n",
        "    gen_img_flag = True\n",
        "    gen_path = \"\"\n",
        "    cost = 0.0  # Initialize cost for local models\n",
        "\n",
        "    if not os.path.exists(input_image_path):\n",
        "        raise FileNotFoundError(f\"Input image not found: {input_image_path}\")\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    image = Image.open(input_image_path)\n",
        "    image = ImageOps.exif_transpose(image)\n",
        "    image = image.convert(\"RGB\")\n",
        "\n",
        "    mask_image = None\n",
        "    if mask_model is not None:\n",
        "        mask_image = _get_deeplab_mask(image, mask_model, mask_model_weights)\n",
        "\n",
        "    try:\n",
        "        if isinstance(pipe, StableDiffusionInpaintPipeline):\n",
        "            if mask_model is None or mask_model_weights is None:\n",
        "                raise ValueError(\"mask_model and mask_model_weights are required for Inpainting pipelines.\")\n",
        "            images = pipe(\n",
        "                prompt=prompt,\n",
        "                image=image,\n",
        "                mask_image=mask_image,\n",
        "                num_inference_steps=50,\n",
        "            ).images\n",
        "        elif isinstance(pipe, StableDiffusionInstructPix2PixPipeline):\n",
        "            images = pipe(\n",
        "                prompt,\n",
        "                image=image,\n",
        "                num_inference_steps=10,\n",
        "                image_guidance_scale=1\n",
        "            ).images\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported HuggingFace pipeline type: {type(pipe)}\")\n",
        "\n",
        "        image_pil = images[0]\n",
        "\n",
        "        timestamp = int(time.time() * 1000)\n",
        "        filename = f\"huggingface_generated_{timestamp}.png\"\n",
        "        gen_path = os.path.join(output_dir, filename)\n",
        "        image_pil.save(gen_path)\n",
        "        logger.info(\n",
        "            f\"------------------- \\\"{gen_path}\\\" generated! \"\n",
        "            f\"(Success: {gen_img_flag}) -------------------\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        gen_img_flag = False\n",
        "        cost = 0.0  # No cost for failed generation or placeholder\n",
        "        logger.exception(\n",
        "            f\"Error generating image with HuggingFace pipeline for prompt '{prompt}': {e}\"\n",
        "        )\n",
        "        # Use the placeholder image function if generation fails\n",
        "        gen_path = _create_placeholder_image(\n",
        "            prompt=prompt,\n",
        "            output_dir=output_dir,\n",
        "            filename_prefix=\"huggingface_generated\"\n",
        "        )\n",
        "\n",
        "    return gen_img_flag, gen_path, cost\n",
        "\n",
        "\n",
        "def _generate_single_image(\n",
        "    input_image_path: str,\n",
        "    prompt: str,\n",
        "    model_name: str,\n",
        "    output_dir: str = \"outputs\",\n",
        "    low_threshold: Optional[int] = None,\n",
        "    high_threshold: Optional[int] = None,\n",
        "    seed: Optional[int] = None,\n",
        "    api_key: Optional[str] = None,\n",
        "    openai_size: str = \"1024x1024\",\n",
        "    openai_quality: str = \"auto\",\n",
        "    openai_n: int = 1,\n",
        "    seedream_size: str = \"2K\",\n",
        "    seedream_watermark: bool = False,\n",
        "    huggingface_pipe: Optional[Any] = None,\n",
        "    mask_model: Optional[Any] = None,\n",
        "    mask_model_weights: Optional[Any] = None\n",
        ") -> Tuple[bool, str, float]:\n",
        "    \"\"\"Generate a single edited image using a specified image model.\n",
        "\n",
        "    This function dispatches the image generation request based on the\n",
        "    selected model:\n",
        "\n",
        "    - **Gemini models**: Use remote image editing with image + prompt.\n",
        "    - **OpenAI models**: Use OpenAI image editing (image + prompt).\n",
        "    - **BytePlus SeeDream models**: Use BytePlus SeeDream image editing (image + prompt).\n",
        "    - **HuggingFace InstructPix2Pix**: Uses a local pre-initialized pipeline.\n",
        "    - **Local models**: Use paired inference (e.g., Pix2Pix, ControlNet).\n",
        "\n",
        "    Args:\n",
        "        input_image_path (str):\n",
        "            Path to the original input image used as the editing source.\n",
        "        prompt (str):\n",
        "            Text instruction describing how the input image should be\n",
        "            modified.\n",
        "        model_name (str):\n",
        "            Name of the image generation or editing model. This value\n",
        "            determines which backend (Gemini, OpenAI, or local) is used.\n",
        "        output_dir (str, optional):\n",
        "            Directory where the generated image will be saved.\n",
        "            Defaults to ``\"outputs\"``.\n",
        "        low_threshold (Optional[int]):\n",
        "            Lower Canny edge threshold used by local paired-inference\n",
        "            models. Ignored for Gemini and OpenAI models.\n",
        "        high_threshold (Optional[int]):\n",
        "            Upper Canny edge threshold used by local paired-inference\n",
        "            models. Ignored for Gemini and OpenAI models.\n",
        "        seed (Optional[int]):\n",
        "            Random seed for reproducibility. Passed through to all\n",
        "            backends that support deterministic generation.\n",
        "        api_key (Optional[str]):\n",
        "            API key required for remote models (Gemini or OpenAI).\n",
        "            Must be provided when using those backends.\n",
        "        openai_size (str):\n",
        "            Output image resolution for OpenAI image editing\n",
        "            (e.g., ``\"1024x1024\"``, ``\"512x512\"``).\n",
        "        openai_quality (str):\n",
        "            Quality setting for OpenAI image generation\n",
        "            (e.g., ``\"low\"``, ``\"medium\"``).\n",
        "        openai_n (int):\n",
        "            Number of images to generate using OpenAI models.\n",
        "            Most OpenAI image-editing models currently support only ``1``.\n",
        "        seedream_size (str):\n",
        "            Output image resolution for OpenAI image editing\n",
        "            (e.g., ``\"2K\"``).\n",
        "        seedream_watermark (bool):\n",
        "            Whether to add a watermark to images generated by SeeDream models.\n",
        "        huggingface_pipe (Optional[Any]): Pre-initialized HuggingFace pipe for\n",
        "            models like `instruct-pix2pix`.\n",
        "        mask_model (Optional[Any]): An optional model that takes a PIL Image\n",
        "            and returns a PIL Image mask.\n",
        "        mask_model_weights (Optional[Any]): Weights/Transforms associated with\n",
        "            the mask_model.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[bool, str, float]: # Updated return type\n",
        "            A tuple containing:\n",
        "            - **bool**: Indicates whether the image was successfully\n",
        "              generated by the model (``True``)\n",
        "              or a placeholder image was used (``False``).\n",
        "            - **str**: File path to the generated (or placeholder) image.\n",
        "            - **float**: The estimated cost of generation.\n",
        "\n",
        "    Raises:\n",
        "        ValueError:\n",
        "            If an API key is required but not provided.\n",
        "        FileNotFoundError:\n",
        "            If no output image is produced by a local model.\n",
        "        RuntimeError:\n",
        "            If an unexpected error occurs during image generation.\n",
        "    \"\"\"\n",
        "    model_name_lc = model_name.lower()\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Gemini (image + prompt)\n",
        "    # --------------------------------------------------\n",
        "    if \"gemini\" in model_name_lc:\n",
        "        if api_key is None:\n",
        "            raise ValueError(\"API key is required for Gemini models.\")\n",
        "\n",
        "        gen_img_flag, gen_path, cost = generate_image_gemini(\n",
        "            input_image_path=input_image_path,\n",
        "            prompt=prompt,\n",
        "            output_dir=output_dir,\n",
        "            api_key=api_key,\n",
        "            model_name=model_name,\n",
        "            seed=seed,\n",
        "        )\n",
        "        return gen_img_flag, gen_path, cost\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # OpenAI (image + prompt editing)\n",
        "    # --------------------------------------------------\n",
        "    if any(key in model_name_lc for key in (\"openai\", \"dall-e\", \"gpt-image\")):\n",
        "        if api_key is None:\n",
        "            raise ValueError(\"API key is required for OpenAI models.\")\n",
        "\n",
        "        gen_img_flag, gen_path, cost = generate_image_openai(\n",
        "            input_image_path=input_image_path,\n",
        "            prompt=prompt,\n",
        "            output_dir=output_dir,\n",
        "            api_key=api_key,\n",
        "            model_name=model_name,\n",
        "            size=openai_size,\n",
        "            quality=openai_quality,\n",
        "            n=openai_n,\n",
        "        )\n",
        "        return gen_img_flag, gen_path, cost\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # BytePlus SeeDream (image + prompt editing)\n",
        "    # --------------------------------------------------\n",
        "    if \"seedream\" in model_name_lc:\n",
        "        if api_key is None:\n",
        "            raise ValueError(\"API key is required for BytePlus SeeDream models.\")\n",
        "\n",
        "        # Validate specific SeeDream models as requested\n",
        "        if model_name not in [\"seedream-4-5-251128\", \"seedream-4-0-250828\"]:\n",
        "            raise ValueError(\n",
        "                f\"Unsupported SeeDream model: {model_name}. \"\n",
        "                \"Please use 'seedream-4-5-251128' or 'seedream-4-0-250828'.\"\n",
        "            )\n",
        "\n",
        "        gen_img_flag, gen_path, cost = generate_image_seedream(\n",
        "            input_image_path=input_image_path,\n",
        "            prompt=prompt,\n",
        "            output_dir=output_dir,\n",
        "            api_key=api_key,\n",
        "            model_name=model_name,\n",
        "            size=seedream_size,\n",
        "            watermark=seedream_watermark,\n",
        "        )\n",
        "        return gen_img_flag, gen_path, cost\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # HuggingFace models (local pipeline)\n",
        "    # --------------------------------------------------\n",
        "    if any(key in model_name_lc for key in (\"instruct-pix2pix\", \"stable-diffusion-2-inpainting\")) and huggingface_pipe is not None:\n",
        "        gen_img_flag, gen_path, cost = generate_image_huggingface(\n",
        "            input_image_path=input_image_path,\n",
        "            prompt=prompt,\n",
        "            pipe=huggingface_pipe,\n",
        "            output_dir=output_dir,\n",
        "            mask_model=mask_model,\n",
        "            mask_model_weights=mask_model_weights,\n",
        "        )\n",
        "        return gen_img_flag, gen_path, cost\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Local paired inference (fallback for other local models)\n",
        "    # --------------------------------------------------\n",
        "    try:\n",
        "        gen_img_flag = True\n",
        "        cost = 0.0  # Initialize cost for local models\n",
        "        run_inference_paired(\n",
        "            model_name=model_name,\n",
        "            input_image=input_image_path,\n",
        "            prompt=prompt,\n",
        "            output_dir=output_dir,\n",
        "            low_threshold=low_threshold,\n",
        "            high_threshold=high_threshold,\n",
        "            seed=seed,\n",
        "        )\n",
        "\n",
        "        files = [\n",
        "            os.path.join(output_dir, fname)\n",
        "            for fname in os.listdir(output_dir)\n",
        "            if os.path.isfile(os.path.join(output_dir, fname))\n",
        "        ]\n",
        "\n",
        "        if not files:\n",
        "            raise FileNotFoundError(\n",
        "                f\"No files found in directory: {output_dir}\"\n",
        "            )\n",
        "\n",
        "        generated_file = max(files, key=os.path.getctime)\n",
        "        path = Path(generated_file)\n",
        "        timestamp = int(time.time() * 1000)\n",
        "\n",
        "        gen_path = str(path.rename(\n",
        "            path.with_name(f\"img2img-turbo_generated_{timestamp}{path.suffix}\")\n",
        "        ))\n",
        "\n",
        "        return gen_img_flag, gen_path, cost\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        raise\n",
        "    except Exception as exc:\n",
        "        raise RuntimeError(\n",
        "            f\"Unexpected error during image generation: {exc}\"\n",
        "        ) from exc\n",
        "\n",
        "\n",
        "def submit_gemini_batch_job(\n",
        "    api_key: str,\n",
        "    image_path: str,\n",
        "    text_list: List[str],\n",
        "    model_name: str = \"gemini-2.5-flash-image\",\n",
        "    temperature: float = 1.0,\n",
        "    top_p: float = 0.95,\n",
        "    top_k: int = 40,\n",
        "    max_output_tokens: int = 8192,\n",
        "    seed: Optional[int] = None,\n",
        "    response_mime_type: str = \"text/plain\"\n",
        ") -> str:\n",
        "    \"\"\"Submits a batch image generation job to the Gemini API.\n",
        "\n",
        "    Args:\n",
        "        api_key (str): Gemini API Key.\n",
        "        image_path (str): Path to the source image.\n",
        "        text_list (List[str]): List of text prompts.\n",
        "        model_name (str): Model name.\n",
        "        temperature (float): Sampling temperature.\n",
        "        top_p (float): Nucleus sampling probability.\n",
        "        top_k (int): Top-k sampling parameter.\n",
        "        max_output_tokens (int): Maximum output tokens.\n",
        "        seed (int | None): Random seed for reproducibility.\n",
        "        response_mime_type (str): MIME type for the response.\n",
        "\n",
        "    Returns:\n",
        "        str: The unique resource name of the submitted batch job.\n",
        "    \"\"\"\n",
        "    # TODO: use context manager for the client\n",
        "    client = genai.Client(api_key=api_key)\n",
        "\n",
        "    logger.info(f\"Uploading image file: {image_path}\")\n",
        "    image_file = client.files.upload(file=image_path)\n",
        "    logger.info(f\"Uploaded image file: {image_file.name} (MIME: {image_file.mime_type})\")\n",
        "\n",
        "    # Define list of requests\n",
        "    requests_data = []\n",
        "\n",
        "    for ix, text in enumerate(text_list):\n",
        "        custom_id = f\"request_{ix}_image\"\n",
        "\n",
        "        # Build generation_config dictionary dynamically\n",
        "        gen_config_dict = {\n",
        "            \"temperature\": temperature,\n",
        "            \"top_p\": top_p,\n",
        "            \"top_k\": top_k,\n",
        "            \"max_output_tokens\": max_output_tokens,\n",
        "            \"response_mime_type\": response_mime_type\n",
        "        }\n",
        "        if seed is not None:\n",
        "            gen_config_dict[\"seed\"] = seed\n",
        "\n",
        "        requests_data.append({\n",
        "            \"custom_id\": custom_id,\n",
        "            \"request\": {\n",
        "                \"contents\": [\n",
        "                    {\n",
        "                        \"parts\": [\n",
        "                            {\"text\": text},\n",
        "                            {\n",
        "                                \"file_data\": {\n",
        "                                    \"file_uri\": image_file.uri,\n",
        "                                    \"mime_type\": image_file.mime_type\n",
        "                                }\n",
        "                            }\n",
        "                        ]\n",
        "                    }\n",
        "                ],\n",
        "                \"generation_config\": gen_config_dict\n",
        "            }\n",
        "        })\n",
        "\n",
        "    json_file_path = 'batch_image_gen_requests.json'\n",
        "\n",
        "    logger.info(f\"\\nCreating JSONL file: {json_file_path}\")\n",
        "    with open(json_file_path, 'w') as f:\n",
        "        for req in requests_data:\n",
        "            f.write(json.dumps(req) + '\\n')\n",
        "\n",
        "    logger.info(f\"Uploading JSONL file: {json_file_path}\")\n",
        "    batch_input_file = client.files.upload(file=json_file_path)\n",
        "    logger.info(f\"Uploaded JSONL file: {batch_input_file.name}\")\n",
        "\n",
        "    logger.info(\"\\nCreating batch job...\")\n",
        "    batch_multimodal_job = client.batches.create(\n",
        "        model=model_name,\n",
        "        src=batch_input_file.name,\n",
        "        config={\n",
        "            'display_name': 'my-batch-image-gen-job',\n",
        "        }\n",
        "    )\n",
        "    logger.info(f\"Created batch job: {batch_multimodal_job.name}\")\n",
        "\n",
        "    return batch_multimodal_job.name\n",
        "\n",
        "\n",
        "def retrieve_gemini_batch_results(\n",
        "    api_key: str,\n",
        "    job_name: str,\n",
        "    text_list: List[str],\n",
        "    model_name: str,\n",
        "    output_dir: str = \"outputs\"\n",
        ") -> Tuple[List[Tuple[bool, str]], float]:\n",
        "    \"\"\"Polls for completion of a Gemini batch job and saves the results.\n",
        "\n",
        "    Args:\n",
        "        api_key (str): Gemini API Key.\n",
        "        job_name (str): The unique resource name of the batch job to check.\n",
        "        text_list (List[str]): Original list of prompts (needed for order/fallbacks).\n",
        "        model_name (str): The Gemini model name used for batch generation.\n",
        "        output_dir (str): Directory to save outputs.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[Tuple[bool, str]], float]: A tuple containing:\n",
        "            - List[Tuple[bool, str]]: A list of tuples (generation_flag, image_path).\n",
        "                - generation_flag: True if generated by AI, False if placeholder.\n",
        "                - image_path: Path to the saved image file.\n",
        "            - float: The total estimated cost for the entire batch generation.\n",
        "    \"\"\"\n",
        "    # TODO: use context manager for the client\n",
        "    client = genai.Client(api_key=api_key)\n",
        "\n",
        "    logger.info(f\"Polling status for job: {job_name}\")\n",
        "    logger.info(\"Waiting for job to complete (this may take a moment)...\")\n",
        "\n",
        "    # Define batch pricing (half of standard Gemini pricing)\n",
        "    batch_input_cost_2_5_flash = 0.0005 / 2\n",
        "    batch_output_cost_2_5_flash = 0.002 / 2\n",
        "    batch_input_cost_3_pro = 0.001 / 2\n",
        "    batch_output_cost_3_pro = 0.004 / 2\n",
        "\n",
        "    # Polling loop to wait for the job to finish\n",
        "    while True:\n",
        "        batch_multimodal_job = client.batches.get(name=job_name)\n",
        "        state = batch_multimodal_job.state.name\n",
        "\n",
        "        if state in ['JOB_STATE_SUCCEEDED', 'JOB_STATE_FAILED', 'JOB_STATE_CANCELLED']:\n",
        "            logger.info(f\"\\nJob finished with state: {state}\")\n",
        "            break\n",
        "\n",
        "        time.sleep(30)  # Wait 30 seconds before checking again\n",
        "        print(\".\", end=\"\", flush=True)\n",
        "\n",
        "    # Dictionary to store results: {custom_id: (flag, path)}\n",
        "    processed_results = {}\n",
        "    total_cost = 0.0\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    if batch_multimodal_job.state.name == 'JOB_STATE_SUCCEEDED':\n",
        "        result_file_name = batch_multimodal_job.dest.file_name\n",
        "        logger.info(f\"Results available in file: {result_file_name}\")\n",
        "\n",
        "        logger.info(\"Downloading and parsing result file content...\")\n",
        "        file_content_bytes = client.files.download(file=result_file_name)\n",
        "        file_content = file_content_bytes.decode('utf-8')\n",
        "\n",
        "        # Determine the cost per item for the specified model in batch mode\n",
        "        item_input_cost = 0.0\n",
        "        item_output_cost = 0.0\n",
        "        if model_name == \"gemini-2.5-flash-image\":\n",
        "            item_input_cost = batch_input_cost_2_5_flash\n",
        "            item_output_cost = batch_output_cost_2_5_flash\n",
        "        elif model_name == \"gemini-3-pro-image-preview\":\n",
        "            item_input_cost = batch_input_cost_3_pro\n",
        "            item_output_cost = batch_output_cost_3_pro\n",
        "        else:\n",
        "            logger.warning(f\"Unknown Gemini model '{model_name}'. Cost will be 0.\")\n",
        "\n",
        "        # Parse each line of the JSONL result\n",
        "        for line in file_content.splitlines():\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            # The result file is also a JSONL file. Parse each line.\n",
        "            try:\n",
        "                parsed_response = json.loads(line)\n",
        "                custom_id = parsed_response.get('custom_id') or parsed_response.get('key')\n",
        "                current_item_cost = 0.0\n",
        "\n",
        "                # Check for successful response structure\n",
        "                if (\n",
        "                    'response' in parsed_response\n",
        "                    and 'candidates' in parsed_response['response']\n",
        "                    and parsed_response['response']['candidates']\n",
        "                ):\n",
        "                    candidates = parsed_response['response']['candidates'][0]\n",
        "                    found_image = False\n",
        "\n",
        "                    if 'content' in candidates and 'parts' in candidates['content']:\n",
        "                        for part in candidates['content']['parts']:\n",
        "                            if 'inlineData' in part:\n",
        "                                # Found an image!\n",
        "                                mime = part['inlineData']['mimeType']\n",
        "                                data = base64.b64decode(part['inlineData']['data'])\n",
        "\n",
        "                                # Determine extension\n",
        "                                ext = \".png\" if \"png\" in mime else \".jpg\"\n",
        "                                timestamp = int(time.time() * 1000)\n",
        "                                filename = f\"{custom_id}_{timestamp}{ext}\"\n",
        "                                save_path = os.path.join(output_dir, filename)\n",
        "\n",
        "                                # Save Image\n",
        "                                with open(save_path, \"wb\") as img_f:\n",
        "                                    img_f.write(data)\n",
        "\n",
        "                                current_item_cost = item_input_cost + item_output_cost\n",
        "                                processed_results[custom_id] = (True, save_path)\n",
        "                                found_image = True\n",
        "                                total_cost += current_item_cost  # Accumulate total cost\n",
        "                                break\n",
        "\n",
        "                    if not found_image:\n",
        "                        # API returned a candidate but no inlineData (likely text refusal or filter)\n",
        "                        logger.warning(f\"No image found in response for {custom_id}\")\n",
        "                        processed_results[custom_id] = (False, None)  # Placeholder, no path yet\n",
        "                else:\n",
        "                    logger.warning(f\"Invalid response structure for {custom_id}\")\n",
        "                    processed_results[custom_id] = (False, None)  # Placeholder, no path yet\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error parsing line: {e}\")\n",
        "                # If parsing fails, treat as failed generation with no cost\n",
        "                custom_id_from_error = line.split('\"custom_id\": \"')[1].split('\"')[0] if '\"custom_id\":' in line else f\"unknown_error_{time.time()}\"\n",
        "                processed_results[custom_id_from_error] = (False, None)\n",
        "    else:\n",
        "        logger.warning(f\"Job failed or was cancelled. Final state: {batch_multimodal_job.state.name}\")\n",
        "\n",
        "    # Final Compilation: Ensure return list matches input text_list order\n",
        "    final_output_list = []\n",
        "\n",
        "    for ix, text in enumerate(text_list):\n",
        "        custom_id = f\"request_{ix}_image\"\n",
        "\n",
        "        if custom_id in processed_results:\n",
        "            flag, path = processed_results[custom_id]\n",
        "            if path is None:  # If path wasn't set (e.g., API refusal),\n",
        "                placeholder_path = _create_placeholder_image(\n",
        "                    prompt=text,\n",
        "                    output_dir=output_dir,\n",
        "                    filename_prefix=custom_id\n",
        "                )\n",
        "                final_output_list.append((False, placeholder_path))\n",
        "            else:\n",
        "                final_output_list.append((flag, path))\n",
        "        else:\n",
        "            logger.warning(f\"Generating placeholder for missing result: {custom_id}\")\n",
        "            placeholder_path = _create_placeholder_image(\n",
        "                prompt=text,\n",
        "                output_dir=output_dir,\n",
        "                filename_prefix=custom_id\n",
        "            )\n",
        "            final_output_list.append((False, placeholder_path))\n",
        "\n",
        "    return final_output_list, total_cost\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66oJ7f2EBCHC"
      },
      "source": [
        "# Generate image from prompts and calculate wasserstein distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJBDv1ZsBDpg"
      },
      "outputs": [],
      "source": [
        "def generate_images_from_prompts(\n",
        "    input_image_path: str,\n",
        "    prompts: List[str],\n",
        "    model_name: str,\n",
        "    output_dir: str = \"outputs\",\n",
        "    low_threshold: Optional[int] = None,\n",
        "    high_threshold: Optional[int] = None,\n",
        "    seed: Optional[int] = None,\n",
        "    api_key: Optional[str] = None,\n",
        "    openai_size: str = \"1024x1024\",\n",
        "    openai_quality: str = \"auto\",\n",
        "    openai_n: int = 1,\n",
        "    seedream_size: str = \"2K\",\n",
        "    seedream_watermark: bool = False,\n",
        "    huggingface_pipe: Optional[Any] = None,\n",
        "    mask_model: Optional[Any] = None,\n",
        "    mask_model_weights: Optional[Any] = None\n",
        ") -> Tuple[List[Tuple[bool, str]], float]:\n",
        "    \"\"\"Generate multiple images from a list of text prompts.\n",
        "\n",
        "    Args:\n",
        "        input_image_path (str): Path to the original input image.\n",
        "        prompts (List[str]): List of text prompts.\n",
        "        model_name (str): Name of the image generation model.\n",
        "        output_dir (Optional[str]): Directory for saving outputs.\n",
        "        low_threshold (Optional[int]): Canny low threshold value.\n",
        "        high_threshold (Optional[int]): Canny high threshold value.\n",
        "        seed (Optional[int]): Random seed for reproducibility.\n",
        "        api_key (Optional[str]): API key for remote models.\n",
        "        openai_size (str):\n",
        "            Output image resolution for OpenAI image editing\n",
        "            (e.g., ``\"1024x1024\"``, ``\"512x512\"``).\n",
        "        openai_quality (str):\n",
        "            Quality setting for OpenAI image generation\n",
        "            (e.g., ``\"low\"``, ``\"medium\"``).\n",
        "        openai_n (int):\n",
        "            Number of images to generate using OpenAI models.\n",
        "            Most OpenAI image-editing models currently support only ``1``.\n",
        "        seedream_size (str):\n",
        "            Output image resolution for OpenAI image editing\n",
        "            (e.g., ``\"2K\"``).\n",
        "        seedream_watermark (bool):\n",
        "            Whether to add a watermark to images generated by SeeDream models.\n",
        "        huggingface_pipe (Optional[Any]): Pre-initialized HuggingFace pipe for\n",
        "            models like `instruct-pix2pix`.\n",
        "        mask_model (Optional[Any]): An optional model that takes a PIL Image\n",
        "            and returns a PIL Image mask.\n",
        "        mask_model_weights (Optional[Any]): Weights/Transforms associated with\n",
        "            the mask_model.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[Tuple[bool, str]], float]: A tuple containing:\n",
        "            - List[Tuple[bool, str]]: Generation flags and image paths.\n",
        "            - float: The total estimated cost of all image generations.\n",
        "    \"\"\"\n",
        "    total_prompts = len(prompts)\n",
        "    generated_images_paths = []\n",
        "    total_cost = 0.0  # Initialize total cost\n",
        "\n",
        "    logger.info(\n",
        "        f\"Starting image generation: {total_prompts} prompts | model={model_name}\"\n",
        "    )\n",
        "\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    for idx, text in enumerate(prompts, start=1):\n",
        "        iter_start = time.perf_counter()\n",
        "\n",
        "        logger.debug(f\"Prompt {idx} text: {text}\")\n",
        "\n",
        "        gen_img_flag, gen_path, cost = _generate_single_image(\n",
        "            input_image_path=input_image_path,\n",
        "            prompt=text,\n",
        "            output_dir=output_dir,\n",
        "            model_name=model_name,\n",
        "            low_threshold=low_threshold,\n",
        "            high_threshold=high_threshold,\n",
        "            seed=seed,\n",
        "            api_key=api_key,\n",
        "            openai_size=openai_size,\n",
        "            openai_quality=openai_quality,\n",
        "            openai_n=openai_n,\n",
        "            seedream_size=seedream_size,\n",
        "            seedream_watermark=seedream_watermark,\n",
        "            huggingface_pipe=huggingface_pipe,\n",
        "            mask_model=mask_model,\n",
        "            mask_model_weights=mask_model_weights,\n",
        "        )\n",
        "\n",
        "        generated_images_paths.append((gen_img_flag, gen_path))\n",
        "        total_cost += cost\n",
        "\n",
        "        iter_duration = time.perf_counter() - iter_start\n",
        "        elapsed = time.perf_counter() - start_time\n",
        "        avg_time = elapsed / idx\n",
        "        eta = avg_time * (total_prompts - idx)\n",
        "\n",
        "        # Throttled logging for large batches\n",
        "        if idx % 5 == 0 or idx == total_prompts:\n",
        "            logger.info(\n",
        "                f\"Progress {idx}/{total_prompts} | \"\n",
        "                f\"Success={gen_img_flag} | \"\n",
        "                f\"Iter={iter_duration:.4f}s | \"\n",
        "                f\"ETA={eta:.4f}s | \"\n",
        "                f\"Cost={cost:.4f} | \"\n",
        "                f\"Total={total_cost:.4f}\"\n",
        "            )\n",
        "\n",
        "        if not gen_img_flag:\n",
        "            logger.warning(\n",
        "                f\"Image generation failed at {idx}/{total_prompts} \"\n",
        "                f\"(path={gen_path})\"\n",
        "            )\n",
        "\n",
        "    total_duration = time.perf_counter() - start_time\n",
        "    avg_duration = total_duration / max(total_prompts, 1)\n",
        "\n",
        "    logger.info(\n",
        "        f\"Image generation completed: {total_prompts} prompts | \"\n",
        "        f\"Total cost={total_cost:.4f} | \"\n",
        "        f\"Total time={total_duration:.4f}s | \"\n",
        "        f\"Avg/prompt={avg_duration:.4f}s\"\n",
        "    )\n",
        "\n",
        "    return generated_images_paths, total_cost\n",
        "\n",
        "\n",
        "def compute_wasserstein_distances(\n",
        "    input_image_path: str,\n",
        "    generated_images: List[Tuple[bool, str]],\n",
        "    embedding_processor: Any,\n",
        "    embedding_model: Any,\n",
        "    prompts: List[str],\n",
        "    display_image: bool = False,\n",
        "    output_dir: str = \"outputs\",\n",
        ") -> List[Tuple[str, float]]:\n",
        "    \"\"\"Compute Wasserstein distances between original and generated images.\n",
        "\n",
        "    Args:\n",
        "        input_image_path (str): Path to the original input image.\n",
        "        generated_images (List[Tuple[bool, str]]): Generated image info.\n",
        "        embedding_processor (Any): Preprocessing object for embeddings.\n",
        "        embedding_model (Any): Model used for embedding extraction.\n",
        "        prompts (List[str]): Prompts associated with generated images.\n",
        "        display_image (bool): Whether to display generated images.\n",
        "        output_dir (str): Directory to saving computed distances.\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[str, float]]: Image indices and distance values.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    orig_emb = extract_image_embedding(input_image_path, embedding_processor, embedding_model)\n",
        "\n",
        "    distances = []\n",
        "    for idx, ((gen_flag, img_path), text) in enumerate(zip(generated_images, prompts)):\n",
        "\n",
        "        if gen_flag is not None and not gen_flag:\n",
        "            dist = float('inf')\n",
        "        else:\n",
        "            gen_emb = extract_image_embedding(img_path, embedding_processor, embedding_model)\n",
        "\n",
        "            if gen_emb is None or orig_emb is None:\n",
        "                raise ValueError(\n",
        "                    \"One or both embeddings are None. Check embedding extraction.\"\n",
        "                )\n",
        "\n",
        "            gen_emb = np.asarray(gen_emb)\n",
        "            orig_emb = np.asarray(orig_emb)\n",
        "\n",
        "            if gen_emb.size == 0 or orig_emb.size == 0:\n",
        "                raise ValueError(\n",
        "                    \"Embeddings are empty. Cannot compute Wasserstein distance.\"\n",
        "                )\n",
        "\n",
        "            # Compute Wasserstein distance\n",
        "            dist = wasserstein_distance(gen_emb, orig_emb)\n",
        "\n",
        "        distances.append((str(idx), dist))\n",
        "        if display_image:\n",
        "            gen_img = Image.open(img_path)\n",
        "            # Display result\n",
        "            logger.info(\"Perturbation:\")\n",
        "            logger.info(f\"Perturbed Text: {text}\")\n",
        "            logger.info(f\"Wasserstein distance (generated vs orig): {dist}\")\n",
        "\n",
        "            plt.figure(figsize=(8, 8))\n",
        "            plt.imshow(gen_img)\n",
        "            plt.title(f\"Perturbed Text: {text}\", fontsize=12)\n",
        "            plt.axis(\"off\")\n",
        "            plt.show()\n",
        "\n",
        "    save_path = os.path.join(output_dir, \"WD_dists_generated_vs_orig.npy\")\n",
        "    np.save(save_path, np.array(distances))\n",
        "    logger.info(\"All generated embeddings and Wasserstein distances saved.\")\n",
        "\n",
        "    return distances\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNlfNdpgBGAN"
      },
      "source": [
        "# WMD-based similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kzh0R2wCBHaK"
      },
      "outputs": [],
      "source": [
        "def compute_wmd_scores(model, original: str, responses: list) -> list:\n",
        "    \"\"\"Compute safe WMD distances between prompt and perturbed outputs.\n",
        "\n",
        "    Args:\n",
        "        model: Word2Vec model.\n",
        "        original (str): prompt.\n",
        "        responses (list): List of perturbed texts.\n",
        "\n",
        "    Returns:\n",
        "        list: List of (perturbed_text, distance).\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    for text in responses:\n",
        "        dist = model.wmdistance(original, text)\n",
        "        scores.append((text, dist))\n",
        "\n",
        "    for text, dist in scores:\n",
        "        logger.info(\"Perturbed Text: %s\", text)\n",
        "        logger.info(\"Distance Score: %.4f\", dist)\n",
        "        logger.info(\"-\" * 50)\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "def normalize_similarities(\n",
        "    distances: list[tuple[str, float]],\n",
        "    mode: str = \"linear\"\n",
        ") -> list[tuple[str, float]]:\n",
        "    \"\"\"\n",
        "    Convert distance values into similarity scores in range [0, 1].\n",
        "\n",
        "    Supports two normalization strategies:\n",
        "\n",
        "    1. 'linear':\n",
        "        similarity = 1 - MinMax(distance)\n",
        "        Recommended for regression-based explainability:\n",
        "        Preserves linear proportionality and avoids non-linear distortion.\n",
        "\n",
        "    2. 'inverse':\n",
        "        similarity = MinMax(1 / (distance + ε))\n",
        "        Matches original pix2pix notebook.\n",
        "        Emphasizes small distances more aggressively (non-linear boost).\n",
        "\n",
        "    Args:\n",
        "        distances (list[tuple[str, float]]):\n",
        "            List of (text, distance) pairs.\n",
        "        mode (str):\n",
        "            Normalization mode. One of {\"linear\", \"inverse\"}.\n",
        "\n",
        "    Returns:\n",
        "        list[tuple[str, float]]:\n",
        "            List of (text, similarity) pairs.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If invalid mode is provided.\n",
        "    \"\"\"\n",
        "    if mode not in (\"linear\", \"inverse\"):\n",
        "        raise ValueError(\"mode must be one of: 'linear', 'inverse'\")\n",
        "\n",
        "    # Extract only the numeric distance values\n",
        "    dist_values = np.array([d for _, d in distances], dtype=float)\n",
        "\n",
        "    if len(dist_values) == 0:\n",
        "        logger.error(\"Distance list is empty. Cannot normalize similarities.\")\n",
        "        return []\n",
        "\n",
        "    # ==========================\n",
        "    #   MODE: INVERSE\n",
        "    # ==========================\n",
        "    if mode == \"inverse\":\n",
        "        epsilon = 1e-8\n",
        "        inv = 1.0 / (dist_values + epsilon)\n",
        "\n",
        "        min_v = inv.min()\n",
        "        max_v = inv.max()\n",
        "\n",
        "        if max_v == min_v:\n",
        "            # All distances identical => all similarities identical\n",
        "            sim_vals = np.ones_like(inv)\n",
        "        else:\n",
        "            sim_vals = (inv - min_v) / (max_v - min_v)\n",
        "\n",
        "        # Pack results\n",
        "        results = [(text, float(sim)) for (text, _), sim in zip(distances, sim_vals)]\n",
        "\n",
        "        # Logging\n",
        "        for text, sim in results:\n",
        "            logger.info(\"Inverse Mode => Text: %s | Similarity: %.4f\", text, sim)\n",
        "            logger.info(\"-\" * 50)\n",
        "\n",
        "        return results\n",
        "\n",
        "    # ==========================\n",
        "    #   MODE: LINEAR\n",
        "    # ==========================\n",
        "    min_v = dist_values.min()\n",
        "    max_v = dist_values.max()\n",
        "\n",
        "    if max_v == min_v:\n",
        "        # No variation in distances => give uniform similarity\n",
        "        sim_vals = np.ones_like(dist_values)\n",
        "    else:\n",
        "        norm = (dist_values - min_v) / (max_v - min_v)\n",
        "        sim_vals = 1.0 - norm  # invert because small distance => high similarity\n",
        "\n",
        "    # Pack results\n",
        "    results = [(text, float(sim)) for (text, _), sim in zip(distances, sim_vals)]\n",
        "\n",
        "    # Logging\n",
        "    for text, sim in results:\n",
        "        logger.info(\"Linear Mode => Text: %s | Similarity: %.4f\", text, sim)\n",
        "        logger.info(\"-\" * 50)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebJOfVzHBKVE"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByvAtwuKBMPA"
      },
      "outputs": [],
      "source": [
        "def get_model_and_importances(model: Any, method: str) -> np.ndarray:\n",
        "    \"\"\"Extracts coefficients or feature importances from a trained model.\n",
        "\n",
        "    Args:\n",
        "        model: A trained scikit-learn or XGBoost model instance.\n",
        "        method (str): The method name (e.g., 'lime', 'randomforest').\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The feature importances or coefficients.\n",
        "\n",
        "    Raises:\n",
        "        AttributeError: If the model lacks both '.coef_' and\n",
        "                        '.feature_importances_'.\n",
        "    \"\"\"\n",
        "    # Linear models use .coef_\n",
        "    if hasattr(model, 'coef_'):\n",
        "        return model.coef_\n",
        "\n",
        "    # Tree-based models use .feature_importances_\n",
        "    if hasattr(model, 'feature_importances_'):\n",
        "        return model.feature_importances_\n",
        "\n",
        "    raise AttributeError(\n",
        "        f\"Model trained with '{method}' does not have a \"\n",
        "        \"'.coef_' or '.feature_importances_' attribute.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def fit_surrogate_model(\n",
        "    perturbations: List[np.ndarray],\n",
        "    similarities: List[Tuple[str, float]],\n",
        "    wmd_scores: List[Tuple[str, float]],\n",
        "    seed: Optional[int] = 1024,\n",
        "    method: str = \"xgboost\",\n",
        "    kernel_width: float = 0.25,\n",
        "    ridge_alpha: float = 1.0,\n",
        ") -> Tuple[Any, np.ndarray, np.ndarray]:\n",
        "    \"\"\"Fits a surrogate model (Linear or Non-Linear) to the perturbation data.\n",
        "\n",
        "    Args:\n",
        "        perturbations (List[np.ndarray]): List of binary perturbation vectors.\n",
        "        similarities (List[Tuple[str, float]]): List of (text, similarity)\n",
        "            pairs.\n",
        "        wmd_scores (List[Tuple[str, float]]): List of (text, distance) pairs.\n",
        "        seed (Optional[int]): Random seed for model reproducibility.\n",
        "        method (str): The method to use ('glm_ols', 'glm_ridge',\n",
        "            'lime_ols', 'lime_ridge', 'baylime', 'randomforest',\n",
        "            'gradientboosting' or 'xgboost'). Defaults to 'xgboost'.\n",
        "        kernel_width (float): Width for the exponential kernel (used for\n",
        "            weighting).\n",
        "        ridge_alpha (float): Regularization strength (alpha) for Ridge models.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Any, np.ndarray, np.ndarray]: (trained_model,\n",
        "            feature_contributions, weights)\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If an unknown method is provided.\n",
        "    \"\"\"\n",
        "    # 1. Prepare Data\n",
        "    X = np.vstack(perturbations)\n",
        "    y = np.array([s for _, s in similarities])\n",
        "    dvals = np.array([d for _, d in wmd_scores])\n",
        "    n_samples = len(y)\n",
        "\n",
        "    # 2. Determine Weights\n",
        "    is_global_ols = method == SurrogateMethod.GLM_OLS.value\n",
        "    is_global_ridge = method == SurrogateMethod.GLM_RIDGE.value\n",
        "    is_global = is_global_ols or is_global_ridge\n",
        "\n",
        "    if is_global:\n",
        "        # Global models (GLM_OLS, GLM_RIDGE) are unweighted\n",
        "        weights = np.ones(n_samples)\n",
        "    else:\n",
        "        # All LIME and Non-Linear models use proximity weighting\n",
        "        # Weight = sqrt(exp(-distance^2 / sigma^2))\n",
        "        weights = np.sqrt(np.exp(-(dvals ** 2) / (kernel_width ** 2)))\n",
        "\n",
        "    logger.debug(f\"Weights (head of array): {weights[:5]} (Method: {method})\")\n",
        "\n",
        "    # 3. Model Dispatch and Fitting\n",
        "\n",
        "    # Define model initialization based on method\n",
        "    model_map = {\n",
        "        SurrogateMethod.LIME.value: LinearRegression(),\n",
        "        SurrogateMethod.LIME_RIDGE.value: Ridge(alpha=ridge_alpha, random_state=seed),\n",
        "        SurrogateMethod.GLM_OLS.value: LinearRegression(),\n",
        "        SurrogateMethod.GLM_RIDGE.value: Ridge(alpha=ridge_alpha, random_state=seed),\n",
        "        SurrogateMethod.BAYLIME.value: BayesianRidge(),\n",
        "        SurrogateMethod.RANDOMFOREST.value: RandomForestRegressor(random_state=seed),\n",
        "        SurrogateMethod.GRADIENT_BOOSTING.value: GradientBoostingRegressor(random_state=seed),\n",
        "        SurrogateMethod.XGBOOST.value: XGBRegressor(random_state=seed, verbosity=0),\n",
        "    }\n",
        "\n",
        "    model = model_map.get(method, None)\n",
        "\n",
        "    if model is None:\n",
        "        raise ValueError(\n",
        "            f\"Unknown method: {method}. Please use one of the values from\"\n",
        "            \" SurrogateMethod.\"\n",
        "        )\n",
        "\n",
        "    # Fit the model: Pass sample_weight only if it's not a global (unweighted) model.\n",
        "    if is_global:\n",
        "        # GLM_OLS and GLM_RIDGE are unweighted global fits\n",
        "        model.fit(X, y)\n",
        "    else:\n",
        "        # All LIME and Non-Linear models use sample_weight\n",
        "        model.fit(X, y, sample_weight=weights)\n",
        "\n",
        "    # 4. Extract Feature Contributions\n",
        "    feature_contributions = get_model_and_importances(model, method)\n",
        "\n",
        "    # 5. Standardized Logging\n",
        "    is_linear_model = method in [\n",
        "        SurrogateMethod.GLM_OLS.value, SurrogateMethod.GLM_RIDGE.value,\n",
        "        SurrogateMethod.LIME.value, SurrogateMethod.LIME_RIDGE.value,\n",
        "        SurrogateMethod.BAYLIME.value\n",
        "    ]\n",
        "\n",
        "    # 6. Standardized Logging\n",
        "    if is_linear_model:\n",
        "        # Log coefficients with higher precision for linear models\n",
        "        logger.info(f\"Coefficients ({method}): {feature_contributions}\")\n",
        "    else:\n",
        "        # Log feature importances for non-linear models\n",
        "        logger.info(f\"Feature Importances ({method}): {feature_contributions}\")\n",
        "\n",
        "    return model, feature_contributions, weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km27ruPeBPz6"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL7cKGyiBRIb"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(\n",
        "    trained_model: Any,\n",
        "    feature_contributions: np.ndarray,\n",
        "    weights: np.ndarray,\n",
        "    similarities: List[Tuple[str, float]],\n",
        "    perturbations: List[np.ndarray]\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"Compute metrics using scikit-learn and NumPy.\n",
        "\n",
        "    Supports Linear and Non-Linear models.\n",
        "\n",
        "    Args:\n",
        "        trained_model: Trained model (Linear or Non-Linear).\n",
        "        feature_contributions (np.ndarray): feature_contributions.\n",
        "        weights (np.ndarray): Sample weights used during training.\n",
        "        similarities (List[Tuple[str, float]]): List of (text, similarity)\n",
        "            pairs.\n",
        "        perturbations (List[np.ndarray]): List/Array of perturbation vectors.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float]: Dictionary of computed metrics.\n",
        "    \"\"\"\n",
        "    # 1. Prepare inputs\n",
        "    y_true = np.array([s for _, s in similarities])\n",
        "\n",
        "    # Handle input type for predict (BayesianRidge expects array)\n",
        "    X = np.vstack(perturbations) if isinstance(perturbations, list) else perturbations\n",
        "    y_pred = trained_model.predict(X).ravel()\n",
        "\n",
        "    # Core Metrics via scikit-learn\n",
        "    # Note: We calculate weighted metrics even for GLM (where weights=1)\n",
        "    # to maintain interface consistency.\n",
        "    mse = mean_squared_error(y_true, y_pred, sample_weight=weights)\n",
        "    mae = mean_absolute_error(y_true, y_pred, sample_weight=weights)\n",
        "    r2 = r2_score(y_true, y_pred, sample_weight=weights)\n",
        "\n",
        "    # derived Metrics (NumPy)\n",
        "    n = len(y_true)\n",
        "    p = len(feature_contributions)\n",
        "    diff = y_true - y_pred\n",
        "\n",
        "    mean_loss = np.abs(np.mean(y_true) - np.mean(y_pred))\n",
        "    mean_l1 = np.mean(np.abs(diff))\n",
        "    mean_l2 = np.mean(diff ** 2)\n",
        "\n",
        "    weighted_l1_norm_n = np.sum(weights * np.abs(diff)) / n\n",
        "    weighted_l2_norm_n = np.sum(weights * (diff ** 2)) / n\n",
        "\n",
        "    # Weighted Adjusted R-squared\n",
        "    if n > p + 1:\n",
        "        adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "    else:\n",
        "        adj_r2 = np.nan\n",
        "\n",
        "    return {\n",
        "        \"Weighted Mean Squared Error (MSEω)\": mse,\n",
        "        \"Weighted Mean Absolute Error (MAEω)\": mae,\n",
        "        \"Weighted R-squared (R²ω)\": r2,\n",
        "        \"Weighted Adjusted R-squared (R²ω_adj)\": adj_r2,\n",
        "        \"Mean Loss (Lm)\": mean_loss,\n",
        "        \"Mean L1 Loss (Unweighted MAE)\": mean_l1,\n",
        "        \"Mean L2 Loss (Unweighted MSE)\": mean_l2,\n",
        "        \"Weighted L1 Loss (Norm by N)\": weighted_l1_norm_n,\n",
        "        \"Weighted L2 Loss (Norm by N)\": weighted_l2_norm_n,\n",
        "    }\n",
        "\n",
        "\n",
        "def find_best_surrogate_model(\n",
        "    perturbations: List[np.ndarray],\n",
        "    similarities: List[Tuple[str, float]],\n",
        "    wmd_scores: List[Tuple[str, float]],\n",
        "    seed: int = 1024,\n",
        "    kernel_width: float = 0.25,\n",
        "    ridge_alpha: float = 1.0,\n",
        ") -> Tuple[str, float]:\n",
        "    \"\"\"\n",
        "    Finds the surrogate model method (from SurrogateMethod enum) that yields\n",
        "    the highest Weighted R-squared (R²ω) score.\n",
        "\n",
        "    Args:\n",
        "        perturbations (List[np.ndarray]): List of binary perturbation vectors.\n",
        "        similarities (List[Tuple[str, float]]): List of (text, similarity) pairs.\n",
        "        wmd_scores (List[Tuple[str, float]]): List of (text, distance) pairs.\n",
        "        seed (int): Random seed for model reproducibility.\n",
        "        kernel_width (float): Width for the exponential kernel (used for weighting).\n",
        "        ridge_alpha (float): Regularization strength (alpha) for Ridge models.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[str, float]: A tuple containing the name of the best surrogate method\n",
        "                           and its corresponding R²ω score.\n",
        "    \"\"\"\n",
        "    best_r2_score = -float('inf')\n",
        "    best_method_name = \"\"\n",
        "\n",
        "    logger.info(\"Starting search for the best surrogate model...\")\n",
        "\n",
        "    for method_enum in SurrogateMethod:\n",
        "        method_name = method_enum.value\n",
        "        logger.info(f\"  Testing surrogate method: {method_name}\")\n",
        "\n",
        "        try:\n",
        "            trained_model, feature_contributions, weights = fit_surrogate_model(\n",
        "                perturbations=perturbations,\n",
        "                similarities=similarities,\n",
        "                wmd_scores=wmd_scores,\n",
        "                seed=seed,\n",
        "                method=method_name,\n",
        "                kernel_width=kernel_width,\n",
        "                ridge_alpha=ridge_alpha,\n",
        "            )\n",
        "\n",
        "            metrics = compute_metrics(\n",
        "                trained_model=trained_model,\n",
        "                feature_contributions=feature_contributions,\n",
        "                weights=weights,\n",
        "                similarities=similarities,\n",
        "                perturbations=perturbations,\n",
        "            )\n",
        "\n",
        "            current_r2 = metrics.get(\"Weighted R-squared (R²ω)\", -float('inf'))\n",
        "            logger.info(f\"    {method_name} R²ω: {current_r2:.4f}\")\n",
        "\n",
        "            if current_r2 > best_r2_score:\n",
        "                best_r2_score = current_r2\n",
        "                best_method_name = method_name\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"  Error processing method {method_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    logger.info(\"\\nSearch complete.\")\n",
        "    logger.info(f\"Best surrogate method found: {best_method_name} with R²ω = {best_r2_score:.4f}\")\n",
        "\n",
        "    return best_method_name, best_r2_score\n",
        "\n",
        "\n",
        "def print_metrics(metrics: Dict):\n",
        "    print('-' * 100)\n",
        "    print(\"Fidelity:\")\n",
        "    for name, value in metrics.items():\n",
        "        print(f\"{name}: {value}\")\n",
        "    print('-' * 100)\n",
        "\n",
        "\n",
        "def calculate_stability_score(\n",
        "    prompt_one: str,\n",
        "    contributions_one: np.ndarray,\n",
        "    prompt_two: str,\n",
        "    contributions_two: np.ndarray\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"Calculates stability metrics between two prompts using Jaccard indices.\n",
        "\n",
        "    This function aligns the feature contributions of two similar prompts based\n",
        "    on their word tokens and computes both the Generalized Jaccard Similarity\n",
        "    and Jaccard Distance between their contribution vectors.\n",
        "\n",
        "    Args:\n",
        "        prompt_one (str): The first text prompt.\n",
        "        contributions_one (np.ndarray): Feature contribution scores for prompt_one.\n",
        "        prompt_two (str): The second text prompt.\n",
        "        contributions_two (np.ndarray): Feature contribution scores for prompt_two.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]: A tuple containing:\n",
        "            - Jaccard Similarity: A value between 0 and 1 indicating similarity\n",
        "              (1.0 means identical importance distributions).\n",
        "            - Jaccard Distance: A value between 0 and 1 indicating dissimilarity\n",
        "              (0.0 means identical importance distributions).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the length of prompts and contribution vectors do not match.\n",
        "    \"\"\"\n",
        "    words_one = prompt_one.split()\n",
        "    words_two = prompt_two.split()\n",
        "\n",
        "    if len(words_one) != len(contributions_one) or len(words_two) != len(contributions_two):\n",
        "        raise ValueError(\"Length of prompts and contribution vectors must match.\")\n",
        "\n",
        "    # Map words to their contributions\n",
        "    # Using dictionaries to map words to their corresponding importance scores.\n",
        "    # Note: Duplicate words will take the value of the last occurrence.\n",
        "    map_one: Dict[str, float] = dict(zip(words_one, contributions_one))\n",
        "    map_two: Dict[str, float] = dict(zip(words_two, contributions_two))\n",
        "\n",
        "    # Create the union of all unique words from both prompts\n",
        "    unique_words = sorted(list(set(words_one) | set(words_two)))\n",
        "\n",
        "    # Align vectors based on the union of words.\n",
        "    # We use absolute values to measure the magnitude of importance.\n",
        "    vec_one = np.array([abs(map_one.get(w, 0.0)) for w in unique_words])\n",
        "    vec_two = np.array([abs(map_two.get(w, 0.0)) for w in unique_words])\n",
        "\n",
        "    # Compute Generalized Jaccard Similarity (Ruzicka similarity)\n",
        "    # Formula: sum(min(a, b)) / sum(max(a, b))\n",
        "    numerator = np.sum(np.minimum(vec_one, vec_two))\n",
        "    denominator = np.sum(np.maximum(vec_one, vec_two))\n",
        "\n",
        "    if denominator == 0:\n",
        "        # Both vectors are zero, implying they are identical (empty or zero-weight).\n",
        "        jaccard_similarity = 1.0\n",
        "    else:\n",
        "        jaccard_similarity = numerator / denominator\n",
        "\n",
        "    jaccard_distance = 1.0 - jaccard_similarity\n",
        "\n",
        "    return jaccard_similarity, jaccard_distance\n",
        "\n",
        "\n",
        "def calculate_token_auc(\n",
        "    text: str,\n",
        "    scores: List[float],\n",
        "    truth: List[int]\n",
        ") -> float:\n",
        "    \"\"\"Calculates the Area Under the ROC Curve (AUC) for token importance.\n",
        "\n",
        "    This function splits the input text into tokens and evaluates how well the\n",
        "    provided contribution scores align with the ground truth binary labels.\n",
        "\n",
        "    Args:\n",
        "        text (str): The raw input prompt/text to be split into tokens.\n",
        "        scores (List[float]): Predicted importance scores for each token.\n",
        "        truth (List[int]): Ground truth binary labels (1 for relevant, 0 otherwise).\n",
        "\n",
        "    Returns:\n",
        "        float: The calculated ROC AUC score. Returns 0.5 if only one class\n",
        "            is present in the truth labels (as AUC is undefined).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the number of tokens derived from the text does not\n",
        "            match the lengths of the scores or truth lists.\n",
        "    \"\"\"\n",
        "    # Split text into tokens based on whitespace\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Validation of input lengths\n",
        "    if not (len(tokens) == len(scores) == len(truth)):\n",
        "        raise ValueError(\n",
        "            f\"Dimension mismatch: Found {len(tokens)} tokens, \"\n",
        "            f\"{len(scores)} scores, and {len(truth)} truth labels.\"\n",
        "        )\n",
        "\n",
        "    # AUC requires at least one positive and one negative sample\n",
        "    unique_classes = np.unique(truth)\n",
        "    if len(unique_classes) < 2:\n",
        "        return 0.5\n",
        "\n",
        "    y_true = np.array(truth)\n",
        "    y_scores = np.array(scores)\n",
        "\n",
        "    return float(roc_auc_score(y_true, y_scores))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBT0SErrbe52"
      },
      "source": [
        "# Save data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoRPm3nVbfcx"
      },
      "outputs": [],
      "source": [
        "def save_perturbation_data_to_csv(\n",
        "    perturbations: List[np.ndarray],\n",
        "    similarities: List[Tuple[str, float]],\n",
        "    wmd_scores: List[Tuple[str, float]],\n",
        "    output_path: str = \"perturbation_data.csv\"\n",
        ") -> str:\n",
        "    \"\"\"Consolidates perturbation data, similarities, and WMD scores and saves\n",
        "    them to a single CSV file.\n",
        "\n",
        "    Args:\n",
        "        perturbations (List[np.ndarray]): List of binary perturbation vectors.\n",
        "        similarities (List[Tuple[str, float]]): List of (text, similarity) pairs\n",
        "                                                (y_true equivalents).\n",
        "        wmd_scores (List[Tuple[str, float]]): List of (text, distance) pairs\n",
        "                                              (for weights calculation).\n",
        "        output_path (str): Full path (including filename) where the CSV\n",
        "                           should be saved.\n",
        "\n",
        "    Returns:\n",
        "        str: The full path to the saved CSV file.\n",
        "    \"\"\"\n",
        "    if not perturbations or not similarities or not wmd_scores:\n",
        "        raise ValueError(\"All input lists (perturbations, similarities,\"\n",
        "                         \" wmd_scores) must be non-empty.\")\n",
        "\n",
        "    # 1. Process Similarities (y_true)\n",
        "    # The similarities list is (perturbed_text, similarity_score)\n",
        "    perturbed_texts_sim = [t for t, _ in similarities]\n",
        "    similarity_scores = [s for _, s in similarities]\n",
        "\n",
        "    # 2. Process WMD Scores\n",
        "    # The wmd_scores list is (perturbed_text, wmd_distance)\n",
        "    wmd_distances = [d for _, d in wmd_scores]\n",
        "    # Note: We assume the order of similarities and wmd_scores matches\n",
        "    # the order of perturbations, as implied by the pipeline.\n",
        "\n",
        "    # 3. Process Perturbations (X matrix)\n",
        "    # Stack the list of arrays into a 2D matrix\n",
        "    X_perturbations = np.vstack(perturbations)\n",
        "\n",
        "    # Create column names for the perturbation vector features (x1, x2, ...)\n",
        "    n_features = X_perturbations.shape[1]\n",
        "    feature_cols = [f\"x_{i+1}\" for i in range(n_features)]\n",
        "\n",
        "    # 4. Create DataFrame\n",
        "    # Start with perturbation vectors\n",
        "    df = pd.DataFrame(X_perturbations, columns=feature_cols)\n",
        "\n",
        "    # Add text identifiers and target variables\n",
        "    df.insert(0, 'Perturbed Text', perturbed_texts_sim)\n",
        "    df['Similarity_Score'] = similarity_scores\n",
        "    df['WMD_Distance'] = wmd_distances\n",
        "\n",
        "    # 5. Save to CSV\n",
        "    save_dir = os.path.dirname(output_path)\n",
        "    if save_dir and not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    df.to_csv(output_path, index=False)\n",
        "\n",
        "    print(f\"Data successfully saved to: {output_path}\")\n",
        "    return output_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt7yUhCBBTnA"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_text_heatmap(\n",
        "    words: list,\n",
        "    scores: np.ndarray,\n",
        "    title: str = \"\",\n",
        "    width: float = 10.0,\n",
        "    height: float = 0.4,\n",
        "    verbose: int = 0,\n",
        "    max_word_per_line: int = 20,\n",
        "    word_spacing: int = 20,\n",
        "    score_fontsize: int = 10,\n",
        "    save_path: str | None = None\n",
        ") -> None:\n",
        "    \"\"\"Plot a heatmap-like visualization over text tokens.\n",
        "\n",
        "    Each token is shown inside a colored box based on its score, with the\n",
        "    numeric score displayed underneath it.\n",
        "\n",
        "    Args:\n",
        "        words (list): List of text tokens.\n",
        "        scores (np.ndarray): Array of per-token scores.\n",
        "        title (str): Title shown on the plot.\n",
        "        width (float): Figure width in inches.\n",
        "        height (float): Figure height in inches.\n",
        "        verbose (int): If 0, hide axes (clean output).\n",
        "        max_word_per_line (int): Max number of tokens per visual line.\n",
        "        word_spacing (int): Horizontal spacing between tokens.\n",
        "        score_fontsize (int): Font size for numeric score labels.\n",
        "        save_path (str | None): Optional save path.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(width, height))\n",
        "    ax = plt.gca()\n",
        "    ax.set_title(title, loc=\"left\")\n",
        "\n",
        "    # Color map normalization\n",
        "    cmap = plt.cm.ScalarMappable(cmap=plt.cm.bwr)\n",
        "    cmap.set_clim(0, 1)\n",
        "\n",
        "    denom = np.max(np.abs(scores))\n",
        "    if denom == 0:\n",
        "        denom = 1e-8   # avoid division by zero\n",
        "    normalized = 0.5 * scores / denom + 0.5\n",
        "\n",
        "    canvas = ax.figure.canvas\n",
        "    transform = ax.transData\n",
        "\n",
        "    y = -0.2  # starting y\n",
        "\n",
        "    for i, (word, score, ns) in enumerate(zip(words, scores, normalized)):\n",
        "        r, g, b, _ = cmap.to_rgba(ns, bytes=True)\n",
        "        color = f\"#{r:02x}{g:02x}{b:02x}\"\n",
        "\n",
        "        # draw token\n",
        "        txt = ax.text(\n",
        "            0.0, y, word,\n",
        "            bbox={\n",
        "                \"facecolor\": color,\n",
        "                \"pad\": 5.0,\n",
        "                \"linewidth\": 1,\n",
        "                \"boxstyle\": \"round,pad=0.5\"\n",
        "            },\n",
        "            transform=transform,\n",
        "            fontsize=14\n",
        "        )\n",
        "        txt.draw(canvas.get_renderer())\n",
        "        ex = txt.get_window_extent()\n",
        "\n",
        "        # draw numeric score under token\n",
        "        score_txt = ax.text(\n",
        "            0.01,\n",
        "            y - 1.0,\n",
        "            f\"{score:.2f}\",\n",
        "            transform=transform,\n",
        "            fontsize=score_fontsize,\n",
        "            ha=\"center\"\n",
        "        )\n",
        "        score_txt.draw(canvas.get_renderer())\n",
        "\n",
        "        # new transform for next token\n",
        "        if (i + 1) % max_word_per_line == 0:\n",
        "            y -= 2.5\n",
        "            transform = ax.transData\n",
        "        else:\n",
        "            transform = transforms.offset_copy(\n",
        "                txt._transform,\n",
        "                x=ex.width + word_spacing,\n",
        "                units=\"dots\"\n",
        "            )\n",
        "\n",
        "    if verbose == 0:\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_actual_vs_predicted(\n",
        "    linear_model: LinearRegression,\n",
        "    results_dict: Dict[str, Any],\n",
        "    save_dir: str = \"output\",\n",
        "    filename: str = \"actual_vs_predicted_plot.png\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Creates and saves a beautiful Actual vs Predicted scatter plot\n",
        "    using pre-computed values from your results dictionary.\n",
        "\n",
        "    Point size = sample weight (larger = more important sample)\n",
        "\n",
        "    Args:\n",
        "        linear_model: Trained LinearRegression model.\n",
        "        results_dict: Your full results dictionary (as returned by your model)\n",
        "        save_dir: Where to save the plot\n",
        "        filename: Name of the output image\n",
        "\n",
        "    Returns:\n",
        "        str: Full path to the saved plot\n",
        "    \"\"\"\n",
        "    y_true = np.array([s for _, s in results_dict[\"similarities\"]])\n",
        "    y_pred = linear_model.predict(results_dict[\"perturbations\"]).ravel()\n",
        "\n",
        "    # Extract data from your existing results\n",
        "    weights = np.array(results_dict[\"weights\"])\n",
        "    weighted_r2 = results_dict[\"metrics\"][\"Weighted R-squared (R²ω)\"]\n",
        "    weighted_adj_r2 = results_dict[\"metrics\"][\"Weighted Adjusted R-squared (R²ω_adj)\"]\n",
        "\n",
        "    # Optional: fallback if y_true/y_pred not stored yet\n",
        "    if y_true is None or y_pred is None:\n",
        "        raise ValueError(\"results_dict must contain 'y_true' and 'y_pred' (or add them before calling)\")\n",
        "\n",
        "    # Normalize weights for point sizes (50 to 500 looks great)\n",
        "    if weights.max() > 0:\n",
        "        point_sizes = (weights / weights.max()) * 450 + 50\n",
        "    else:\n",
        "        point_sizes = np.full_like(weights, 100)\n",
        "\n",
        "    # Create plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "    scatter = ax.scatter(\n",
        "        y_true, y_pred,\n",
        "        s=point_sizes,\n",
        "        c=weights,\n",
        "        cmap='plasma',\n",
        "        alpha=0.75,\n",
        "        edgecolors='black',\n",
        "        linewidth=0.5\n",
        "    )\n",
        "\n",
        "    # Perfect prediction line\n",
        "    min_val = min(y_true.min(), y_pred.min()) * 0.98\n",
        "    max_val = max(y_true.max(), y_pred.max()) * 1.02\n",
        "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2.5, label='Perfect Prediction')\n",
        "\n",
        "    ax.set_xlim(min_val, max_val)\n",
        "    ax.set_ylim(min_val, max_val)\n",
        "\n",
        "    ax.set_xlabel('Actual Values', fontsize=13, fontweight='bold')\n",
        "    ax.set_ylabel('Predicted Values', fontsize=13, fontweight='bold')\n",
        "    ax.set_title(\n",
        "        'Actual vs Predicted Values\\n'\n",
        "        f'Weighted R² = {weighted_r2:.4f}  •  Adjusted R² = {weighted_adj_r2:.4f}',\n",
        "        fontsize=15, fontweight='bold', pad=20\n",
        "    )\n",
        "\n",
        "    ax.legend(loc='upper left', fontsize=11)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Colorbar\n",
        "    cbar = plt.colorbar(scatter, ax=ax, shrink=0.8)\n",
        "    cbar.set_label('Sample Weight (Importance)', rotation=270, labelpad=20, fontsize=11)\n",
        "\n",
        "    # Save\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    save_path = os.path.join(save_dir, filename)\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "    print(f\"Plot saved: {save_path}\")\n",
        "    return save_path\n",
        "\n",
        "\n",
        "def plot_bar_chart(\n",
        "    data: dict,\n",
        "    title: str,\n",
        "    xaxis_title: str,\n",
        "    yaxis_title: str,\n",
        "    width: int = 800,\n",
        "    height: int = 600,\n",
        "    image_save_path: str | None = None\n",
        ") -> str | None:\n",
        "    \"\"\"\n",
        "    Generates and optionally saves a Plotly bar chart for single values per category.\n",
        "\n",
        "    Args:\n",
        "        data (dict): A dictionary where keys are keywords (strings)\n",
        "                     and values are single numerical data points.\n",
        "        title (str): The title of the bar chart.\n",
        "        xaxis_title (str): The label for the x-axis.\n",
        "        yaxis_title (str): The label for the y-axis.\n",
        "        width (int): The width of the plot in pixels.\n",
        "        height (int): The height of the plot in pixels.\n",
        "        image_save_path (str | None): The file path to save the plot as a static image.\n",
        "\n",
        "    Returns:\n",
        "        str | None: The file path to the saved plot, or None if not saved.\n",
        "    \"\"\"\n",
        "    words = list(data.keys())\n",
        "    # Assuming values are single-element lists, extract the single value\n",
        "    contributions = [values[0] for values in data.values()]\n",
        "\n",
        "    fig = go.Figure(data=[go.Bar(x=words, y=contributions, marker_color='skyblue')])\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis_title=xaxis_title,\n",
        "        yaxis_title=yaxis_title,\n",
        "        showlegend=False,\n",
        "        xaxis=dict(\n",
        "            tickangle=45,\n",
        "            tickfont=dict(size=14, family=\"Arial\", color=\"black\", weight=\"bold\")\n",
        "        ),\n",
        "        yaxis=dict(\n",
        "            tickfont=dict(size=12, family=\"Arial\", color=\"black\", weight=\"bold\")\n",
        "        ),\n",
        "        title_font=dict(size=16, family=\"Arial\", color=\"black\", weight=\"bold\"),\n",
        "        width=width,\n",
        "        height=height\n",
        "    )\n",
        "\n",
        "    if image_save_path:\n",
        "        save_dir = os.path.dirname(image_save_path)\n",
        "        if save_dir and not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "        pio.write_image(fig, image_save_path)\n",
        "        print(f\"Bar chart saved to: {image_save_path}\")\n",
        "\n",
        "        bar_chart_img = plt.imread(image_save_path)\n",
        "\n",
        "        plt.figure(figsize=(width // 100, height // 100))\n",
        "        plt.imshow(bar_chart_img)\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "        return image_save_path\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def plot_box_plot(\n",
        "    data_updated: dict,\n",
        "    title: str,\n",
        "    xaxis_title: str,\n",
        "    yaxis_title: str,\n",
        "    width: int = 800,\n",
        "    height: int = 800,\n",
        "    image_save_path: str | None = None\n",
        ") -> str | None:\n",
        "    \"\"\"\n",
        "    Generates and optionally saves a Plotly box plot.\n",
        "\n",
        "    Args:\n",
        "        data_updated (dict): A dictionary where keys are keywords (strings)\n",
        "                             and values are lists of numerical data points.\n",
        "        title (str): The title of the box plot.\n",
        "        xaxis_title (str): The label for the x-axis.\n",
        "        yaxis_title (str): The label for the y-axis.\n",
        "        width (int): The width of the plot in pixels.\n",
        "        height (int): The height of the plot in pixels.\n",
        "        image_save_path (str | None): The file path to save the plot as a static image.\n",
        "\n",
        "    Returns:\n",
        "        str | None: The file path to the saved plot, or None if not saved.\n",
        "    \"\"\"\n",
        "    fig = go.Figure()\n",
        "\n",
        "    for keyword, values in data_updated.items():\n",
        "        fig.add_trace(go.Box(y=values, name=keyword, boxpoints=\"all\"))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis_title=xaxis_title,\n",
        "        yaxis_title=yaxis_title,\n",
        "        showlegend=False,\n",
        "        xaxis=dict(\n",
        "            tickangle=45,\n",
        "            tickfont=dict(size=14, family=\"Arial\", color=\"black\", weight=\"bold\")\n",
        "        ),\n",
        "        yaxis=dict(\n",
        "            tickfont=dict(size=12, family=\"Arial\", color=\"black\", weight=\"bold\")\n",
        "        ),\n",
        "        title_font=dict(size=16, family=\"Arial\", color=\"black\", weight=\"bold\"),\n",
        "        width=width,\n",
        "        height=height\n",
        "    )\n",
        "\n",
        "    if image_save_path:\n",
        "        save_dir = os.path.dirname(image_save_path)\n",
        "        if save_dir and not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "        pio.write_image(fig, image_save_path)\n",
        "        print(f\"Box plot saved to: {image_save_path}\")\n",
        "\n",
        "        box_plot_img = plt.imread(image_save_path)\n",
        "\n",
        "        plt.figure(figsize=(width // 100, height // 100))\n",
        "        plt.imshow(box_plot_img)\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "        return image_save_path\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def plot_stability_visualization(\n",
        "    prompt_one: str,\n",
        "    contributions_one: np.ndarray,\n",
        "    prompt_two: str,\n",
        "    contributions_two: np.ndarray,\n",
        "    width: float = 12.0,\n",
        "    height: float = 8.0,\n",
        "    save_path: str | None = None\n",
        ") -> None:\n",
        "    \"\"\"Visualizes the stability flow between two prompts and a generative model.\n",
        "\n",
        "    Plots Prompt 1 (top) and Prompt 2 (bottom) as heatmaps, with a 'Generative\n",
        "    Model' node in the center. Edges are drawn from Prompt 1 words to the model,\n",
        "    and from the model to Prompt 2 words.\n",
        "\n",
        "    Args:\n",
        "        prompt_one (str): The first text prompt.\n",
        "        contributions_one (np.ndarray): Feature contributions for prompt_one.\n",
        "        prompt_two (str): The second text prompt.\n",
        "        contributions_two (np.ndarray): Feature contributions for prompt_two.\n",
        "        width (float): Figure width in inches.\n",
        "        height (float): Figure height in inches.\n",
        "        save_path (str | None): Optional path to save the resulting plot.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    words_one = prompt_one.split()\n",
        "    words_two = prompt_two.split()\n",
        "\n",
        "    # Normalize scores for coloring (global normalization across both prompts)\n",
        "    all_scores = np.concatenate([contributions_one, contributions_two])\n",
        "    denom = np.max(np.abs(all_scores))\n",
        "    if denom == 0:\n",
        "        denom = 1e-8\n",
        "\n",
        "    # Helper to get color\n",
        "    cmap = plt.cm.ScalarMappable(cmap=plt.cm.bwr)\n",
        "    cmap.set_clim(0, 1)\n",
        "\n",
        "    def get_color(score):\n",
        "        norm_score = 0.5 * score / denom + 0.5\n",
        "        r, g, b, _ = cmap.to_rgba(norm_score, bytes=True)\n",
        "        return f\"#{r:02x}{g:02x}{b:02x}\"\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(width, height))\n",
        "    ax.set_xlim(0, 100)\n",
        "    ax.set_ylim(0, 100)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    # Layout Configuration\n",
        "    y_prompt_one = 85\n",
        "    y_model = 50\n",
        "    y_prompt_two = 15\n",
        "\n",
        "    # Calculate horizontal spacing\n",
        "    # We distribute words evenly across the width (10 to 90)\n",
        "    def calculate_x_positions(num_words):\n",
        "        return np.linspace(10, 90, num_words)\n",
        "\n",
        "    x_pos_one = calculate_x_positions(len(words_one))\n",
        "    x_pos_two = calculate_x_positions(len(words_two))\n",
        "    x_model = 50  # Center\n",
        "\n",
        "    # Draw Generative Model (Center)\n",
        "    model_box = FancyBboxPatch(\n",
        "        (x_model - 5, y_model - 3), 10, 6,\n",
        "        boxstyle=\"round,pad=0.2\",\n",
        "        fc=\"#E0E0E0\", ec=\"black\", lw=2\n",
        "    )\n",
        "    ax.add_patch(model_box)\n",
        "    ax.text(x_model, y_model, \"Generative\\nModel\",\n",
        "            ha=\"center\", va=\"center\", fontsize=12, fontweight=\"bold\")\n",
        "\n",
        "    # Draw Prompt One (Top)\n",
        "    for i, (word, score, x) in enumerate(zip(words_one, contributions_one, x_pos_one)):\n",
        "        color = get_color(score)\n",
        "\n",
        "        # Word Box\n",
        "        ax.text(\n",
        "            x, y_prompt_one, word,\n",
        "            bbox=dict(facecolor=color, pad=5.0, linewidth=1, boxstyle=\"round,pad=0.5\"),\n",
        "            fontsize=12, ha=\"center\"\n",
        "        )\n",
        "\n",
        "        # Score Value\n",
        "        ax.text(\n",
        "            x, y_prompt_one - 5, f\"{score:.2f}\",\n",
        "            fontsize=9, ha=\"center\"\n",
        "        )\n",
        "\n",
        "        # Edge: Prompt One -> Model\n",
        "        # Starting slightly below the score\n",
        "        ax.annotate(\n",
        "            \"\",\n",
        "            xy=(x_model, y_model + 3),  # Target (Top of model box)\n",
        "            xytext=(x, y_prompt_one - 6),   # Source (Bottom of score)\n",
        "            arrowprops=dict(arrowstyle=\"->\", color=\"gray\", alpha=0.5, shrinkA=5, shrinkB=5)\n",
        "        )\n",
        "\n",
        "    # Draw Prompt Two (Bottom)\n",
        "    for i, (word, score, x) in enumerate(zip(words_two, contributions_two, x_pos_two)):\n",
        "        color = get_color(score)\n",
        "\n",
        "        # Word Box\n",
        "        ax.text(\n",
        "            x, y_prompt_two, word,\n",
        "            bbox=dict(facecolor=color, pad=5.0, linewidth=1, boxstyle=\"round,pad=0.5\"),\n",
        "            fontsize=12, ha=\"center\"\n",
        "        )\n",
        "\n",
        "        # Score Value\n",
        "        ax.text(\n",
        "            x, y_prompt_two - 5, f\"{score:.2f}\",\n",
        "            fontsize=9, ha=\"center\"\n",
        "        )\n",
        "\n",
        "        # Edge: Model -> Prompt Two\n",
        "        # Target is top of the word box (approx y_prompt_two + padding)\n",
        "        ax.annotate(\n",
        "            \"\",\n",
        "            xy=(x, y_prompt_two + 2),    # Target (Top of word box)\n",
        "            xytext=(x_model, y_model - 3),   # Source (Bottom of model box)\n",
        "            arrowprops=dict(arrowstyle=\"->\", color=\"gray\", alpha=0.5, shrinkA=5, shrinkB=5)\n",
        "        )\n",
        "\n",
        "    # Titles\n",
        "    ax.text(50, 95, \"Prompt 1 Source\", fontsize=14, fontweight=\"bold\", ha=\"center\")\n",
        "    ax.text(50, 5, \"Prompt 2 Target\", fontsize=14, fontweight=\"bold\", ha=\"center\")\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_importance_roc_curve(\n",
        "    truth: List[int],\n",
        "    scores: List[float],\n",
        "    title: str = \"ROC Curve - Token Importance\",\n",
        "    save_path: Optional[str] = None\n",
        ") -> None:\n",
        "    \"\"\"Plots the Receiver Operating Characteristic (ROC) curve for importance scores.\n",
        "\n",
        "    This visualization helps evaluate how well the model's contribution scores\n",
        "    distinguish between ground-truth relevant and irrelevant tokens.\n",
        "\n",
        "    Args:\n",
        "        truth (List[int]): Ground truth binary labels (1 for relevant, 0 otherwise).\n",
        "        scores (List[float]): Predicted importance/contribution scores.\n",
        "        title (str): Title of the plot.\n",
        "        save_path (Optional[str]): If provided, saves the plot to this file path.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    y_true = np.array(truth)\n",
        "    y_scores = np.array(scores)\n",
        "\n",
        "    # Check if both classes are present\n",
        "    if len(np.unique(y_true)) < 2:\n",
        "        print(\"Warning: ROC curve cannot be plotted with only one class in truth labels.\")\n",
        "        return\n",
        "\n",
        "    # Calculate FPR, TPR and Area under the curve\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    # Plot the ROC curve\n",
        "    plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (area = {roc_auc:.4f})\")\n",
        "\n",
        "    # Plot the diagonal baseline (random classifier)\n",
        "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
        "\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel(\"False Positive Rate (Irrelevant tokens marked as important)\")\n",
        "    plt.ylabel(\"True Positive Rate (Relevant tokens correctly identified)\")\n",
        "    plt.title(title)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "HoP8oWteVEkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhM_qZItBXgC"
      },
      "source": [
        "# High-level pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_interpretability_pipeline(\n",
        "    prompt: str,\n",
        "    input_image_path: str,\n",
        "    model_name: str,\n",
        "    model_txt: Optional[Any] = None,\n",
        "    output_dir: str = \"output\",\n",
        "    num_perturb: int = 10,\n",
        "    low_threshold: int = 70,\n",
        "    high_threshold: int = 200,\n",
        "    seed: int = 1024,\n",
        "    kernel_width: float = 0.25,\n",
        "    ridge_alpha: float = 1.0,\n",
        "    plot_visualization: bool = True,\n",
        "    mode: str = \"linear\",\n",
        "    surrogate_method: str = \"xgboost\",\n",
        "    api_key: Optional[str] = None,\n",
        "    batch: bool = False,\n",
        "    find_best_method: bool = False,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Run the full image-text interpretability pipeline.\n",
        "\n",
        "    This pipeline performs text perturbation, image generation, embedding\n",
        "    comparison, similarity normalization, and surrogate model fitting to\n",
        "    explain prompt-level influence on image generation.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): Input prompt describing the desired image edit.\n",
        "        input_image_path (str): Path to the original input image.\n",
        "        model_name (str): Name of the image generation model.\n",
        "        model_txt (Optional[Any]): Word embedding model (e.g., Word2Vec).\n",
        "        output_dir (str): Directory for saving all outputs.\n",
        "        num_perturb (int): Number of text perturbations to generate.\n",
        "        low_threshold (int): Canny edge detector low threshold.\n",
        "        high_threshold (int): Canny edge detector high threshold.\n",
        "        seed (int): Global random seed for reproducibility.\n",
        "        kernel_width (float): Kernel width for similarity weighting.\n",
        "        ridge_alpha (float): Regularization strength for Ridge models.\n",
        "        plot_visualization (bool): Whether to generate visual outputs.\n",
        "        mode (str): Similarity normalization mode (\"linear\" or \"inverse\").\n",
        "        surrogate_method (str): Surrogate model type.\n",
        "        api_key (Optional[str]): API key for remote image models.\n",
        "        batch (bool): Whether to use batch image generation (Gemini only).\n",
        "        find_best_method (bool): If True, dynamically finds the best surrogate\n",
        "            model method based on R²ω score.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: Dictionary containing perturbations, similarities,\n",
        "        surrogate model outputs, metrics, and file paths.\n",
        "    \"\"\"\n",
        "    logger.info(\"Setting seeds for reproducibility...\")\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    # Check for CUDA availability and set seeds accordingly\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "    else:\n",
        "        logger.warning(\"CUDA not available. Running on CPU.\")\n",
        "\n",
        "    logger.info(\"Setup Facebook DINOv2 processor & model...\")\n",
        "    processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\", use_fast=True)\n",
        "    # Move model to CUDA if available, otherwise keep on CPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = AutoModel.from_pretrained(\"facebook/dinov2-base\").to(device)\n",
        "\n",
        "    if model_txt is None:\n",
        "        logger.info(\"Loading Google news Word2Vec model...\")\n",
        "        # TODO: Find Better cache solution for this model\n",
        "        model_txt = load_google_news_vectors()\n",
        "        # Precompute and cache vector norms to speed up similarity/WMD calculations\n",
        "        model_txt.fill_norms(force=True)\n",
        "\n",
        "    logger.info(\"Preparing output directory...\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    if not os.path.exists(input_image_path):\n",
        "        raise FileNotFoundError(\n",
        "            f\"Input image not found at {input_image_path}\"\n",
        "        )\n",
        "\n",
        "    logger.info(\"Validating input prompt...\")\n",
        "    if not isinstance(prompt, str):\n",
        "        raise TypeError(\"Prompt must be a string.\")\n",
        "\n",
        "    normalized_prompt = prompt.strip()\n",
        "\n",
        "    if not normalized_prompt:\n",
        "        raise ValueError(\"Prompt cannot be empty or whitespace only.\")\n",
        "\n",
        "    if len(normalized_prompt) < 10 and normalized_prompt.split() >= 3:\n",
        "        raise ValueError(\n",
        "            \"Prompt is too short for reliable image editing.\\n\"\n",
        "            f\"Provided prompt ({len(normalized_prompt)} chars): \"\n",
        "            f\"\\\"{prompt}\\\"\\n\"\n",
        "            \"Please use a more descriptive prompt \"\n",
        "            \"(at least 18-20 characters).\"\n",
        "        )\n",
        "\n",
        "    logger.info(\"Generating text perturbations...\")\n",
        "    responses, perturbations = generate_perturbations(\n",
        "        text=normalized_prompt,\n",
        "        num_perturb=num_perturb,\n",
        "    )\n",
        "\n",
        "    total_cost = 0.0\n",
        "    huggingface_pipe = None\n",
        "    mask_model = None\n",
        "    mask_model_weights = None\n",
        "\n",
        "    # Initialize HuggingFace pipe if model_name is instruct-pix2pix\n",
        "    if \"instruct-pix2pix\" in model_name.lower():\n",
        "        model_name = \"timbrooks/instruct-pix2pix\"\n",
        "        logger.info(\"Initializing StableDiffusionInstructPix2PixPipeline...\")\n",
        "        huggingface_pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n",
        "            model_name, dtype=torch.float16, safety_checker=None\n",
        "        )\n",
        "        huggingface_pipe.to(device)\n",
        "        huggingface_pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(\n",
        "            huggingface_pipe.scheduler.config\n",
        "        )\n",
        "    elif \"stable-diffusion-2-inpainting\" in model_name.lower():\n",
        "        model_name = \"sd2-community/stable-diffusion-2-inpainting\"\n",
        "        logger.info(\"Initializing StableDiffusionInpaintPipeline...\")\n",
        "        huggingface_pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
        "            model_name, dtype=torch.float16\n",
        "        )\n",
        "        huggingface_pipe.to(device)\n",
        "        huggingface_pipe.enable_attention_slicing()\n",
        "\n",
        "        logger.info(\"Loading DeepLabV3+ segmentation model...\")\n",
        "        mask_model_weights = DeepLabV3_ResNet101_Weights.DEFAULT\n",
        "        mask_model = deeplabv3_resnet101(weights=mask_model_weights)\n",
        "        mask_model.to(device)\n",
        "\n",
        "    logger.info(\"Starting image generation step...\")\n",
        "    if batch and \"gemini\" in model_name.lower():\n",
        "        if api_key is None:\n",
        "            raise ValueError(\"API key is required for Gemini models.\")\n",
        "        logger.info(\n",
        "            \"Using batch image generation via Gemini API.\"\n",
        "        )\n",
        "        batch_job_name = submit_gemini_batch_job(\n",
        "            api_key=api_key,\n",
        "            image_path=input_image_path,\n",
        "            text_list=responses,\n",
        "            model_name=model_name,\n",
        "            seed=seed,\n",
        "        )\n",
        "\n",
        "        generated_images_paths, total_cost = retrieve_gemini_batch_results(\n",
        "            api_key=api_key,\n",
        "            job_name=batch_job_name,\n",
        "            text_list=responses,\n",
        "            model_name=model_name,\n",
        "            output_dir=output_dir\n",
        "        )\n",
        "    else:\n",
        "        generated_images_paths, total_cost = generate_images_from_prompts(\n",
        "            input_image_path=input_image_path,\n",
        "            prompts=responses,\n",
        "            model_name=model_name,\n",
        "            output_dir=output_dir,\n",
        "            low_threshold=low_threshold,\n",
        "            high_threshold=high_threshold,\n",
        "            seed=seed,\n",
        "            api_key=api_key,\n",
        "            huggingface_pipe=huggingface_pipe,\n",
        "            mask_model=mask_model,\n",
        "            mask_model_weights=mask_model_weights,\n",
        "        )\n",
        "\n",
        "    logger.info(\"Computing Wasserstein distances between images...\")\n",
        "    image_distances = compute_wasserstein_distances(\n",
        "        input_image_path=input_image_path,\n",
        "        generated_images=generated_images_paths,\n",
        "        embedding_model=model,\n",
        "        embedding_processor=processor,\n",
        "        prompts=responses,\n",
        "        output_dir=output_dir,\n",
        "    )\n",
        "\n",
        "    logger.info(\"Computing WMD scores between prompt & perturbations...\")\n",
        "    wmd_scores = compute_wmd_scores(model_txt, normalized_prompt, responses)\n",
        "\n",
        "    # TODO: In the main code we find similarty with inversed mode\n",
        "    logger.info(\"Normalizing similarity scores...\")\n",
        "    sims = normalize_similarities(\n",
        "        distances=image_distances,\n",
        "        mode=mode,\n",
        "    )\n",
        "\n",
        "    # Dynamically find the best surrogate method if flag is True\n",
        "    if find_best_method:\n",
        "        logger.info(\"Finding the best surrogate model...\")\n",
        "        best_method, best_r2 = find_best_surrogate_model(\n",
        "            perturbations=perturbations,\n",
        "            similarities=sims,\n",
        "            wmd_scores=wmd_scores,\n",
        "            seed=seed,\n",
        "            kernel_width=kernel_width,\n",
        "            ridge_alpha=ridge_alpha,\n",
        "        )\n",
        "        surrogate_method = best_method\n",
        "        logger.info(f\"Using best surrogate method: {surrogate_method} (R²ω: {best_r2:.4f})\")\n",
        "\n",
        "    logger.info(f\"Training surrogate model using method: {surrogate_method}...\")\n",
        "    trained_model, feature_contributions, weights = fit_surrogate_model(\n",
        "        perturbations=perturbations,\n",
        "        similarities=sims,\n",
        "        wmd_scores=wmd_scores,\n",
        "        seed=seed,\n",
        "        method=surrogate_method,\n",
        "        kernel_width=kernel_width,\n",
        "        ridge_alpha=ridge_alpha,\n",
        "    )\n",
        "\n",
        "    logger.info(\"Computing metrics...\")\n",
        "    metrics = compute_metrics(\n",
        "        trained_model=trained_model,\n",
        "        feature_contributions=feature_contributions,\n",
        "        weights=weights,\n",
        "        similarities=sims,\n",
        "        perturbations=perturbations,\n",
        "    )\n",
        "\n",
        "    logger.info(\"Save variables data to pickle file...\")\n",
        "    save_to_pickle(\n",
        "        output_path=os.path.join(output_dir, f\"{model_name.replace('/', '_')}.pkl\"),\n",
        "        responses=responses,\n",
        "        perturbations=perturbations,\n",
        "        image_distances=image_distances,\n",
        "        wmd_scores=wmd_scores,\n",
        "        sims=sims,\n",
        "        mode=mode,\n",
        "        normalized_prompt=normalized_prompt,\n",
        "        num_perturb=num_perturb,\n",
        "        seed=seed,\n",
        "    )\n",
        "\n",
        "    logger.info(\"Saving perturbation data to CSV...\")\n",
        "    csv_path = save_perturbation_data_to_csv(\n",
        "        perturbations=perturbations,\n",
        "        similarities=sims,\n",
        "        wmd_scores=wmd_scores,\n",
        "        output_path=os.path.join(output_dir, f\"perturbation_data_{surrogate_method}.csv\")\n",
        "    )\n",
        "\n",
        "    results = {\n",
        "        \"prompt\": normalized_prompt,\n",
        "        \"csv_output_path\": csv_path,\n",
        "        \"responses\": responses,\n",
        "        \"perturbations\": perturbations,\n",
        "        \"wmd_scores\": wmd_scores,\n",
        "        \"similarities\": sims,\n",
        "        \"feature_contributions\": feature_contributions,\n",
        "        \"weights\": weights,\n",
        "        \"metrics\": metrics,\n",
        "        \"total_image_generation_cost\": total_cost\n",
        "    }\n",
        "\n",
        "    logger.info(f\"Total cost of generated these images is {total_cost}\")\n",
        "    print_metrics(metrics)\n",
        "\n",
        "    if plot_visualization:\n",
        "        logger.info(\"Visualizing results...\")\n",
        "        words = normalized_prompt.split()\n",
        "        heatmap_save_path = os.path.join(\n",
        "            output_dir,\n",
        "            f\"heatmap_{surrogate_method.upper()}.png\"\n",
        "        )\n",
        "        plot_text_heatmap(words, feature_contributions,\n",
        "                          f\"Text Heatmap ({surrogate_method.upper()})\",\n",
        "                          save_path=heatmap_save_path,)\n",
        "\n",
        "        plot_actual_vs_predicted(\n",
        "            trained_model,\n",
        "            results,\n",
        "            save_dir=output_dir,\n",
        "            filename=f\"actual_vs_predicted_{surrogate_method}.png\"\n",
        "        )\n",
        "\n",
        "        data_for_bar_chart = {word: [contribution] for word, contribution in zip(words, feature_contributions)}\n",
        "\n",
        "        bar_chart_save_path = os.path.join(\n",
        "            output_dir,\n",
        "            f\"bar_chart_{surrogate_method.upper()}.png\"\n",
        "        )\n",
        "        plot_bar_chart(\n",
        "            data_for_bar_chart,\n",
        "            title=f\"Bar Chart of Word Contributions ({surrogate_method.upper()})\",\n",
        "            xaxis_title=\"Keywords\",\n",
        "            yaxis_title=\"Contribution Value\",\n",
        "            image_save_path=bar_chart_save_path\n",
        "        )\n",
        "\n",
        "        data_updated = {word: [contribution] for word, contribution in zip(words, feature_contributions)}\n",
        "\n",
        "        box_plot_save_path = os.path.join(\n",
        "            output_dir,\n",
        "            f\"box_plot_{surrogate_method.upper()}.png\"\n",
        "        )\n",
        "        plot_box_plot(\n",
        "            data_updated,\n",
        "            title=f\"Box Plot of Word Contributions ({surrogate_method.upper()})\",\n",
        "            xaxis_title=\"Keywords\",\n",
        "            yaxis_title=\"Contribution Value\",\n",
        "            image_save_path=box_plot_save_path\n",
        "        )\n",
        "\n",
        "    logger.info(\"Pipeline execution completed successfully.\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "5pQVscq4V1T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AUC"
      ],
      "metadata": {
        "id": "CBZkXYx48Xjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUC_DICT = {\n",
        "    \"Deblurring\": {\n",
        "        \"ori_exp\": \"Remove the blurriness from the image.\",\n",
        "        \"truth\": [1, 0, 1, 0, 0, 0]  # Remove(1), blurriness(1)\n",
        "    },\n",
        "    \"HazeRemoval\": {\n",
        "        \"ori_exp\": \"Remove the haze from the image.\",\n",
        "        \"truth\": [1, 0, 1, 0, 0, 0]  # Remove(1), haze(1)\n",
        "    },\n",
        "    \"Lowlight\": {\n",
        "        \"ori_exp\": \"Enhance the brightness of an image.\",\n",
        "        \"truth\": [1, 0, 1, 0, 0, 0]  # Enhance(1), brightness(1)\n",
        "    },\n",
        "    \"NoiseRemoval\": {\n",
        "        \"ori_exp\": \"Remove the noise from the image.\",\n",
        "        \"truth\": [1, 0, 1, 0, 0, 0]  # Remove(1), noise(1)\n",
        "    },\n",
        "    \"RainRemoval\": {\n",
        "        \"ori_exp\": \"Remove the rain from the image.\",\n",
        "        \"truth\": [1, 0, 1, 0, 0, 0]  # Remove(1), rain(1)\n",
        "    },\n",
        "    \"ShadowRemoval\": {\n",
        "        \"ori_exp\": \"Remove the shadow from the image.\",\n",
        "        \"truth\": [1, 0, 1, 0, 0, 0]  # Remove(1), shadow(1)\n",
        "    },\n",
        "    \"SnowRemoval\": {\n",
        "        \"ori_exp\": \"Remove the snow from the image.\",\n",
        "        \"truth\": [1, 0, 1, 0, 0, 0]  # Remove(1), snow(1)\n",
        "    },\n",
        "    \"WatermarkRemoval\": {\n",
        "        \"ori_exp\": \"Remove the watermark from the image.\",\n",
        "        \"truth\": [1, 0, 1, 0, 0, 0]  # Remove(1), watermark(1)\n",
        "    },\n",
        "    \"Counting\": {\n",
        "        \"ori_exp\": \"Add a cat to the shoe rack\",\n",
        "        \"truth\": [1, 1, 1, 0, 0, 1, 1]  # Add(1), a(1), cat(1), shoe(1), rack(1)\n",
        "    },\n",
        "    \"DirectionPerception\": {\n",
        "        \"ori_exp\": \"Add a cat to the bottom of the skateboard\",\n",
        "        \"truth\": [1, 1, 1, 0, 0, 1, 0, 0, 1]  # Add(1), a(1), cat(1), bottom(1), skateboard(1)\n",
        "    },\n",
        "    \"ObjectRemoval\": {\n",
        "        \"ori_exp\": \"Remove the red slippers from the image\",\n",
        "        \"truth\": [1, 0, 1, 1, 0, 0, 0]  # Remove(1), red(1), slippers(1)\n",
        "    },\n",
        "    \"Replacement\": {\n",
        "        \"ori_exp\": \"replace motorcycle with dog\",\n",
        "        \"truth\": [1, 1, 0, 1]  # replace(1), motorcycle(1), dog(1)\n",
        "    },\n",
        "    \"BGReplacement\": {\n",
        "        \"ori_exp\": \"Change the background of this photo to snow\",\n",
        "        \"truth\": [1, 0, 1, 0, 0, 1, 0, 1]  # Change(1), background(1), photo(1), snow(1)\n",
        "    },\n",
        "    \"ColorAlteration\": {\n",
        "        \"ori_exp\": \"Change the color of the bear to brown\",\n",
        "        \"truth\": [1, 0, 1, 0, 0, 1, 0, 1]  # Change(1), color(1), bear(1), brown(1)\n",
        "    },\n",
        "    \"StyleAlteration\": {\n",
        "        \"ori_exp\": \"Change the image to oil painting style\",\n",
        "        \"truth\": [1, 0, 1, 0, 1, 1, 1]  # Change(1), image(1), oil(1), painting(1), style(1)\n",
        "    },\n",
        "    \"RegionAccuracy\": {\n",
        "        \"ori_exp\": \"Change the color of the bear to brown\",\n",
        "        \"truth\": [1, 0, 1, 0, 0, 1, 0, 1]  # Same as ColorAlteration\n",
        "    }\n",
        "}\n",
        "\n",
        "MODELS_DIR = [\n",
        "    \"I-Pix2Pix\",\n",
        "    \"Diffusers_I\",\n",
        "    \"I2I-Turbo\",\n",
        "    \"GPT-Image-1-Mini\",\n",
        "    \"GPT-Image-1.5\",\n",
        "    \"SeeDream-4.5\",\n",
        "    \"Nano-Banana-Pro\",\n",
        "]"
      ],
      "metadata": {
        "id": "Jf9a1tMq8Ysa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxy-U1R0BdFv"
      },
      "source": [
        "# Execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhdqlr1GNBlF"
      },
      "source": [
        "## Load API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVKxfXtskjP3"
      },
      "outputs": [],
      "source": [
        "gemini_api_key = os.getenv(\"GEMINI_API_KEY\", None)\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\", None)\n",
        "ark_api_key = os.getenv(\"ARK_API_KEY\", None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7GBAbIfNF4h"
      },
      "source": [
        "## Download I2EBench Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6Vm2xOPnPkh"
      },
      "outputs": [],
      "source": [
        "i2ebench_dir = \"/content/drive/MyDrive/I2EBench\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CfPjd6SNKu8"
      },
      "outputs": [],
      "source": [
        "low_level_editing: List[str] = ['Deblurring', 'HazeRemoval', 'Lowlight', 'NoiseRemoval', 'RainRemoval', 'ShadowRemoval', 'SnowRemoval', 'WatermarkRemoval']\n",
        "high_level_edition: List[str] = ['Counting', 'DirectionPerception', 'ObjectRemoval', 'Replacement', 'BGReplacement', 'ColorAlteration', 'StyleAlteration', 'RegionAccuracy']\n",
        "\n",
        "low_level_editing_data = load_i2ebench_data(\n",
        "    root_dir=i2ebench_dir,\n",
        "    categories_dir=low_level_editing,\n",
        "    limits_per_category=1,\n",
        ")\n",
        "high_level_editing_data = load_i2ebench_data(\n",
        "    root_dir=i2ebench_dir,\n",
        "    categories_dir=high_level_edition,\n",
        "    limits_per_category=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMvEi973OFKy"
      },
      "source": [
        "## loading Google news Word2Vec model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py69W1nPCuHB",
        "outputId": "084b395b-5305-4b07-8771-a4ec51c19cc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Loading Google news Word2Vec model...\n",
            "INFO:__main__:Loading 'word2vec-google-news-300' via gensim cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"Loading Google news Word2Vec model...\")\n",
        "# TODO: Find Better cache solution for this model\n",
        "model_txt = load_google_news_vectors()\n",
        "# Precompute and cache vector norms to speed up similarity/WMD calculations\n",
        "model_txt.fill_norms(force=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2mbvHHHM8co"
      },
      "source": [
        "## OpenAI Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPiMZemCPjzg"
      },
      "source": [
        "### Low level editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4BQOv1SO7Ba"
      },
      "outputs": [],
      "source": [
        "model_name = \"gpt-image-1-mini\"\n",
        "num_perturb = 10\n",
        "find_best_method = True\n",
        "parent_output_dir = \"output_openai_low_level_editing\"\n",
        "os.makedirs(parent_output_dir, exist_ok=True)\n",
        "for cat, data in low_level_editing_data.items():\n",
        "    output_dir = os.path.join(parent_output_dir, f\"output_openai_{cat}_low_level_editing\")\n",
        "    for img_path, prompt in data:\n",
        "        results = run_interpretability_pipeline(\n",
        "            prompt=prompt,\n",
        "            input_image_path=img_path,\n",
        "            output_dir=output_dir,\n",
        "            model_name=model_name,\n",
        "            model_txt=model_txt,\n",
        "            num_perturb=num_perturb,\n",
        "            mode=\"inverse\",\n",
        "            api_key=openai_api_key,\n",
        "            find_best_method=find_best_method,\n",
        "        )\n",
        "        full_path = os.path.join(output_dir, f\"results_openai_{cat}_low_level_editing.pkl\")\n",
        "        with open(full_path, \"wb\") as f:\n",
        "            pickle.dump(results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLD2PWFATDaT"
      },
      "outputs": [],
      "source": [
        "!zip -r output_openai_low_level_editing.zip output_openai_low_level_editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moR2K8QKTKcQ"
      },
      "outputs": [],
      "source": [
        "!cp output_openai_low_level_editing.zip /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU_MYpm6QIrd"
      },
      "source": [
        "### High level editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTKwWtqWknQ0"
      },
      "outputs": [],
      "source": [
        "model_name = \"gpt-image-1-mini\"\n",
        "num_perturb = 10\n",
        "find_best_method = True\n",
        "parent_output_dir = \"output_openai_high_level_editing\"\n",
        "os.makedirs(parent_output_dir, exist_ok=True)\n",
        "for cat, data in high_level_edition_data.items():\n",
        "    output_dir = os.path.join(parent_output_dir, f\"output_openai_{cat}_high_level_editing\")\n",
        "    for img_path, prompt in data:\n",
        "        results = run_interpretability_pipeline(\n",
        "            prompt=prompt,\n",
        "            input_image_path=img_path,\n",
        "            output_dir=output_dir,\n",
        "            model_name=model_name,\n",
        "            model_txt=model_txt,\n",
        "            num_perturb=num_perturb,\n",
        "            mode=\"inverse\",\n",
        "            api_key=openai_api_key,\n",
        "            find_best_method=find_best_method,\n",
        "        )\n",
        "        full_path = os.path.join(output_dir, f\"results_openai_{cat}_high_level_editing.pkl\")\n",
        "        with open(full_path, \"wb\") as f:\n",
        "            pickle.dump(results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJPXhKSPZPoG"
      },
      "outputs": [],
      "source": [
        "!zip -r output_openai_high_level_editing.zip output_openai_high_level_editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRwXiXShZQND"
      },
      "outputs": [],
      "source": [
        "!cp output_openai_high_level_editing.zip /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-xkXpLSRfDM"
      },
      "source": [
        "## SeeDream Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4u2p4WbRfDN"
      },
      "source": [
        "### Low level editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "740ovOndRfDN"
      },
      "outputs": [],
      "source": [
        "model_name = \"seedream-4-0-250828\"\n",
        "num_perturb = 10\n",
        "find_best_method = True\n",
        "parent_output_dir = \"output_seedream_low_level_editing\"\n",
        "os.makedirs(parent_output_dir, exist_ok=True)\n",
        "for cat, data in low_level_editing_data.items():\n",
        "    output_dir = os.path.join(parent_output_dir, f\"output_seedream_{cat}_low_level_editing\")\n",
        "    for img_path, prompt in data:\n",
        "        results = run_interpretability_pipeline(\n",
        "            prompt=prompt,\n",
        "            input_image_path=img_path,\n",
        "            output_dir=output_dir,\n",
        "            model_name=model_name,\n",
        "            model_txt=model_txt,\n",
        "            num_perturb=num_perturb,\n",
        "            mode=\"inverse\",\n",
        "            api_key=ark_api_key,\n",
        "            find_best_method=find_best_method,\n",
        "        )\n",
        "        full_path = os.path.join(output_dir, f\"results_seedream_{cat}_low_level_editing.pkl\")\n",
        "        with open(full_path, \"wb\") as f:\n",
        "            pickle.dump(results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAMgqnQhZarb"
      },
      "outputs": [],
      "source": [
        "!zip -r output_seedream_low_level_editing.zip output_seedream_low_level_editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUvyoaxDZe7D"
      },
      "outputs": [],
      "source": [
        "!cp output_seedream_low_level_editing.zip /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH1mH4enRfDN"
      },
      "source": [
        "### High level editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-yXDfy_RfDN"
      },
      "outputs": [],
      "source": [
        "model_name = \"seedream-4-0-250828\"\n",
        "num_perturb = 10\n",
        "find_best_method = True\n",
        "parent_output_dir = \"output_seedream_high_level_editing\"\n",
        "os.makedirs(parent_output_dir, exist_ok=True)\n",
        "for cat, data in high_level_editing_data.items():\n",
        "    output_dir = os.path.join(parent_output_dir, f\"output_seedream_{cat}_high_level_editing\")\n",
        "    for img_path, prompt in data:\n",
        "        results = run_interpretability_pipeline(\n",
        "            prompt=prompt,\n",
        "            input_image_path=img_path,\n",
        "            output_dir=output_dir,\n",
        "            model_name=model_name,\n",
        "            model_txt=model_txt,\n",
        "            num_perturb=num_perturb,\n",
        "            mode=\"inverse\",\n",
        "            api_key=ark_api_key,\n",
        "            find_best_method=find_best_method,\n",
        "        )\n",
        "        full_path = os.path.join(output_dir, f\"results_seedream_{cat}_high_level_editing.pkl\")\n",
        "        with open(full_path, \"wb\") as f:\n",
        "            pickle.dump(results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GxUX9x5aMsI"
      },
      "outputs": [],
      "source": [
        "!zip -r output_seedream_high_level_editing.zip output_seedream_high_level_editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "He0MBrpYaMfP"
      },
      "outputs": [],
      "source": [
        "!cp output_seedream_high_level_editing.zip /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuu00rzBRiFo"
      },
      "source": [
        "## Gemini Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBchNA8URiFp"
      },
      "source": [
        "### Low level editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMj-P4P3RiFp"
      },
      "outputs": [],
      "source": [
        "model_name = \"gemini-2.5-flash-image\"\n",
        "num_perturb = 10\n",
        "find_best_method = True\n",
        "parent_output_dir = \"output_gemini_low_level_editing\"\n",
        "os.makedirs(parent_output_dir, exist_ok=True)\n",
        "for cat, data in low_level_editing_data.items():\n",
        "    output_dir = os.path.join(parent_output_dir, f\"output_gemini_{cat}_low_level_editing\")\n",
        "    for img_path, prompt in data:\n",
        "        results = run_interpretability_pipeline(\n",
        "            prompt=prompt,\n",
        "            input_image_path=img_path,\n",
        "            output_dir=output_dir,\n",
        "            model_name=model_name,\n",
        "            model_txt=model_txt,\n",
        "            num_perturb=num_perturb,\n",
        "            mode=\"inverse\",\n",
        "            api_key=gemini_api_key,\n",
        "            batch=True,\n",
        "            find_best_method=find_best_method,\n",
        "        )\n",
        "        full_path = os.path.join(output_dir, f\"results_gemini_{cat}_low_level_editing.pkl\")\n",
        "        with open(full_path, \"wb\") as f:\n",
        "            pickle.dump(results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5LhEEAxbe32"
      },
      "outputs": [],
      "source": [
        "!zip -r output_gemini_low_level_editing.zip output_gemini_low_level_editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRV20TXmbfWH"
      },
      "outputs": [],
      "source": [
        "!cp output_gemini_low_level_editing.zip /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDr5V02kRiFp"
      },
      "source": [
        "### High level editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3qHDpZrRiFq"
      },
      "outputs": [],
      "source": [
        "model_name = \"gemini-2.5-flash-image\"\n",
        "num_perturb = 10\n",
        "find_best_method = True\n",
        "parent_output_dir = \"output_gemini_high_level_editing\"\n",
        "os.makedirs(parent_output_dir, exist_ok=True)\n",
        "for cat, data in high_level_editing_data.items():\n",
        "    output_dir = os.path.join(parent_output_dir, f\"output_gemini_{cat}_high_level_editing\")\n",
        "    for img_path, prompt in data:\n",
        "        results = run_interpretability_pipeline(\n",
        "            prompt=prompt,\n",
        "            input_image_path=img_path,\n",
        "            output_dir=output_dir,\n",
        "            model_name=model_name,\n",
        "            model_txt=model_txt,\n",
        "            num_perturb=num_perturb,\n",
        "            mode=\"inverse\",\n",
        "            api_key=gemini_api_key,\n",
        "            batch=True,\n",
        "            find_best_method=find_best_method,\n",
        "        )\n",
        "        full_path = os.path.join(output_dir, f\"results_gemini_{cat}_high_level_editing.pkl\")\n",
        "        with open(full_path, \"wb\") as f:\n",
        "            pickle.dump(results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dz-JsjAIbiRW"
      },
      "outputs": [],
      "source": [
        "!zip -r output_gemini_high_level_editing.zip output_gemini_high_level_editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gu_zwYZobhlr"
      },
      "outputs": [],
      "source": [
        "!cp output_gemini_high_level_editing.zip /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kx_ZFrxJQRq"
      },
      "source": [
        "# Instruct-pix2pix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCQRiDMGJ92L"
      },
      "source": [
        "## Low level editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rg9vFIZo9J1Y"
      },
      "outputs": [],
      "source": [
        "model_name = \"timbrooks/instruct-pix2pix\"\n",
        "num_perturb = 150\n",
        "find_best_method = True\n",
        "parent_output_dir = \"output_instruct-pix2pix_low_level_editing\"\n",
        "os.makedirs(parent_output_dir, exist_ok=True)\n",
        "for cat, data in low_level_editing_data.items():\n",
        "    output_dir = os.path.join(parent_output_dir, f\"output_instruct-pix2pix_{cat}_low_level_editing\")\n",
        "    for img_path, prompt in data:\n",
        "        results = run_interpretability_pipeline(\n",
        "            prompt=prompt,\n",
        "            input_image_path=img_path,\n",
        "            output_dir=output_dir,\n",
        "            model_name=model_name,\n",
        "            model_txt=model_txt,\n",
        "            num_perturb=num_perturb,\n",
        "            mode=\"inverse\",\n",
        "            find_best_method=find_best_method,\n",
        "        )\n",
        "        full_path = os.path.join(output_dir, f\"results_instruct-pix2pix_{cat}_low_level_editing.pkl\")\n",
        "        with open(full_path, \"wb\") as f:\n",
        "            pickle.dump(results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f_u-CdIKOL7"
      },
      "outputs": [],
      "source": [
        "!zip -r output_instruct-pix2pix_low_level_editing.zip output_instruct-pix2pix_low_level_editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdWNvQvWKN-2"
      },
      "outputs": [],
      "source": [
        "!cp output_instruct-pix2pix_low_level_editing.zip /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koiQ07LqJ_vJ"
      },
      "source": [
        "## High level editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEpzZhwi9JyB"
      },
      "outputs": [],
      "source": [
        "model_name = \"timbrooks/instruct-pix2pix\"\n",
        "num_perturb = 10\n",
        "find_best_method = True\n",
        "parent_output_dir = \"output_instruct-pix2pix_high_level_editing\"\n",
        "os.makedirs(parent_output_dir, exist_ok=True)\n",
        "for cat, data in high_level_editing_data.items():\n",
        "    output_dir = os.path.join(parent_output_dir, f\"output_instruct-pix2pix_{cat}_high_level_editing\")\n",
        "    for img_path, prompt in data:\n",
        "        results = run_interpretability_pipeline(\n",
        "            prompt=prompt,\n",
        "            input_image_path=img_path,\n",
        "            output_dir=output_dir,\n",
        "            model_name=model_name,\n",
        "            model_txt=model_txt,\n",
        "            num_perturb=num_perturb,\n",
        "            mode=\"inverse\",\n",
        "            find_best_method=find_best_method,\n",
        "        )\n",
        "        full_path = os.path.join(output_dir, f\"results_instruct-pix2pix_{cat}_high_level_editing.pkl\")\n",
        "        with open(full_path, \"wb\") as f:\n",
        "            pickle.dump(results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GNRxbaF9JqE"
      },
      "outputs": [],
      "source": [
        "!zip -r output_instruct-pix2pix_high_level_editing.zip output_instruct-pix2pix_high_level_editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TC7MrI1VKSCT"
      },
      "outputs": [],
      "source": [
        "!cp output_instruct-pix2pix_high_level_editing.zip /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_C7XODkgFaV"
      },
      "source": [
        "# Stable Diffusion v2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrHiH8NfgPlN"
      },
      "source": [
        "## Low level editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S1Is8OSgUpa"
      },
      "outputs": [],
      "source": [
        "model_name = \"stabilityai/stable-diffusion-2-inpainting\"\n",
        "num_perturb = 10\n",
        "find_best_method = True\n",
        "parent_output_dir = \"output_stable-diffusion-2-inpainting_low_level_editing\"\n",
        "os.makedirs(parent_output_dir, exist_ok=True)\n",
        "for cat, data in low_level_editing_data.items():\n",
        "    output_dir = os.path.join(parent_output_dir, f\"output_stable-diffusion-2-inpainting_{cat}_low_level_editing\")\n",
        "    for img_path, prompt in data:\n",
        "        results = run_interpretability_pipeline(\n",
        "            prompt=prompt,\n",
        "            input_image_path=img_path,\n",
        "            output_dir=output_dir,\n",
        "            model_name=model_name,\n",
        "            model_txt=model_txt,\n",
        "            num_perturb=num_perturb,\n",
        "            mode=\"inverse\",\n",
        "            find_best_method=find_best_method,\n",
        "        )\n",
        "        full_path = os.path.join(output_dir, f\"results_stable-diffusion-2-inpainting_{cat}_low_level_editing.pkl\")\n",
        "        with open(full_path, \"wb\") as f:\n",
        "            pickle.dump(results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86HWCMt0gle0"
      },
      "outputs": [],
      "source": [
        "!zip -r output_stable-diffusion-2-inpainting_low_level_editing.zip output_stable-diffusion-2-inpainting_low_level_editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjJXEdXzglQU"
      },
      "outputs": [],
      "source": [
        "!cp output_stable-diffusion-2-inpainting_low_level_editing.zip /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhzDkISLgPVv"
      },
      "source": [
        "## High level editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rEr7rqAgURK"
      },
      "outputs": [],
      "source": [
        "model_name = \"stabilityai/stable-diffusion-2-inpainting\"\n",
        "num_perturb = 10\n",
        "find_best_method = True\n",
        "parent_output_dir = \"output_stable-diffusion-2-inpainting_high_level_editing\"\n",
        "os.makedirs(parent_output_dir, exist_ok=True)\n",
        "for cat, data in high_level_editing_data.items():\n",
        "    output_dir = os.path.join(parent_output_dir, f\"output_stable-diffusion-2-inpainting_{cat}_high_level_editing\")\n",
        "    for img_path, prompt in data:\n",
        "        results = run_interpretability_pipeline(\n",
        "            prompt=prompt,\n",
        "            input_image_path=img_path,\n",
        "            output_dir=output_dir,\n",
        "            model_name=model_name,\n",
        "            model_txt=model_txt,\n",
        "            num_perturb=num_perturb,\n",
        "            mode=\"inverse\",\n",
        "            find_best_method=find_best_method,\n",
        "        )\n",
        "        full_path = os.path.join(output_dir, f\"results_stable-diffusion-2-inpainting_{cat}_high_level_editing.pkl\")\n",
        "        with open(full_path, \"wb\") as f:\n",
        "            pickle.dump(results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Nt0xdmQg2NU"
      },
      "outputs": [],
      "source": [
        "!zip -r output_stable-diffusion-2-inpainting_high_level_editing.zip output_stable-diffusion-2-inpainting_high_level_editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxgVf30ag2Bc"
      },
      "outputs": [],
      "source": [
        "!cp output_stable-diffusion-2-inpainting_high_level_editing.zip /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwXqtLvohHVS"
      },
      "source": [
        "# IMG2IMG Turbo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qniqnMYyhLu2"
      },
      "source": [
        "## Low level editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xF_-DnXEhLJv"
      },
      "outputs": [],
      "source": [
        "model_name = \"edge_to_image\"\n",
        "num_perturb = 10\n",
        "low_threshold = 70\n",
        "high_threshold = 200\n",
        "find_best_method = True\n",
        "parent_output_dir = \"output_img2img-turbo_low_level_editing\"\n",
        "os.makedirs(parent_output_dir, exist_ok=True)\n",
        "for cat, data in low_level_editing_data.items():\n",
        "    output_dir = os.path.join(parent_output_dir, f\"output_img2img-turbo_{cat}_low_level_editing\")\n",
        "    for img_path, prompt in data:\n",
        "        results = run_interpretability_pipeline(\n",
        "            prompt=prompt,\n",
        "            input_image_path=img_path,\n",
        "            output_dir=output_dir,\n",
        "            model_name=model_name,\n",
        "            model_txt=model_txt,\n",
        "            num_perturb=num_perturb,\n",
        "            low_threshold=low_threshold,\n",
        "            high_threshold=high_threshold,\n",
        "            mode=\"inverse\",\n",
        "            find_best_method=find_best_method,\n",
        "        )\n",
        "        full_path = os.path.join(output_dir, f\"results_img2img-turbo_{cat}_low_level_editing.pkl\")\n",
        "        with open(full_path, \"wb\") as f:\n",
        "            pickle.dump(results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLs0E29thQNw"
      },
      "outputs": [],
      "source": [
        "!zip -r output_img2img-turbo_low_level_editing.zip output_img2img-turbo_low_level_editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0Q-K-rihaM0"
      },
      "outputs": [],
      "source": [
        "!cp output_img2img-turbo_low_level_editing.zip /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWylwRqnhOe9"
      },
      "source": [
        "## High level editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI0OvSH-ha8H"
      },
      "outputs": [],
      "source": [
        "model_name = \"edge_to_image\"\n",
        "num_perturb = 10\n",
        "low_threshold = 70\n",
        "high_threshold = 200\n",
        "find_best_method = True\n",
        "parent_output_dir = \"output_img2img-turbo_high_level_editing\"\n",
        "os.makedirs(parent_output_dir, exist_ok=True)\n",
        "for cat, data in high_level_editing_data.items():\n",
        "    output_dir = os.path.join(parent_output_dir, f\"output_img2img-turbo_{cat}_high_level_editing\")\n",
        "    for img_path, prompt in data:\n",
        "        results = run_interpretability_pipeline(\n",
        "            prompt=prompt,\n",
        "            input_image_path=img_path,\n",
        "            output_dir=output_dir,\n",
        "            model_name=model_name,\n",
        "            model_txt=model_txt,\n",
        "            num_perturb=num_perturb,\n",
        "            low_threshold=low_threshold,\n",
        "            high_threshold=high_threshold,\n",
        "            mode=\"inverse\",\n",
        "            find_best_method=find_best_method,\n",
        "        )\n",
        "        full_path = os.path.join(output_dir, f\"results_img2img-turbo_{cat}_high_level_editing.pkl\")\n",
        "        with open(full_path, \"wb\") as f:\n",
        "            pickle.dump(results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX6EASrEh7_J"
      },
      "outputs": [],
      "source": [
        "!zip -r output_img2img-turbo_high_level_editing.zip output_img2img-turbo_high_level_editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WVp5jdnh7xa"
      },
      "outputs": [],
      "source": [
        "!cp output_img2img-turbo_high_level_editing.zip /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculation AUC"
      ],
      "metadata": {
        "id": "YMzMfZXe8lYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    low_level_editing: List[str] = ['Deblurring', 'HazeRemoval', 'Lowlight', 'NoiseRemoval', 'RainRemoval', 'ShadowRemoval', 'SnowRemoval', 'WatermarkRemoval']\n",
        "    high_level_edition: List[str] = ['Counting', 'DirectionPerception', 'ObjectRemoval', 'Replacement', 'BGReplacement', 'ColorAlteration', 'StyleAlteration', 'RegionAccuracy']\n",
        "\n",
        "    parent_dir = \"AUC\"\n",
        "    for model in MODELS_DIR:\n",
        "        logger.info(f\"Start calculate for {model} - low level edition\")\n",
        "\n",
        "        model_path = os.path.join(parent_dir, model, \"low\")\n",
        "        for cat in low_level_editing:\n",
        "            res_path = os.path.join(model_path, cat, f\"results_{cat}.pkl\")\n",
        "            res = load_from_pickle(res_path)\n",
        "\n",
        "            scores = res['feature_contributions']\n",
        "            text = AUC_DICT[cat][\"ori_exp\"]\n",
        "            truth = AUC_DICT[cat][\"truth\"]\n",
        "\n",
        "            auc_val = calculate_token_auc(text, scores, truth)\n",
        "\n",
        "            logger.info(f\"Calculated AUC for {model} - {cat}: {auc_val:.4f}\")\n",
        "\n",
        "            plot_importance_roc_curve(truth, scores, title=f\"ROC Curve {model} - {cat} for: {text}\")\n",
        "\n",
        "        logger.info(f\"Start calculate for {model} - high level edition\")\n",
        "\n",
        "        model_path = os.path.join(parent_dir, model, \"high\")\n",
        "        for cat in high_level_edition:\n",
        "            res_path = os.path.join(model_path, cat, f\"results_{cat}.pkl\")\n",
        "            res = load_from_pickle(res_path)\n",
        "\n",
        "            scores = res['feature_contributions']\n",
        "            text = AUC_DICT[cat][\"ori_exp\"]\n",
        "            truth = AUC_DICT[cat][\"truth\"]\n",
        "\n",
        "            auc_val = calculate_token_auc(text, scores, truth)\n",
        "\n",
        "            logger.info(f\"Calculated AUC for {model} - {cat}: {auc_val:.4f}\")\n",
        "\n",
        "            plot_importance_roc_curve(truth, scores, title=f\"ROC Curve {model} - {cat} for: {text}\")\n"
      ],
      "metadata": {
        "id": "E8VaXaiK8pD8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}