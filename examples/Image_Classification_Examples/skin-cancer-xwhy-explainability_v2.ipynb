{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"top\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:center;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Skin Cancer Detection using EfficientNET with Fairness Evaluation</b></div>","metadata":{}},{"cell_type":"markdown","source":"## Skin Cancer Detection: A Fusion of Innovation and Responsibility\n<div style=\"text-align: justify;\">Skin cancer is not only the most common human malignancy but also a complex medical enigma. The visual subtleties in skin lesions often lead to challenges in accurate and early diagnosis. In a world where precision matters, can artificial intelligence bring a new dimension to this battle? Introducing the HAM10000 dataset (\"Human Against Machine with 10,000 training images\"), a gold mine for researchers and data scientists, consisting of 10,015 dermatoscopic images. This dataset represents a stepping stone towards automation in skin cancer classification, with its seven different classes:</div>\n    <ol>\n        <li>Melanocytic nevi</li>\n        <li>Melanoma</li>\n        <li>Benign keratosis-like lesions</li>\n        <li>Basal cell carcinoma</li>\n        <li>Actinic keratoses</li>\n        <li>Vascular lesions</li>\n        <li>Dermatofibroma</li>\n    </ol>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"top\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Table of content</b></div>\n\n<div style=\"background-color:aliceblue; padding:30px; font-size:15px;color:#034914\">\n    \n<a id=\"TOC\"></a>\n## Table of Content  \n* [Importing Required Libraries](#import)\n* [Data Preprocessing](#pre)\n* [ReSampling (Balancing the Dataset)](#balance)\n* [Creating the Model](#model)\n* [Model Training](#train)\n* [Model Evaluation](#eval)\n* [Fairness Evaluation](#fair)\n* [References](#ref)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Importing Required Libraries</b></div> ","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport time\nimport copy\nfrom glob import glob\nfrom tqdm.notebook import tqdm\n\nimport pandas as pd\nimport numpy as np\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport pandas as pd\n\nimport cv2\nfrom PIL import Image\nimport shutil\n\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import f1_score\nfrom sklearn.linear_model import LinearRegression\n\nimport skimage\n\n# TensorFlow Libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.imagenet_utils import decode_predictions\nfrom tensorflow.keras.layers import Dense, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\n\nnp.random.seed(222)\n\nfrom IPython.display import YouTubeVideo\nimport albumentations as A\n\n# Set environment settings and pandas display options\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_colwidth', None)\n\n# Setting styles\nsns.set_style('darkgrid')\n\n# Warning settings\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\n\nprint('All modules have been imported')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"pre\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b> Data Preprocessing</b></div> ","metadata":{"_uuid":"c170def1ed6bd1e279dc6d5ae86a95cf6cfd2efb"}},{"cell_type":"code","source":"base_skin_dir = os.path.join('/', 'kaggle/input/skin-cancer-mnist-ham10000')\n\nimageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n                     for x in glob(os.path.join(base_skin_dir, '*', '*.jpg'))}\n\nlesion_type_dict = {\n    'nv': 'Melanocytic nevi',\n    'mel': 'Melanoma',\n    'bkl': 'Benign keratosis-like lesions ',\n    'bcc': 'Basal cell carcinoma',\n    'akiec': 'Actinic keratoses',\n    'vasc': 'Vascular lesions',\n    'df': 'Dermatofibroma'\n}\n\ntile_df = pd.read_csv(os.path.join(base_skin_dir, 'HAM10000_metadata.csv'))\ntile_df['path'] = tile_df['image_id'].map(imageid_path_dict.get)\ntile_df['cell_type'] = tile_df['dx'].map(lesion_type_dict.get) \ntile_df['cell_type_idx'] = pd.Categorical(tile_df['cell_type']).codes\ntile_df.sample(3)","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame() # columns=[\"filepaths\", \"labels\"]\n\ndf['filepaths'] =tile_df['path']\ndf['labels'] =tile_df['cell_type_idx']\ndf['labels'] =df['labels'].apply(lambda x: str(x))\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This df will be used for fairness evaluation\ndf2 = pd.DataFrame() \n\ndf2['filepaths'] = tile_df['path']\ndf2['labels']    = tile_df['cell_type_idx']\ndf2['labels']    = df['labels'].apply(lambda x: str(x))\ndf2['age']       = tile_df['age']\ndf2['sex']       = tile_df['sex']\ndf2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.labels.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_df(df, trsize, column):\n    train_df, dummy_df = train_test_split(df, train_size=trsize, shuffle=True, random_state=14, stratify=df[column])\n    valid_df, test_df= train_test_split(dummy_df, train_size=.5, shuffle=True, random_state=14, stratify=dummy_df[column])\n    print('train_df length: ', len(train_df), '  test_df length: ', len(test_df), '  valid_df length: ', len(valid_df)) \n    return train_df, test_df, valid_df    \n\ntrain_df, test_df, valid_df  = split_df(df, .8, 'labels')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df2, test_df2, valid_df2  = split_df(df2, .8, 'labels')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_label_count (df, plot_title):\n    column='labels'\n    xaxis_label='CLASS'\n    yaxis_label='IMAGE COUNT'\n    vcounts=df[column].value_counts()\n    labels=vcounts.keys().tolist()    \n    values=vcounts.tolist() \n    lcount=len(labels)\n    if lcount>55:\n        print_in_color('The number of labels is >55, no plot will be produced')\n    else:\n        width=lcount * 4\n        width=np.min([width, 20])\n        plt.figure(figsize=(width,5)) \n        form = {'family': 'serif', 'color': 'blue', 'size': 25} \n        plt.bar(labels, values)\n        plt.title(plot_title, fontsize= 24, color='blue')\n        plt.xticks(rotation=90, fontsize=18)\n        plt.yticks(fontsize=18)\n        plt.xlabel(xaxis_label, fontdict=form)\n        plt.ylabel(yaxis_label, fontdict=form)\n        if lcount >=8:\n            rotation='vertical'\n        else:\n            rotation='horizontal'\n        for i in range(lcount):\n            plt.text(i, values[i]/2, str(values[i]),fontsize=12, rotation=rotation, color='yellow', ha='center')        \n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count=train_df['labels'].value_counts()\nprint (count)\nplot_title='Initial Train Data Frame Label Count'\nplot_label_count (train_df, plot_title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"balance\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Resampling</b></div> ","metadata":{}},{"cell_type":"code","source":"def trim(df, max_samples, min_samples, column):\n    df=df.copy()\n    classes= df[column].unique()\n    print(classes)\n    class_count=len(classes)\n    length=len(df)\n    print ('dataframe initially is of length ',length, ' with ', class_count, ' classes')\n    groups=df.groupby(column)    \n    trimmed_df = pd.DataFrame(columns = df.columns)\n    groups=df.groupby(column)\n    for label in df[column].unique(): \n        group=groups.get_group(label)\n        count=len(group)    \n        if count > max_samples:\n            sampled_group=group.sample(n=max_samples, random_state=123,axis=0)\n            trimmed_df=pd.concat([trimmed_df, sampled_group], axis=0)\n        else:\n            if count>=min_samples:\n                sampled_group=group        \n                trimmed_df=pd.concat([trimmed_df, sampled_group], axis=0)\n    print('after trimming, the maximum samples in any class is now ',max_samples, ' and the minimum samples in any class is ', min_samples)\n    classes=trimmed_df[column].unique()# return this in case some classes have less than min_samples\n    class_count=len(classes) # return this in case some classes have less than min_samples\n    length=len(trimmed_df)\n    print ('the trimmed dataframe now is of length ',length, ' with ', class_count, ' classes')\n    return trimmed_df, classes, class_count\n\nmax_samples=1000\nmin_samples= 10\ncolumn='labels'\ntrain_df, classes, class_count= trim(train_df, max_samples, min_samples, column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def balance(df, n,column, working_dir, img_size):\n    def get_augmented_image(image):\n        width=int(image.shape[1]*.8)\n        height=int(image.shape[0]*.8)\n        transform= A.Compose([\n            A.HorizontalFlip(p=.5),\n            A.Rotate(limit=30, p=.25),\n            A.RandomBrightnessContrast(p=.5),\n            A.RandomGamma(p=.5),\n            A.RandomCrop(width=width, height=height, p=.25) ])    \n        return transform(image=image)['image']\n    def dummy(image):\n        return image\n\n    df=df.copy()\n    print('Initial length of dataframe is ', len(df))\n    aug_dir=os.path.join(working_dir, 'aug')# directory to store augmented images\n    if os.path.isdir(aug_dir):# start with an empty directory\n        shutil.rmtree(aug_dir)\n    os.mkdir(aug_dir)        \n    for label in df[column].unique():    \n        dir_path=os.path.join(aug_dir,label)    \n        os.mkdir(dir_path) # make class directories within aug directory\n    # create and store the augmented images  \n    total=0    \n    groups=df.groupby(column) # group by class\n    for label in df[column].unique():  # for every class               \n        group=groups.get_group(label)  # a dataframe holding only rows with the specified label \n        sample_count=len(group)   # determine how many samples there are in this class  \n        if sample_count< n: # if the class has less than target number of images\n            aug_img_count=0\n            delta=n - sample_count  # number of augmented images to create\n            target_dir=os.path.join(aug_dir, label)  # define where to write the images            \n            desc=f'augmenting class {label}'\n            for i in tqdm(range(delta), ncols=120, unit='files', colour='blue',desc=desc):\n                j= i % sample_count\n                img_path=group['filepaths'].iloc[j]\n                img=cv2.imread(img_path)\n                img=get_augmented_image(img)\n                fname=os.path.basename(img_path)\n                fname='aug' +str(i) +'-' +fname\n                dest_path=os.path.join(target_dir, fname)                \n                cv2.imwrite(dest_path, img)\n                aug_img_count +=1\n            total +=aug_img_count\n    print('Total Augmented images created= ', total)\n    # create aug_df and merge with train_df to create composite training set ndf\n    aug_fpaths=[]\n    aug_labels=[]\n    classlist=sorted(os.listdir(aug_dir))\n    for klass in classlist:\n        classpath=os.path.join(aug_dir, klass)     \n        flist=sorted(os.listdir(classpath))    \n        for f in flist:        \n            fpath=os.path.join(classpath,f)         \n            aug_fpaths.append(fpath)\n            aug_labels.append(klass)\n    Fseries=pd.Series(aug_fpaths, name='filepaths')\n    Lseries=pd.Series(aug_labels, name='labels')   \n    aug_df=pd.concat([Fseries, Lseries], axis=1)         \n    df=pd.concat([df,aug_df], axis=0).reset_index(drop=True)\n    print('Length of augmented dataframe is now ', len(df))\n    return df \n\nn=1000\nworking_dir=r'/kaggle/working/'\nimg_size = (310,640)\ncolumn='labels'\ntrain_df=balance(train_df, n,column, working_dir, img_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_title='Train DataFrame Label Count After Trimming and Balancing'\nplot_label_count (train_df, plot_title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_gens(batch_size, ycol, train_df, test_df, valid_df, img_size):\n    trgen=ImageDataGenerator(horizontal_flip=True)    \n    t_and_v_gen=ImageDataGenerator()\n    msg='{0:70s} for train generator'.format(' ')\n    print(msg, '\\r', end='') # prints over on the same line\n    train_gen=trgen.flow_from_dataframe(train_df, x_col='filepaths', y_col=ycol, target_size=img_size,\n                                       class_mode='categorical', color_mode='rgb', shuffle=True, batch_size=batch_size)\n    msg='{0:70s} for valid generator'.format(' ')\n    print(msg, '\\r', end='') # prints over on the same line\n    valid_gen=t_and_v_gen.flow_from_dataframe(valid_df, x_col='filepaths', y_col=ycol, target_size=img_size,\n                                       class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=batch_size)\n    # for the test_gen we want to calculate the batch size and test steps such that batch_size X test_steps= number of samples in test set\n    # this insures that we go through all the sample in the test set exactly once.\n    length=len(test_df)\n    test_batch_size=sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and length/n<=80],reverse=True)[0]  \n    test_steps=int(length/test_batch_size)    \n    msg='{0:70s} for test generator'.format(' ')\n    print(msg, '\\r', end='') # prints over on the same line\n    test_gen=t_and_v_gen.flow_from_dataframe(test_df, x_col='filepaths', y_col=ycol, target_size=img_size,\n                                       class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=test_batch_size)\n    # from the generator we can get information we will need later\n    classes=list(train_gen.class_indices.keys())\n    class_indices=list(train_gen.class_indices.values())\n    class_count=len(classes)\n    labels=test_gen.labels\n    print ( 'test batch size: ' ,test_batch_size, '  test steps: ', test_steps, ' number of classes : ', class_count)\n    return train_gen, test_gen, valid_gen, test_steps\n\nbs=16\nycol='labels'\ntrain_gen, test_gen, valid_gen, test_steps = make_gens(bs, ycol, train_df, test_df, valid_df, img_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_image_samples(gen ):\n    t_dict=gen.class_indices\n    classes=list(t_dict.keys())    \n    images,labels=next(gen) # get a sample batch from the generator \n    plt.figure(figsize=(25, 25))\n    length=len(labels)\n    if length<25:   #show maximum of 25 images\n        r=length\n    else:\n        r=25\n    for i in range(r):        \n        plt.subplot(5, 5, i + 1)\n        image=images[i] /255       \n        plt.imshow(image)\n        index=np.argmax(labels[i])\n        class_name=classes[index]\n        plt.title(class_name, color='blue', fontsize=18)\n        plt.axis('off')\n    plt.show()\n    \nshow_image_samples(train_gen )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def F1_score(y_true, y_pred): #taken from old keras source code\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    recall = true_positives / (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n    return f1_val","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_in_color(txt_msg,fore_tupple=(0,255,255),back_tupple=(100,100,100)):\n    #prints the text_msg in the foreground color specified by fore_tupple with the background specified by back_tupple \n    #text_msg is the text, fore_tupple is foregroud color tupple (r,g,b), back_tupple is background tupple (r,g,b)\n    # default parameter print in cyan foreground and gray background\n    rf,gf,bf=fore_tupple\n    rb,gb,bb=back_tupple\n    msg='{0}' + txt_msg\n    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m' \n    print(msg .format(mat), flush=True)\n    print('\\33[0m', flush=True) # returns default print color to back to black\n    return\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"model\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Creating the Model</b></div> ","metadata":{}},{"cell_type":"code","source":"def make_model(img_size, lr, mod_num=3):  \n    img_shape=(img_size[0], img_size[1], 3)\n    if mod_num == 0:\n        base_model=tf.keras.applications.efficientnet.EfficientNetB0(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max')\n        msg='Created EfficientNet B0 model'\n    elif mod_num == 3:\n        base_model=tf.keras.applications.efficientnet.EfficientNetB3(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max') \n        msg='Created EfficientNet B3 model'\n    elif mod_num == 5:\n        base_model=tf.keras.applications.efficientnet.EfficientNetB5(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max') \n        msg='Created EfficientNet B5 model'\n        \n    else:\n        base_model=tf.keras.applications.efficientnet.EfficientNetB7(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max')\n        msg='Created EfficientNet B7 model'   \n   \n    base_model.trainable= True\n    x=base_model.output\n    x=BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n    x = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n                    bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n    x=Dropout(rate=.4, seed=123)(x)       \n    output=Dense(class_count, activation='softmax')(x)\n    model=Model(inputs=base_model.input, outputs=output)\n    # Evidential Begins\n    model.compile(Adamax(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy', F1_score, 'AUC']) \n    msg=msg + f' with initial learning rate set to {lr}'\n    print_in_color(msg)\n    return model\n\nlr=.001\nmodel=make_model(img_size, lr) # using B3 model by default","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LR_ASK(keras.callbacks.Callback):\n    def __init__ (self, model, epochs,  ask_epoch, dwell=True, factor=.4): # initialization of the callback\n        super(LR_ASK, self).__init__()\n        self.model=model               \n        self.ask_epoch=ask_epoch\n        self.epochs=epochs\n        self.ask=True # if True query the user on a specified epoch\n        self.lowest_vloss=np.inf\n        self.lowest_aloss=np.inf\n        self.best_weights=self.model.get_weights() # set best weights to model's initial weights\n        self.best_epoch=1\n        self.plist=[]\n        self.alist=[]\n        self.dwell= dwell\n        self.factor=factor\n        \n    def get_list(self): # define a function to return the list of % validation change\n        return self.plist, self.alist\n    def on_train_begin(self, logs=None): # this runs on the beginning of training\n        if self.ask_epoch == 0: \n            print('you set ask_epoch = 0, ask_epoch will be set to 1', flush=True)\n            self.ask_epoch=1\n        if self.ask_epoch >= self.epochs: # you are running for epochs but ask_epoch>epochs\n            print('ask_epoch >= epochs, will train for ', epochs, ' epochs', flush=True)\n            self.ask=False # do not query the user\n        if self.epochs == 1:\n            self.ask=False # running only for 1 epoch so do not query user\n        else:\n            msg =f'Training will proceed until epoch {ask_epoch} then you will be asked to' \n            print_in_color(msg )\n            msg='enter H to halt training or enter an integer for how many more epochs to run then be asked again'\n            print_in_color(msg)\n            if self.dwell:\n                msg='learning rate will be automatically adjusted during training'\n                print_in_color(msg, (0,255,0))\n        self.start_time= time.time() # set the time at which training started\n       \n    def on_train_end(self, logs=None):   # runs at the end of training  \n        msg=f'loading model with weights from epoch {self.best_epoch}'\n        print_in_color(msg, (0,255,255))\n        self.model.set_weights(self.best_weights) # set the weights of the model to the best weights\n        tr_duration=time.time() - self.start_time   # determine how long the training cycle lasted         \n        hours = tr_duration // 3600\n        minutes = (tr_duration - (hours * 3600)) // 60\n        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n        print_in_color (msg) # print out training duration time\n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        vloss=logs.get('val_loss')  # get the validation loss for this epoch\n        aloss=logs.get('loss')\n        if epoch >0:\n            deltav = self.lowest_vloss- vloss \n            pimprov=(deltav/self.lowest_vloss) * 100 \n            self.plist.append(pimprov)\n            deltaa=self.lowest_aloss-aloss\n            aimprov=(deltaa/self.lowest_aloss) * 100\n            self.alist.append(aimprov)\n        else:\n            pimprov=0.0 \n            aimprov=0.0\n        if vloss< self.lowest_vloss:\n            self.lowest_vloss=vloss\n            self.best_weights=self.model.get_weights() # set best weights to model's initial weights\n            self.best_epoch=epoch + 1            \n            msg=f'\\n validation loss of {vloss:7.4f} is {pimprov:7.4f} % below lowest loss, saving weights from epoch {str(epoch + 1):3s} as best weights'\n            print_in_color(msg, (0,255,0)) # green foreground\n        else: # validation loss increased\n            pimprov=abs(pimprov)\n            msg=f'\\n validation loss of {vloss:7.4f} is {pimprov:7.4f} % above lowest loss of {self.lowest_vloss:7.4f} keeping weights from epoch {str(self.best_epoch)} as best weights'\n            print_in_color(msg, (255,255,0)) # yellow foreground\n            if self.dwell: # if dwell is True when the validation loss increases the learning rate is automatically reduced and model weights are set to best weights\n                lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n                new_lr=lr * self.factor\n                msg=f'learning rate was automatically adjusted from {lr:8.6f} to {new_lr:8.6f}, model weights set to best weights'\n                print_in_color(msg) # cyan foreground\n                tf.keras.backend.set_value(self.model.optimizer.lr, new_lr) # set the learning rate in the optimizer\n                self.model.set_weights(self.best_weights) # set the weights of the model to the best weights      \n                \n        if aloss< self.lowest_aloss:\n            self.lowest_aloss=aloss        \n        if self.ask: # are the conditions right to query the user?\n            if epoch + 1 ==self.ask_epoch: # is this epoch the one for quering the user?\n                msg='press enter to continue or enter a comment  below '\n                print_in_color(msg)\n                comment=input(' ')\n                if comment !='':\n                    comment = 'User comment: ' + comment\n                    print_in_color(comment, (155,245,66))\n                msg='\\n Enter H to end training or  an integer for the number of additional epochs to run then ask again'\n                print_in_color(msg) # cyan foreground\n                ans=input()\n                \n                if ans == 'H' or ans =='h' or ans == '0': # quit training for these conditions\n                    msg=f'you entered {ans},  Training halted on epoch {epoch+1} due to user input\\n'\n                    print_in_color(msg)\n                    self.model.stop_training = True # halt training\n                else: # user wants to continue training\n                    self.ask_epoch += int(ans)\n                    if self.ask_epoch > self.epochs:\n                        print('\\nYou specified maximum epochs of as ', self.epochs, ' cannot train for ', self.ask_epoch, flush =True)\n                    else:\n                        msg=f'you entered {ans} Training will continue to epoch {self.ask_epoch}'\n                        print_in_color(msg) # cyan foreground\n                        if self.dwell==False:\n                            lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n                            msg=f'current LR is  {lr:8.6f}  hit enter to keep  this LR or enter a new LR'\n                            print_in_color(msg) # cyan foreground\n                            ans=input(' ')\n                            if ans =='':\n                                msg=f'keeping current LR of {lr:7.5f}'\n                                print_in_color(msg) # cyan foreground\n                            else:\n                                new_lr=float(ans)\n                                tf.keras.backend.set_value(self.model.optimizer.lr, new_lr) # set the learning rate in the optimizer\n                                msg=f' changing LR to {ans}'\n                                print_in_color(msg) # cyan foreground","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"train\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Model Training</b></div> ","metadata":{}},{"cell_type":"code","source":"epochs = 40\nask_epoch = 2\nask=LR_ASK(model, epochs,  ask_epoch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model.fit(x=train_gen,   epochs=epochs, verbose=1,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tr_plot(tr_data):\n    start_epoch=0\n    #Plot the training and validation data\n    tacc=tr_data.history['accuracy']\n    tloss=tr_data.history['loss']\n    vacc=tr_data.history['val_accuracy']\n    vloss=tr_data.history['val_loss']\n    tf1=tr_data.history['F1_score']\n    vf1=tr_data.history['val_F1_score']\n    tauc=tr_data.history['auc']\n    vauc=tr_data.history['val_auc']\n    Epoch_count=len(tacc)+ start_epoch\n    Epochs=[]\n    for i in range (start_epoch ,Epoch_count):\n        Epochs.append(i+1)   \n    index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n    val_lowest=vloss[index_loss]\n    index_acc=np.argmax(vacc)\n    acc_highest=vacc[index_acc]\n    auc_index=np.argmax(vauc)\n    val_highest_auc=vauc[auc_index]\n    plt.style.use('fivethirtyeight')\n    sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n    vc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n    auc_label='best epoch= ' + str(auc_index + 1 + start_epoch)\n    fig,axes=plt.subplots(nrows=1, ncols=4, figsize=(25,10))\n    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n    axes[0].scatter(Epochs, tloss, s=100, c='red')    \n    axes[0].set_title('Training and Validation Loss')\n    axes[0].set_xlabel('Epochs', fontsize=18)\n    axes[0].set_ylabel('Loss', fontsize=18)\n    axes[0].legend()\n    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n    axes[1].scatter(Epochs, tacc, s=100, c='red')\n    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n    axes[1].set_title('Training and Validation Accuracy')\n    axes[1].set_xlabel('Epochs', fontsize=18)\n    axes[1].set_ylabel('Accuracy', fontsize=18)\n    axes[1].legend()\n    axes[2].plot (Epochs,tf1,'r',label= 'Training F1 score')    \n    axes[2].plot (Epochs,vf1,'g',label= 'Validation F1 score')\n    index_tf1=np.argmax(tf1)#  this is the epoch with the highest training F1 score\n    tf1max=tf1[index_tf1]\n    index_vf1=np.argmax(vf1)# thisiis the epoch with the highest validation F1 score\n    vf1max=vf1[index_vf1]\n    axes[2].scatter(index_vf1+1 +start_epoch,vf1max, s=150, c= 'blue', label=vc_label)    \n    axes[2].scatter(Epochs, tf1, s=100, c='red')\n    axes[2].set_title('Training and Validation F1 score')\n    axes[2].set_xlabel('Epochs', fontsize=18)\n    axes[2].set_ylabel('F1  score', fontsize=18)\n    axes[2].legend()\n    axes[3].plot(Epochs,tauc, 'r', label='Training AUC')\n    axes[3].plot(Epochs,vauc,'g',label='Validation AUC' )\n    axes[3].scatter(auc_index+1 +start_epoch,val_highest_auc, s=150, c= 'blue', label=sc_label)\n    axes[3].scatter(Epochs, tauc, s=100, c='red')    \n    axes[3].set_title('Training and Validation AUC')\n    axes[3].set_xlabel('Epochs', fontsize=18)\n    axes[3].set_ylabel('AUC', fontsize=18)\n    axes[3].legend()\n    plt.tight_layout    \n    plt.show()\n    return \n    \ntr_plot(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"eval\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Model Evaluation</b></div> ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix \n\ndef predictor(test_gen):    \n    y_pred= []\n    error_list=[]\n    error_pred_list = []\n    y_true=test_gen.labels\n    classes=list(test_gen.class_indices.keys())\n    class_count=len(classes)\n    errors=0\n    preds=model.predict(test_gen, verbose=1)\n    tests=len(preds)    \n    for i, p in enumerate(preds):        \n        pred_index=np.argmax(p)         \n        true_index=test_gen.labels[i]  # labels are integer values        \n        if pred_index != true_index: # a misclassification has occurred                                           \n            errors=errors + 1\n            file=test_gen.filenames[i]\n            error_list.append(file)\n            error_class=classes[pred_index]\n            error_pred_list.append(error_class)\n        y_pred.append(pred_index)\n            \n    acc=( 1-errors/tests) * 100\n    msg=f'there were {errors} errors in {tests} tests for an accuracy of {acc:6.2f}'\n    print_in_color(msg, (0,255,255), (100,100,100)) # cyan foreground\n    ypred=np.array(y_pred)\n    ytrue=np.array(y_true)\n    f1score=f1_score(ytrue, ypred, average='weighted')* 100\n    if class_count <=30:\n        cm = confusion_matrix(ytrue, ypred )\n        # plot the confusion matrix\n        plt.figure(figsize=(12, 8))\n        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n        plt.xticks(np.arange(class_count)+.5, classes, rotation=90)\n        plt.yticks(np.arange(class_count)+.5, classes, rotation=0)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Actual\")\n        plt.title(\"Confusion Matrix\")\n        plt.show()\n    clr = classification_report(y_true, y_pred, target_names=classes, digits= 4) # create classification report\n    print(\"Classification Report:\\n----------------------\\n\", clr)\n    return errors, tests, error_list, error_pred_list, f1score, y_pred, y_true\n\nerrors, tests, error_list, error_pred_list, f1score, y_pred, y_true = predictor(test_gen)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"xai\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Step by Step LIME Explainability</b></div> ","metadata":{}},{"cell_type":"code","source":"Xi, _ = test_gen[0] \nif len(Xi.shape) == 4: \n    Xi = Xi[0]\nif Xi.shape[-1] == 3:  # Check if it's a color image\n    Xi = cv2.cvtColor(Xi, cv2.COLOR_BGR2RGB)\nXi = cv2.resize(Xi, (299, 299))\nXi = (Xi / 255.0 - 0.5) * 2\n\nplt.imshow(Xi / 2 + 0.5)\nplt.axis('off')  # Hide axes\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample, _ = test_gen[0]\npreds = model.predict(sample)\npreds[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_pred_classes = preds[0].argsort()\ntop_pred_classes                #Index of top 5 classes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from skimage.segmentation._quickshift import quickshift\n\nsuperpixels = quickshift(Xi, kernel_size=10,max_dist=100, ratio=0.5)\nnum_superpixels = np.unique(superpixels).shape[0]\nnum_superpixels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xi.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mark_boundaries_custom(image, labels):\n    boundaries = np.gradient(labels)\n    boundary_mask = np.logical_or(boundaries[0] != 0, boundaries[1] != 0)\n    marked_image = image.copy()\n    marked_image[boundary_mask] = [1, 0, 0]  # Red color for boundaries\n    return marked_image\n\n\nimg_with_boundaries = mark_boundaries_custom(Xi/2+0.5, superpixels)\nskimage.io.imshow(img_with_boundaries)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mark_boundaries_custom(image, labels, color=(1, 0, 0), alpha=0.5):\n    \"\"\"\n    Mark boundaries on an image.\n\n    Parameters:\n    - image: The input image.\n    - labels: The superpixel labels.\n    - color: The boundary color.\n    - alpha: The blending factor (0 means only image, 1 means only boundary color).\n\n    Returns:\n    - The image with boundaries marked.\n    \"\"\"\n    boundaries = np.gradient(labels)\n    boundary_mask = np.logical_or(boundaries[0] != 0, boundaries[1] != 0)\n    \n    marked_image = image.copy()\n    for i in range(3):  # Assuming RGB image\n        marked_image[boundary_mask, i] = image[boundary_mask, i] * (1 - alpha) + color[i] * alpha\n\n    return marked_image\n\n\nimg_with_boundaries = mark_boundaries_custom(Xi/2+0.5, superpixels)\nskimage.io.imshow(img_with_boundaries)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# skimage.io.imshow(skimage.segmentation.mark_boundaries(Xi/2+0.5, superpixels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_perturb = 1000\nperturbations = np.random.binomial(1, 0.5, size=(num_perturb, num_superpixels))\nperturbations[0] #Show example of perturbation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def perturb_image(img,perturbation,segments):\n    active_pixels = np.where(perturbation == 1)[0]\n    mask = np.zeros(segments.shape)\n    for active in active_pixels:\n          mask[segments == active] = 1 \n    perturbed_image = copy.deepcopy(img)\n    perturbed_image = perturbed_image*mask[:,:,np.newaxis]\n    return perturbed_image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skimage.io.imshow(perturb_image(Xi/2+0.5,perturbations[0],superpixels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\nfor pert in perturbations:\n    perturbed_img = perturb_image(Xi,pert,superpixels)\n    pred = model.predict(perturbed_img[np.newaxis,:,:,:], verbose=0)\n    predictions.append(pred)\n\npredictions = np.array(predictions)\npredictions.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_image = np.ones(num_superpixels)[np.newaxis,:] #Perturbation with all superpixels enabled \ndistances = sklearn.metrics.pairwise_distances(perturbations,original_image, metric='cosine').ravel()\ndistances.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kernel_width = 0.25\nweights = np.sqrt(np.exp(-(distances**2)/kernel_width**2)) #Kernel function\nweights.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_to_explain = top_pred_classes[0]\nsimpler_model = LinearRegression()\nsimpler_model.fit(X=perturbations, y=predictions[:,:,class_to_explain], sample_weight=weights)\ncoeff = simpler_model.coef_[0]\ncoeff","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_top_features = 4\ntop_features = np.argsort(coeff)[-num_top_features:] \ntop_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask = np.zeros(num_superpixels) \nmask[top_features]= True #Activate top superpixels\nskimage.io.imshow(perturb_image(Xi/2+0.5,mask,superpixels) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_heat_mask(superpixels_b, coeff):\n    # Initialize an empty image\n    heat_mask = np.zeros_like(superpixels_b, dtype=float)\n\n    # Iterate over the unique labels of the superpixels\n    for idx, label in enumerate(np.unique(superpixels_b)):\n        # Set the pixels of the current superpixel to its corresponding coefficient\n        heat_mask[superpixels_b == label] = coeff[idx]\n    \n    return heat_mask\n\nheat_mask_LIME = create_heat_mask(superpixels, coeff)\n\nplt.imshow(heat_mask_LIME, cmap='plasma', interpolation='nearest')\nplt.colorbar()\nplt.title('Heatmap of LIME Coeffs')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"xwhy\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>XWhy Explainability</b></div> ","metadata":{}},{"cell_type":"code","source":"def Wasserstein_Dist(XX, YY):\n  \n    import numpy as np\n    nx = len(XX)\n    ny = len(YY)\n    n = nx + ny\n\n    XY = np.concatenate([XX,YY])\n    X2 = np.concatenate([np.repeat(1/nx, nx), np.repeat(0, ny)])\n    Y2 = np.concatenate([np.repeat(0, nx), np.repeat(1/ny, ny)])\n\n    S_Ind = np.argsort(XY)\n    XY_Sorted = XY[S_Ind]\n    X2_Sorted = X2[S_Ind]\n    Y2_Sorted = Y2[S_Ind]\n\n    Res = 0\n    E_CDF = 0\n    F_CDF = 0\n    power = 1\n\n    for ii in range(0, n-2):\n        E_CDF = E_CDF + X2_Sorted[ii]\n        F_CDF = F_CDF + Y2_Sorted[ii]\n        height = abs(F_CDF-E_CDF)\n        width = XY_Sorted[ii+1] - XY_Sorted[ii]\n        Res = Res + (height ** power) * width;  \n \n    return Res\n\ndef  Wasserstein_Dist_PVal(XX, YY):\n    # Information about Bootstrap: \n    # https://towardsdatascience.com/an-introduction-to-the-bootstrap-method-58bcb51b4d60\n    import random\n    nboots = 1000\n    WD = Wasserstein_Dist(XX,YY)\n    na = len(XX)\n    nb = len(YY)\n    n = na + nb\n    comb = np.concatenate([XX,YY])\n    reps = 0\n    bigger = 0\n    for ii in range(1, nboots):\n        e = random.sample(range(n), na)\n        f = random.sample(range(n), nb)\n        boost_WD = Wasserstein_Dist(comb[e],comb[f]);\n        if (boost_WD > WD):\n            bigger = 1 + bigger\n            \n    pVal = bigger/nboots;\n\n    return pVal, WD\n\ndef Wasserstein_Dist_Image(img1, img2):\n    if img1.shape[0] != img2.shape[0] or img1.shape[1] != img2.shape[1]:\n        pritn('input images should have the same size')\n    else:\n        WD = []\n        for ii in range(3):\n            \n            im1 = np.array(img1[:,:,ii].flatten())\n            im2 = np.array(img2[:,:,ii].flatten())\n\n            WD.append(Wasserstein_Dist(im1, im2))\n            \n    return sum(WD)\n  \ndef xwhy_image_wd(X_input, model, num_perturb = 1000, kernel_width = 0.25, eps = 0.01):\n    \n    superpixels = quickshift(X_input, kernel_size=10, max_dist=200, ratio=0.5)\n    num_superpixels = np.unique(superpixels).shape[0]\n    perturbations = np.random.binomial(1, 0.5, size=(num_perturb, num_superpixels))\n    \n    \n    def perturb_image(img,perturbation,segments):\n        active_pixels = np.where(perturbation == 1)[0]\n        mask = np.zeros(segments.shape)\n        for active in active_pixels:\n              mask[segments == active] = 1 \n        perturbed_image = copy.deepcopy(img)\n        perturbed_image = perturbed_image*mask[:,:,np.newaxis]\n        return perturbed_image\n    \n    predictions = []\n    WD_dist = []\n    for pert in perturbations:\n        perturbed_img = perturb_image(X_input,pert,superpixels)\n        pred = model.predict(perturbed_img[np.newaxis,:,:,:], verbose=0)\n        predictions.append(pred)\n        WD_dist = Wasserstein_Dist_Image(X_input, perturbed_img) + eps\n        \n\n    predictions = np.array(predictions)\n    \n    original_image = np.ones(num_superpixels)[np.newaxis,:] #Perturbation with all superpixels enabled \n    distances = sklearn.metrics.pairwise_distances(perturbations,original_image, metric='cosine').ravel()\n    \n    weights = np.sqrt(np.exp(-((WD_dist+distances)**2)/kernel_width**2)) #Kernel function\n    \n    class_to_explain = top_pred_classes[0]\n    simpler_model = LinearRegression()\n    simpler_model.fit(X=perturbations, y=predictions[:,:,class_to_explain], sample_weight=weights)\n    coeff = simpler_model.coef_[0]\n        \n    return coeff, superpixels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ncoeff_wd, superpixels_wd  = xwhy_image_wd(Xi, model=model, num_perturb = 500, kernel_width = 0.25)\n\nnum_top_features = 4\ntop_features = np.argsort(coeff_wd)[-num_top_features:] \nmask = np.zeros(num_superpixels) \nmask[top_features]= True #Activate top superpixels\nskimage.io.imshow(perturb_image(Xi/2+0.5,mask,superpixels_wd))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heat_mask_XWhy = create_heat_mask(superpixels_wd, coeff_wd)\n\nplt.imshow(heat_mask_XWhy, cmap='plasma', interpolation='nearest')\nplt.colorbar()\nplt.title('Heatmap of SMILE Coeffs - WD')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"ref\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>References</b></div> ","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:aliceblue; padding:30px; font-size:15px;color:#034914\">\n\n* [Steel Defect Classification F1score=86%](https://www.kaggle.com/code/gpiosenka/steel-defect-classification-f1score-86?scriptVersionId=116037731) by [Gerry Retiired Director Satcom at General Dynamics](https://www.kaggle.com/gpiosenka) used as a base for modelling.\n* [üî• EDA & ML on Game Play üéÆ (ongoing)](https://www.kaggle.com/code/nguyenthicamlai/eda-ml-on-game-play-ongoing) by [Nguyen Thi Cam Lai](https://www.kaggle.com/nguyenthicamlai) used for HTML-based headers.","metadata":{}},{"cell_type":"markdown","source":"<center> <a href=\"#TOC\" role=\"button\" aria-pressed=\"true\" >‚¨ÜÔ∏è Back to Table of Contents ‚¨ÜÔ∏è</a>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px;border:#034914 solid;padding: 15px;background-color:aliceblue;font-size:90%;text-align:left\">\n\n<h4><b>Authors :</b> Koorosh Aslansefat and Kuniko Paxton </h4>  \n    \n<center> <strong> If you liked this Notebook, please do upvote. </strong>\n    \n<center> <strong> If you have any questions, feel free to contact us! </strong>","metadata":{}},{"cell_type":"markdown","source":"<center> <img src=\"https://gregcfuzion.files.wordpress.com/2022/01/kind-regards-2.png\" style='width: 600px; height: 300px;'>","metadata":{}}]}